# Month 31: Proactor模式——异步完成通知

> **本月主题**：深入学习Proactor模式，掌握真正的异步I/O编程技术
> **前置知识**：Month-30 Reactor模式（事件驱动架构、epoll、EventLoop）
> **学习时长**：140小时（4周 × 35小时/周）
> **难度评级**：★★★★★（高级挑战）

---

## 本月导航

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Month 31 学习路径导航                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  第一周                第二周                第三周                第四周     │
│  ┌─────────┐          ┌─────────┐          ┌─────────┐          ┌─────────┐ │
│  │Proactor │    →     │ Windows │    →     │ Linux   │    →     │ 跨平台  │ │
│  │vs Reactor│          │  IOCP   │          │io_uring │          │ 实战    │ │
│  │         │          │         │          │         │          │         │ │
│  └────┬────┘          └────┬────┘          └────┬────┘          └────┬────┘ │
│       │                    │                    │                    │      │
│       ▼                    ▼                    ▼                    ▼      │
│  ┌─────────┐          ┌─────────┐          ┌─────────┐          ┌─────────┐ │
│  │• 异步I/O │          │• 完成端口│          │• SQ/CQ  │          │• 平台抽象│
│  │• 组件模型│          │• 重叠I/O │          │• liburing│         │• Asio分析│
│  │• 深度对比│          │• API详解 │          │• Proactor│         │• 完整框架│
│  │• POSIX AIO│         │• 高级特性│          │• 实现    │          │• 性能对比│
│  └─────────┘          └─────────┘          └─────────┘          └─────────┘ │
│                                                                             │
│  ════════════════════════════════════════════════════════════════════════  │
│                                                                             │
│                    学习成果：跨平台Proactor异步I/O框架                        │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 本月主题概述

### 为什么学习Proactor模式？

Proactor模式是异步I/O编程的核心架构，代表了I/O编程的最高效形态：

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Proactor模式在现代系统中的应用                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         平台原生实现                                  │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │                                                                     │   │
│  │   Windows IOCP       Linux io_uring      macOS kqueue+aio          │   │
│  │   ┌───────────┐      ┌───────────┐       ┌───────────┐             │   │
│  │   │ 完成端口  │      │ 提交/完成 │       │ 事件通知  │             │   │
│  │   │ 重叠I/O   │      │ 环形缓冲  │       │ AIO扩展   │             │   │
│  │   └───────────┘      └───────────┘       └───────────┘             │   │
│  │   Windows原生         Linux 5.1+         BSD系统                    │   │
│  │   最成熟              最新最高效         有限支持                   │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         跨平台框架                                    │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │                                                                     │   │
│  │   Boost.Asio          libuv              .NET Async                 │   │
│  │   ┌───────────┐      ┌───────────┐       ┌───────────┐             │   │
│  │   │ Proactor  │      │ Reactor   │       │ Task/Async│             │   │
│  │   │ 抽象层    │      │ 模拟      │       │ Pattern   │             │   │
│  │   └───────────┘      └───────────┘       └───────────┘             │   │
│  │   C++标准化           Node.js核心         C#原生支持                │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  应用场景：                                                                  │
│  • 高性能文件服务器（大量磁盘I/O）                                          │
│  • 数据库存储引擎                                                           │
│  • 云存储系统                                                               │
│  • 高频交易系统（极低延迟要求）                                             │
│  • 大规模分布式系统                                                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 与Month-30的衔接

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Reactor → Proactor 演进路径                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Month-30 Reactor模式                   Month-31 Proactor模式              │
│   ┌────────────────────────┐            ┌────────────────────────┐         │
│   │ • 同步I/O多路复用      │            │ • 真正的异步I/O        │         │
│   │ • 内核通知"可以读了"   │     →      │ • 内核通知"读完了"     │         │
│   │ • 用户执行I/O操作      │            │ • 数据已在用户缓冲区   │         │
│   │ • epoll/kqueue/select  │            │ • IOCP/io_uring        │         │
│   └────────────────────────┘            └────────────────────────┘         │
│                                                                             │
│                              对比                                            │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                                                                     │  │
│   │   Reactor                          Proactor                        │  │
│   │   ┌─────────────────────┐         ┌─────────────────────┐          │  │
│   │   │ 1. 注册fd到epoll    │         │ 1. 发起异步读请求   │          │  │
│   │   │ 2. epoll_wait等待   │         │ 2. 内核执行读操作   │          │  │
│   │   │ 3. 返回"fd可读"    │         │ 3. 内核完成，通知   │          │  │
│   │   │ 4. 用户调用read()   │         │ 4. 数据已在缓冲区   │          │  │
│   │   │ 5. 数据拷贝到用户   │         │    直接处理即可     │          │  │
│   │   └─────────────────────┘         └─────────────────────┘          │  │
│   │                                                                     │  │
│   │   系统调用次数: 多               系统调用次数: 少                  │  │
│   │   数据拷贝: 用户主导             数据拷贝: 内核完成                 │  │
│   │   编程复杂度: 中等               编程复杂度: 较高                   │  │
│   │                                                                     │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 知识体系总览

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Proactor模式知识体系                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                           理论基础                                    │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │  I/O模型分类        │  Proactor论文     │  异步编程范式              │   │
│  │  同步/异步/阻塞     │  POSA Vol.2       │  回调/Future/协程          │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                           核心组件                                    │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │                                                                     │   │
│  │  Async Operation      Completion Handler    Proactor                │   │
│  │  ┌─────────────┐      ┌─────────────────┐  ┌──────────────┐        │   │
│  │  │ 异步操作    │      │ 完成回调        │  │ 完成分发器   │        │   │
│  │  │ 请求抽象    │      │ 处理结果        │  │ 事件循环     │        │   │
│  │  └─────────────┘      └─────────────────┘  └──────────────┘        │   │
│  │          │                    │                    │                │   │
│  │          └────────────────────┼────────────────────┘                │   │
│  │                               ▼                                     │   │
│  │                    ┌─────────────────────┐                          │   │
│  │                    │ Async Operation     │                          │   │
│  │                    │ Processor (内核)    │                          │   │
│  │                    └─────────────────────┘                          │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                          平台实现                                     │   │
│  ├──────────────────┬──────────────────┬──────────────────────────────┤   │
│  │   Windows IOCP    │  Linux io_uring  │    模拟Proactor             │   │
│  │   ┌────────────┐ │ ┌──────────────┐ │ ┌────────────────────────┐   │   │
│  │   │ 完成端口   │ │ │ SQ/CQ队列   │ │ │ Reactor + 线程池       │   │   │
│  │   │ 重叠I/O    │ │ │ 共享内存    │ │ │ 模拟异步完成           │   │   │
│  │   └────────────┘ │ └──────────────┘ │ └────────────────────────┘   │   │
│  │                  │                  │                              │   │
│  │   成熟稳定       │  高性能新特性    │  兼容性方案                  │   │
│  └──────────────────┴──────────────────┴──────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 本月学习目标

### 理论目标

| 序号 | 目标 | 验证方式 |
|:----:|------|----------|
| T1 | 理解同步/异步、阻塞/非阻塞的区别 | 能画出四象限图并解释 |
| T2 | 掌握Proactor模式核心组件 | 能画出架构图并解释各组件 |
| T3 | 理解Reactor与Proactor的本质区别 | 能从多个维度对比分析 |
| T4 | 掌握Windows IOCP原理 | 能解释完成端口工作流程 |
| T5 | 掌握Linux io_uring原理 | 能解释SQ/CQ机制 |
| T6 | 理解跨平台异步I/O设计 | 能设计平台抽象接口 |
| T7 | 了解Boost.Asio架构 | 能解释io_context原理 |
| T8 | 理解异步编程的挑战 | 能分析回调地狱等问题 |

### 实践目标

| 序号 | 目标 | 产出物 |
|:----:|------|--------|
| P1 | 实现I/O模型对比演示 | `io_model_compare.cpp` |
| P2 | 实现模拟Proactor | `simulated_proactor.hpp` |
| P3 | 实现IOCP服务器（概念） | `iocp_server.cpp` |
| P4 | 实现io_uring基础操作 | `io_uring_basic.cpp` |
| P5 | 实现io_uring Proactor | `proactor_uring.hpp` |
| P6 | 实现跨平台接口 | `async_io.hpp` |
| P7 | 实现完整Proactor框架 | `proactor_framework/` |
| P8 | 性能对比测试 | `benchmark/` |

---

## 第一周：Proactor vs Reactor深度对比（Day 1-7）

> **本周目标**：理解异步I/O的本质，深入对比Reactor与Proactor模式
> **学习时长**：35小时
> **核心产出**：I/O模型对比实验代码

---

### Day 1-2：异步I/O基础概念（10小时）

#### 1.1 I/O模型的四象限分类

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          I/O模型四象限分类                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  首先要区分两组概念：                                                        │
│                                                                             │
│  阻塞(Blocking) vs 非阻塞(Non-blocking)：                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ 描述的是：调用I/O操作时，如果不能立即完成，调用者是否等待            │   │
│  │ • 阻塞：调用者挂起，直到操作完成                                     │   │
│  │ • 非阻塞：调用者立即返回，通过返回值/错误码知道状态                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  同步(Synchronous) vs 异步(Asynchronous)：                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ 描述的是：I/O操作完成时，数据是否已经准备好                          │   │
│  │ • 同步：I/O操作返回时，用户需要自己完成数据拷贝                      │   │
│  │ • 异步：I/O操作完成时，数据已经被拷贝到用户缓冲区                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│                                                                             │
│                    ┌────────────────────┬────────────────────┐             │
│                    │      阻塞          │      非阻塞        │             │
│  ┌─────────────────┼────────────────────┼────────────────────┤             │
│  │                 │                    │                    │             │
│  │      同步       │  ① 同步阻塞       │  ② 同步非阻塞     │             │
│  │                 │    (传统read)      │    (非阻塞read)    │             │
│  │                 │                    │    + 轮询          │             │
│  │                 │                    │                    │             │
│  ├─────────────────┼────────────────────┼────────────────────┤             │
│  │                 │                    │                    │             │
│  │      异步       │  ③ 异步阻塞       │  ④ 异步非阻塞     │             │
│  │                 │    (较少见)        │    (IOCP/io_uring) │             │
│  │                 │                    │    真正的AIO       │             │
│  │                 │                    │                    │             │
│  └─────────────────┴────────────────────┴────────────────────┘             │
│                                                                             │
│  注意：I/O多路复用(select/poll/epoll)属于同步I/O！                          │
│  因为epoll_wait返回后，用户仍需调用read/write完成数据传输                   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 1.2 五种I/O模型详解

```cpp
// io_models.cpp - 五种I/O模型演示
#include <sys/socket.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/epoll.h>
#include <aio.h>
#include <iostream>
#include <cstring>

/*
┌─────────────────────────────────────────────────────────────────────────────┐
│                          五种I/O模型                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 阻塞I/O (Blocking I/O)                                                  │
│     ┌────────────────────────────────────────────────────────────────┐     │
│     │  应用程序          内核                                        │     │
│     │     │                │                                         │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│                                         │     │
│     │     │                │ 等待数据...                              │     │
│     │     │   阻塞等待     │                                         │     │
│     │     │                │ 数据到达，拷贝到用户空间                 │     │
│     │     │ <──────────────│                                         │     │
│     │     │   返回数据     │                                         │     │
│     └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│  2. 非阻塞I/O (Non-blocking I/O)                                           │
│     ┌────────────────────────────────────────────────────────────────┐     │
│     │  应用程序          内核                                        │     │
│     │     │                │                                         │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│ 无数据                                  │     │
│     │     │ <──────────────│ 返回EAGAIN                              │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│ 无数据                                  │     │
│     │     │ <──────────────│ 返回EAGAIN                              │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│ 数据到达                                │     │
│     │     │ <──────────────│ 返回数据                                │     │
│     └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│  3. I/O多路复用 (I/O Multiplexing) - 仍是同步I/O！                         │
│     ┌────────────────────────────────────────────────────────────────┐     │
│     │  应用程序          内核                                        │     │
│     │     │                │                                         │     │
│     │     │ epoll_wait()   │                                         │     │
│     │     │ ──────────────>│ 等待事件...                              │     │
│     │     │   阻塞等待     │                                         │     │
│     │     │ <──────────────│ fd可读                                  │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│ 数据拷贝                                │     │
│     │     │ <──────────────│ 返回数据                                │     │
│     └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│  4. 信号驱动I/O (Signal-driven I/O)                                        │
│     ┌────────────────────────────────────────────────────────────────┐     │
│     │  应用程序          内核                                        │     │
│     │     │                │                                         │     │
│     │     │ sigaction()    │                                         │     │
│     │     │ ──────────────>│ 注册信号处理                            │     │
│     │     │ <──────────────│                                         │     │
│     │     │   继续执行     │ 等待数据...                              │     │
│     │     │                │                                         │     │
│     │     │ <── SIGIO ─────│ 数据到达，发送信号                       │     │
│     │     │   read()       │                                         │     │
│     │     │ ──────────────>│ 数据拷贝                                │     │
│     │     │ <──────────────│ 返回数据                                │     │
│     └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
│  5. 异步I/O (Asynchronous I/O) - 真正的异步！                              │
│     ┌────────────────────────────────────────────────────────────────┐     │
│     │  应用程序          内核                                        │     │
│     │     │                │                                         │     │
│     │     │ aio_read()     │                                         │     │
│     │     │ ──────────────>│ 立即返回                                │     │
│     │     │ <──────────────│                                         │     │
│     │     │   继续执行     │ 内核等待数据...                          │     │
│     │     │   其他工作     │ 内核拷贝数据到用户缓冲区                 │     │
│     │     │                │                                         │     │
│     │     │ <── 通知 ──────│ 数据已在缓冲区，通知完成                 │     │
│     │     │   直接处理     │                                         │     │
│     └────────────────────────────────────────────────────────────────┘     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/

// 模型1：阻塞I/O
void blocking_io_example(int fd) {
    char buf[1024];

    // read会阻塞，直到有数据可读
    ssize_t n = read(fd, buf, sizeof(buf));

    if (n > 0) {
        // 处理数据
        std::cout << "Read " << n << " bytes (blocking)" << std::endl;
    }
}

// 模型2：非阻塞I/O
void nonblocking_io_example(int fd) {
    // 设置非阻塞
    int flags = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);

    char buf[1024];

    while (true) {
        ssize_t n = read(fd, buf, sizeof(buf));

        if (n > 0) {
            std::cout << "Read " << n << " bytes (non-blocking)" << std::endl;
            break;
        } else if (n < 0) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) {
                // 没有数据，继续轮询
                // 这里可以做其他事情
                usleep(1000);  // 避免忙等
                continue;
            }
            break;  // 真正的错误
        } else {
            // EOF
            break;
        }
    }
}

// 模型3：I/O多路复用（同步I/O！）
void multiplexing_io_example(int fd) {
    int epfd = epoll_create1(0);

    epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = fd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);

    epoll_event events[10];

    // 等待事件（同步等待）
    int n = epoll_wait(epfd, events, 10, -1);

    if (n > 0) {
        // 注意：这里还需要调用read()完成数据拷贝
        // 所以I/O多路复用仍然是同步I/O
        char buf[1024];
        ssize_t bytes = read(fd, buf, sizeof(buf));
        std::cout << "Read " << bytes << " bytes (multiplexing)" << std::endl;
    }

    close(epfd);
}

// 模型5：POSIX异步I/O（真正的异步）
void async_io_example(int fd) {
    char buf[1024];

    struct aiocb cb;
    memset(&cb, 0, sizeof(cb));

    cb.aio_fildes = fd;
    cb.aio_buf = buf;
    cb.aio_nbytes = sizeof(buf);
    cb.aio_offset = 0;

    // 发起异步读请求，立即返回
    if (aio_read(&cb) == -1) {
        perror("aio_read");
        return;
    }

    std::cout << "Async read initiated, doing other work..." << std::endl;

    // 可以做其他事情...

    // 等待完成（也可以用信号通知）
    while (aio_error(&cb) == EINPROGRESS) {
        usleep(1000);
    }

    // 获取结果
    ssize_t n = aio_return(&cb);
    if (n > 0) {
        std::cout << "Async read completed: " << n << " bytes" << std::endl;
    }
}

/*
关键区别总结：

┌─────────────────────────────────────────────────────────────────────────────┐
│  模型              │ 等待数据阶段 │ 数据拷贝阶段 │ 分类                     │
├─────────────────────────────────────────────────────────────────────────────┤
│  阻塞I/O           │ 阻塞        │ 阻塞        │ 同步阻塞                 │
│  非阻塞I/O         │ 轮询        │ 阻塞        │ 同步非阻塞               │
│  I/O多路复用       │ 阻塞        │ 阻塞        │ 同步阻塞                 │
│  信号驱动I/O       │ 非阻塞      │ 阻塞        │ 同步非阻塞               │
│  异步I/O           │ 非阻塞      │ 非阻塞      │ 异步非阻塞               │
└─────────────────────────────────────────────────────────────────────────────┘

只有异步I/O才是真正的异步：
• 数据拷贝由内核完成
• 完成后通知用户
• 用户直接处理数据
*/
```

#### 1.3 POSIX AIO的设计与局限

```cpp
// posix_aio_example.cpp - POSIX AIO演示
#include <aio.h>
#include <fcntl.h>
#include <unistd.h>
#include <signal.h>
#include <cstring>
#include <iostream>
#include <vector>

/*
POSIX AIO (Asynchronous I/O) 介绍：

┌─────────────────────────────────────────────────────────────────────────────┐
│                          POSIX AIO概述                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  POSIX AIO是POSIX标准定义的异步I/O接口：                                    │
│                                                                             │
│  主要API：                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ aio_read()     - 发起异步读                                          │   │
│  │ aio_write()    - 发起异步写                                          │   │
│  │ aio_error()    - 检查操作状态                                        │   │
│  │ aio_return()   - 获取操作结果                                        │   │
│  │ aio_suspend()  - 等待操作完成                                        │   │
│  │ aio_cancel()   - 取消操作                                            │   │
│  │ lio_listio()   - 批量发起多个操作                                    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  核心数据结构 - struct aiocb：                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │ int             aio_fildes;    // 文件描述符                         │   │
│  │ volatile void*  aio_buf;       // 缓冲区                             │   │
│  │ size_t          aio_nbytes;    // 字节数                             │   │
│  │ off_t           aio_offset;    // 文件偏移                           │   │
│  │ int             aio_reqprio;   // 请求优先级                         │   │
│  │ struct sigevent aio_sigevent;  // 完成通知方式                       │   │
│  │ int             aio_lio_opcode;// lio_listio操作码                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/

// 完成通知的信号处理器
volatile sig_atomic_t aio_completed = 0;

void aio_completion_handler(int signo, siginfo_t* info, void* context) {
    (void)signo;
    (void)context;

    // 获取aiocb指针
    struct aiocb* cb = (struct aiocb*)info->si_value.sival_ptr;

    // 检查状态
    int error = aio_error(cb);
    if (error == 0) {
        ssize_t ret = aio_return(cb);
        std::cout << "[Signal] AIO completed, bytes: " << ret << std::endl;
    } else if (error > 0) {
        std::cout << "[Signal] AIO error: " << strerror(error) << std::endl;
    }

    aio_completed = 1;
}

// 使用信号通知的异步读
void aio_with_signal_notification() {
    std::cout << "\n=== AIO with Signal Notification ===" << std::endl;

    // 创建测试文件
    int fd = open("/tmp/aio_test.txt", O_RDWR | O_CREAT | O_TRUNC, 0644);
    write(fd, "Hello, Async I/O!", 17);
    lseek(fd, 0, SEEK_SET);

    // 设置信号处理器
    struct sigaction sa;
    sa.sa_flags = SA_SIGINFO;
    sa.sa_sigaction = aio_completion_handler;
    sigemptyset(&sa.sa_mask);
    sigaction(SIGUSR1, &sa, nullptr);

    // 准备aiocb
    char buffer[256];
    struct aiocb cb;
    memset(&cb, 0, sizeof(cb));

    cb.aio_fildes = fd;
    cb.aio_buf = buffer;
    cb.aio_nbytes = sizeof(buffer);
    cb.aio_offset = 0;

    // 设置完成通知方式：信号
    cb.aio_sigevent.sigev_notify = SIGEV_SIGNAL;
    cb.aio_sigevent.sigev_signo = SIGUSR1;
    cb.aio_sigevent.sigev_value.sival_ptr = &cb;

    // 发起异步读
    aio_completed = 0;
    if (aio_read(&cb) == -1) {
        perror("aio_read");
        close(fd);
        return;
    }

    std::cout << "AIO read initiated, waiting for signal..." << std::endl;

    // 等待信号
    while (!aio_completed) {
        pause();
    }

    std::cout << "Data: " << buffer << std::endl;

    close(fd);
    unlink("/tmp/aio_test.txt");
}

// 使用线程通知的异步读
void aio_with_thread_notification() {
    std::cout << "\n=== AIO with Thread Notification ===" << std::endl;

    int fd = open("/tmp/aio_test2.txt", O_RDWR | O_CREAT | O_TRUNC, 0644);
    write(fd, "Thread notification test!", 25);
    lseek(fd, 0, SEEK_SET);

    char buffer[256];
    struct aiocb cb;
    memset(&cb, 0, sizeof(cb));

    cb.aio_fildes = fd;
    cb.aio_buf = buffer;
    cb.aio_nbytes = sizeof(buffer);
    cb.aio_offset = 0;

    // 设置完成通知方式：线程
    cb.aio_sigevent.sigev_notify = SIGEV_THREAD;
    cb.aio_sigevent.sigev_notify_function = [](union sigval val) {
        struct aiocb* cb = (struct aiocb*)val.sival_ptr;
        int error = aio_error(cb);
        if (error == 0) {
            ssize_t ret = aio_return(cb);
            std::cout << "[Thread] AIO completed, bytes: " << ret << std::endl;
        }
    };
    cb.aio_sigevent.sigev_value.sival_ptr = &cb;
    cb.aio_sigevent.sigev_notify_attributes = nullptr;

    if (aio_read(&cb) == -1) {
        perror("aio_read");
        close(fd);
        return;
    }

    std::cout << "AIO read initiated, waiting for thread callback..." << std::endl;

    // 等待完成
    while (aio_error(&cb) == EINPROGRESS) {
        usleep(1000);
    }

    std::cout << "Data: " << buffer << std::endl;

    close(fd);
    unlink("/tmp/aio_test2.txt");
}

// 批量异步操作
void aio_batch_operations() {
    std::cout << "\n=== AIO Batch Operations (lio_listio) ===" << std::endl;

    // 创建多个测试文件
    std::vector<int> fds;
    std::vector<struct aiocb> cbs(3);
    std::vector<std::vector<char>> buffers(3, std::vector<char>(256));

    for (int i = 0; i < 3; ++i) {
        char filename[64];
        snprintf(filename, sizeof(filename), "/tmp/aio_batch_%d.txt", i);
        int fd = open(filename, O_RDWR | O_CREAT | O_TRUNC, 0644);

        char data[64];
        snprintf(data, sizeof(data), "Data from file %d", i);
        write(fd, data, strlen(data));
        lseek(fd, 0, SEEK_SET);

        fds.push_back(fd);

        memset(&cbs[i], 0, sizeof(struct aiocb));
        cbs[i].aio_fildes = fd;
        cbs[i].aio_buf = buffers[i].data();
        cbs[i].aio_nbytes = buffers[i].size();
        cbs[i].aio_offset = 0;
        cbs[i].aio_lio_opcode = LIO_READ;
    }

    // 准备aiocb指针数组
    std::vector<struct aiocb*> cb_list;
    for (auto& cb : cbs) {
        cb_list.push_back(&cb);
    }

    // 批量发起
    if (lio_listio(LIO_WAIT, cb_list.data(), cb_list.size(), nullptr) == -1) {
        perror("lio_listio");
    } else {
        std::cout << "All operations completed:" << std::endl;
        for (size_t i = 0; i < cbs.size(); ++i) {
            ssize_t ret = aio_return(&cbs[i]);
            std::cout << "  File " << i << ": " << ret << " bytes - "
                      << buffers[i].data() << std::endl;
        }
    }

    // 清理
    for (int fd : fds) {
        close(fd);
    }
    for (int i = 0; i < 3; ++i) {
        char filename[64];
        snprintf(filename, sizeof(filename), "/tmp/aio_batch_%d.txt", i);
        unlink(filename);
    }
}

/*
POSIX AIO的局限性：

┌─────────────────────────────────────────────────────────────────────────────┐
│                        POSIX AIO的问题                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 实现质量参差不齐                                                        │
│     ┌─────────────────────────────────────────────────────────────────┐    │
│     │ • Linux glibc使用用户态线程池模拟，并非真正的内核异步            │    │
│     │ • 每个AIO操作可能创建一个线程                                    │    │
│     │ • 性能反而可能不如同步I/O                                        │    │
│     └─────────────────────────────────────────────────────────────────┘    │
│                                                                             │
│  2. 功能限制                                                                │
│     ┌─────────────────────────────────────────────────────────────────┐    │
│     │ • 不支持网络socket（只支持文件）                                  │    │
│     │ • 没有统一的完成队列                                              │    │
│     │ • 取消操作不可靠                                                  │    │
│     └─────────────────────────────────────────────────────────────────┘    │
│                                                                             │
│  3. 接口设计问题                                                            │
│     ┌─────────────────────────────────────────────────────────────────┐    │
│     │ • aiocb生命周期管理复杂                                           │    │
│     │ • 信号通知方式容易出错                                            │    │
│     │ • 缺乏现代异步编程模式支持                                        │    │
│     └─────────────────────────────────────────────────────────────────┘    │
│                                                                             │
│  因此，现代系统提供了更好的方案：                                           │
│  • Windows: IOCP (I/O Completion Ports)                                    │
│  • Linux: io_uring (since 5.1)                                             │
│  • BSD/macOS: kqueue + aio                                                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/
```

---

### Day 3-4：Proactor模式核心组件（10小时）

#### 2.1 Proactor模式架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Proactor模式架构                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│                              ┌─────────────────┐                            │
│                              │   Application   │                            │
│                              └────────┬────────┘                            │
│                                       │                                     │
│                                       │ 1.发起异步操作                      │
│                                       ▼                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    Asynchronous Operation                            │   │
│  │  ┌────────────────┐ ┌────────────────┐ ┌────────────────┐           │   │
│  │  │  Async Read    │ │  Async Write   │ │  Async Accept  │           │   │
│  │  │  + buffer      │ │  + data        │ │  + callback    │           │   │
│  │  │  + callback    │ │  + callback    │ │                │           │   │
│  │  └───────┬────────┘ └───────┬────────┘ └───────┬────────┘           │   │
│  │          │                  │                  │                     │   │
│  └──────────┼──────────────────┼──────────────────┼─────────────────────┘   │
│             │ 2.提交操作       │                  │                         │
│             └──────────────────┼──────────────────┘                         │
│                                ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │               Asynchronous Operation Processor                       │   │
│  │                        (操作系统内核)                                 │   │
│  │  ┌──────────────────────────────────────────────────────────────┐   │   │
│  │  │  • 接收异步操作请求                                           │   │   │
│  │  │  • 执行实际I/O操作                                            │   │   │
│  │  │  • 将数据拷贝到用户缓冲区                                     │   │   │
│  │  │  • 将完成事件加入完成队列                                     │   │   │
│  │  └──────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────┬────────────────────────────────────────┘   │
│                                │ 3.操作完成                                 │
│                                ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Completion Event Queue                           │   │
│  │  ┌──────────────────────────────────────────────────────────────┐   │   │
│  │  │   Event1 → Event2 → Event3 → ...                              │   │   │
│  │  │   (op, result, bytes_transferred)                             │   │   │
│  │  └──────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────┬────────────────────────────────────────┘   │
│                                │ 4.获取完成事件                             │
│                                ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                          Proactor                                    │   │
│  │  ┌──────────────────────────────────────────────────────────────┐   │   │
│  │  │  run():                                                       │   │   │
│  │  │    while (running) {                                          │   │   │
│  │  │      event = wait_for_completion();  // 等待完成事件          │   │   │
│  │  │      handler = event.get_handler();  // 获取完成处理器        │   │   │
│  │  │      handler.handle_event(event);    // 调用处理器            │   │   │
│  │  │    }                                                          │   │   │
│  │  └──────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────┬────────────────────────────────────────┘   │
│                                │ 5.分发完成事件                             │
│                                ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     Completion Handler                               │   │
│  │  ┌────────────────┐ ┌────────────────┐ ┌────────────────┐           │   │
│  │  │ ReadHandler    │ │ WriteHandler   │ │ AcceptHandler  │           │   │
│  │  │ • 处理读完成   │ │ • 处理写完成   │ │ • 处理连接完成 │           │   │
│  │  │ • 数据已就绪   │ │ • 继续下一步   │ │ • 创建会话     │           │   │
│  │  └────────────────┘ └────────────────┘ └────────────────┘           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 2.2 Proactor组件详解

```cpp
// proactor_components.hpp - Proactor模式核心组件
#pragma once

#include <functional>
#include <memory>
#include <vector>
#include <queue>
#include <mutex>

namespace proactor {

// =============================================================================
// Completion Handler - 完成处理器接口
// =============================================================================

class CompletionHandler {
public:
    virtual ~CompletionHandler() = default;

    // 操作完成时调用
    virtual void handle_completion(int result, size_t bytes_transferred) = 0;
};

// =============================================================================
// Asynchronous Operation - 异步操作抽象
// =============================================================================

class AsyncOperation {
public:
    enum class Type {
        READ,
        WRITE,
        ACCEPT,
        CONNECT,
        TIMEOUT
    };

    AsyncOperation(Type type, int fd)
        : type_(type), fd_(fd) {}

    virtual ~AsyncOperation() = default;

    Type type() const { return type_; }
    int fd() const { return fd_; }

    // 设置完成处理器
    void set_handler(std::shared_ptr<CompletionHandler> handler) {
        handler_ = std::move(handler);
    }

    // 获取缓冲区（用于读写操作）
    virtual char* buffer() { return nullptr; }
    virtual size_t buffer_size() const { return 0; }
    virtual const char* data() const { return nullptr; }
    virtual size_t data_size() const { return 0; }

    // 完成回调
    void complete(int result, size_t bytes_transferred) {
        if (handler_) {
            handler_->handle_completion(result, bytes_transferred);
        }
    }

protected:
    Type type_;
    int fd_;
    std::shared_ptr<CompletionHandler> handler_;
};

// =============================================================================
// Async Read Operation - 异步读操作
// =============================================================================

class AsyncReadOp : public AsyncOperation {
public:
    using Callback = std::function<void(int result, const char* data, size_t len)>;

    AsyncReadOp(int fd, size_t buffer_size, Callback cb)
        : AsyncOperation(Type::READ, fd),
          buffer_(buffer_size),
          callback_(std::move(cb)) {}

    char* buffer() override { return buffer_.data(); }
    size_t buffer_size() const override { return buffer_.size(); }

    void complete(int result, size_t bytes_transferred) {
        if (callback_) {
            if (result >= 0 && bytes_transferred > 0) {
                callback_(result, buffer_.data(), bytes_transferred);
            } else {
                callback_(result, nullptr, 0);
            }
        }
    }

private:
    std::vector<char> buffer_;
    Callback callback_;
};

// =============================================================================
// Async Write Operation - 异步写操作
// =============================================================================

class AsyncWriteOp : public AsyncOperation {
public:
    using Callback = std::function<void(int result, size_t bytes_written)>;

    AsyncWriteOp(int fd, std::string data, Callback cb)
        : AsyncOperation(Type::WRITE, fd),
          data_(std::move(data)),
          callback_(std::move(cb)) {}

    const char* data() const override { return data_.data(); }
    size_t data_size() const override { return data_.size(); }

    void complete(int result, size_t bytes_transferred) {
        if (callback_) {
            callback_(result, bytes_transferred);
        }
    }

private:
    std::string data_;
    Callback callback_;
};

// =============================================================================
// Async Accept Operation - 异步接受连接操作
// =============================================================================

class AsyncAcceptOp : public AsyncOperation {
public:
    using Callback = std::function<void(int client_fd)>;

    AsyncAcceptOp(int listen_fd, Callback cb)
        : AsyncOperation(Type::ACCEPT, listen_fd),
          callback_(std::move(cb)) {}

    void complete(int result, size_t /*bytes_transferred*/) {
        if (callback_) {
            callback_(result);  // result就是新连接的fd
        }
    }

private:
    Callback callback_;
};

// =============================================================================
// Completion Event - 完成事件
// =============================================================================

struct CompletionEvent {
    AsyncOperation* operation;
    int result;
    size_t bytes_transferred;
};

// =============================================================================
// Completion Event Queue - 完成事件队列
// =============================================================================

class CompletionEventQueue {
public:
    void push(const CompletionEvent& event) {
        std::lock_guard<std::mutex> lock(mutex_);
        events_.push(event);
    }

    bool pop(CompletionEvent& event) {
        std::lock_guard<std::mutex> lock(mutex_);
        if (events_.empty()) {
            return false;
        }
        event = events_.front();
        events_.pop();
        return true;
    }

    bool empty() const {
        std::lock_guard<std::mutex> lock(mutex_);
        return events_.empty();
    }

private:
    std::queue<CompletionEvent> events_;
    mutable std::mutex mutex_;
};

// =============================================================================
// Proactor - 完成事件分发器
// =============================================================================

class Proactor {
public:
    virtual ~Proactor() = default;

    // 发起异步读
    virtual void async_read(int fd, size_t buffer_size,
                           AsyncReadOp::Callback callback) = 0;

    // 发起异步写
    virtual void async_write(int fd, std::string data,
                            AsyncWriteOp::Callback callback) = 0;

    // 发起异步accept
    virtual void async_accept(int listen_fd,
                             AsyncAcceptOp::Callback callback) = 0;

    // 运行事件循环
    virtual void run() = 0;

    // 停止事件循环
    virtual void stop() = 0;
};

} // namespace proactor
```

#### 2.3 组件交互序列图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Proactor模式交互序列图                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Application     Proactor       OS Kernel      CompletionQueue   Handler   │
│      │              │              │                │              │        │
│      │  1.async_read│              │                │              │        │
│      │─────────────→│              │                │              │        │
│      │              │  2.submit    │                │              │        │
│      │              │─────────────→│                │              │        │
│      │              │              │                │              │        │
│      │  立即返回    │              │                │              │        │
│      │◀─────────────│              │                │              │        │
│      │              │              │                │              │        │
│      │  继续执行    │              │ 3.内核执行I/O  │              │        │
│      │  其他工作    │              │ 数据拷贝到     │              │        │
│      │              │              │ 用户缓冲区     │              │        │
│      │              │              │                │              │        │
│      │              │              │  4.完成        │              │        │
│      │              │              │───────────────→│              │        │
│      │              │              │                │              │        │
│      │              │  5.run()等待 │                │              │        │
│      │              │─────────────────────────────→ │              │        │
│      │              │              │                │              │        │
│      │              │  6.获取完成事件               │              │        │
│      │              │◀─────────────────────────────│              │        │
│      │              │              │                │              │        │
│      │              │  7.分发到Handler             │              │        │
│      │              │─────────────────────────────────────────────→│        │
│      │              │              │                │              │        │
│      │              │              │                │   8.处理     │        │
│      │              │              │                │   完成的     │        │
│      │              │              │                │   数据       │        │
│      │              │              │                │              │        │
│                                                                             │
│  关键点：                                                                    │
│  • 步骤2-4由内核异步完成，应用程序不阻塞                                     │
│  • 数据拷贝在内核中完成，完成后直接可用                                      │
│  • Proactor在run()中等待并分发完成事件                                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Day 5-7：Reactor vs Proactor深度对比（15小时）

#### 3.1 事件通知时机对比

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Reactor vs Proactor 事件通知对比                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Reactor模式 - "通知你可以做了"                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  时间线：                                                           │   │
│  │  ──────────────────────────────────────────────────────────────→   │   │
│  │                                                                     │   │
│  │  内核：    │等待数据...│数据到达│                                   │   │
│  │            └──────────┴───┬────┘                                   │   │
│  │                           │                                        │   │
│  │  通知：                   ▼ "fd可读了！"                            │   │
│  │                           │                                        │   │
│  │  用户：                   │──→│ read() │──→│ 处理数据 │            │   │
│  │                           │   └────────┘   └──────────┘            │   │
│  │                           │                                        │   │
│  │  特点：                   │                                        │   │
│  │  • 通知时机：数据到达，可以读了                                    │   │
│  │  • 用户动作：需要自己调用read()完成数据拷贝                        │   │
│  │  • 阻塞点：read()系统调用期间阻塞                                  │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  Proactor模式 - "通知你事情做完了"                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  时间线：                                                           │   │
│  │  ──────────────────────────────────────────────────────────────→   │   │
│  │                                                                     │   │
│  │  用户：│async_read()│                    │ 处理数据 │              │   │
│  │        └─────┬──────┘                    └─────┬────┘              │   │
│  │              │                                 ↑                    │   │
│  │              │ 发起请求                        │                    │   │
│  │              ▼                                 │                    │   │
│  │  内核：      │等待数据│数据到达│拷贝到缓冲│完成│                   │   │
│  │              └────────┴────────┴─────────┴──┬─┘                   │   │
│  │                                              │                     │   │
│  │  通知：                                      ▼ "读完了！"          │   │
│  │                                                                     │   │
│  │  特点：                                                            │   │
│  │  • 通知时机：I/O操作完成，数据已在缓冲区                           │   │
│  │  • 用户动作：直接处理数据，无需额外系统调用                        │   │
│  │  • 阻塞点：无阻塞，发起请求后立即返回                              │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 3.2 详细对比分析

```cpp
// reactor_vs_proactor.cpp - 两种模式的对比分析
#include <iostream>
#include <chrono>

/*
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Reactor vs Proactor 多维度对比                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 数据拷贝次数                                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  Reactor (以接收数据为例):                                          │   │
│  │  ┌──────────┐    ┌──────────┐    ┌──────────┐                      │   │
│  │  │ 网卡     │ →  │ 内核缓冲 │ →  │ 用户缓冲 │                      │   │
│  │  │ 硬件     │    │ (socket) │    │ (应用)   │                      │   │
│  │  └──────────┘    └──────────┘    └──────────┘                      │   │
│  │       DMA           1次            read()                           │   │
│  │                                   (用户调用)                        │   │
│  │                                                                     │   │
│  │  Proactor (以接收数据为例):                                         │   │
│  │  ┌──────────┐    ┌──────────┐                                      │   │
│  │  │ 网卡     │ →  │ 用户缓冲 │  (可能直接DMA到用户空间)              │   │
│  │  │ 硬件     │    │ (应用)   │                                      │   │
│  │  └──────────┘    └──────────┘                                      │   │
│  │       DMA          0~1次                                            │   │
│  │                 (内核完成)                                          │   │
│  │                                                                     │   │
│  │  注：io_uring支持注册用户缓冲区，可减少拷贝                         │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  2. 系统调用次数                                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  Reactor (处理一次读事件):                                          │   │
│  │  1. epoll_wait() - 等待事件                                         │   │
│  │  2. read()       - 读取数据                                         │   │
│  │  共 2 次系统调用                                                    │   │
│  │                                                                     │   │
│  │  Proactor (处理一次读事件):                                         │   │
│  │  1. io_uring_submit() - 提交请求（可批量）                          │   │
│  │  2. io_uring_wait_cqe() - 等待完成                                  │   │
│  │  共 2 次系统调用                                                    │   │
│  │                                                                     │   │
│  │  但io_uring的优势：                                                 │   │
│  │  • submit可以批量提交多个请求                                       │   │
│  │  • 共享内存减少数据传输                                             │   │
│  │  • SQPOLL模式可完全避免系统调用                                     │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  3. 编程模型复杂度                                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  Reactor:                                                           │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │ void on_readable(int fd) {                                  │   │   │
│  │  │     char buf[1024];                                         │   │   │
│  │  │     ssize_t n = read(fd, buf, sizeof(buf)); // 同步读       │   │   │
│  │  │     if (n > 0) {                                            │   │   │
│  │  │         process(buf, n);                                    │   │   │
│  │  │     }                                                       │   │   │
│  │  │ }                                                           │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │  优点：顺序流程，易于理解                                          │   │
│  │  缺点：需要处理EAGAIN，边缘触发复杂                                 │   │
│  │                                                                     │   │
│  │  Proactor:                                                          │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │ void start_read(int fd) {                                   │   │   │
│  │  │     async_read(fd, [](int result, char* data, size_t len) { │   │   │
│  │  │         if (result > 0) {                                   │   │   │
│  │  │             process(data, len);                             │   │   │
│  │  │             start_read(fd); // 继续读，回调嵌套             │   │   │
│  │  │         }                                                   │   │   │
│  │  │     });                                                     │   │   │
│  │  │ }                                                           │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  │  优点：真正非阻塞，高并发性能好                                    │   │
│  │  缺点：回调嵌套（回调地狱），生命周期管理复杂                      │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  4. 适用场景                                                                │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │  Reactor更适合：                                                    │   │
│  │  • 网络I/O为主的场景                                                │   │
│  │  • 需要处理大量短连接                                               │   │
│  │  • I/O操作本身很快（小数据量）                                      │   │
│  │  • 平台兼容性要求高                                                 │   │
│  │                                                                     │   │
│  │  Proactor更适合：                                                   │   │
│  │  • 磁盘I/O密集型场景                                                │   │
│  │  • 大数据量传输                                                     │   │
│  │  • 需要极低延迟                                                     │   │
│  │  • 可以使用特定平台特性                                             │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/

// 性能对比表
void print_comparison_table() {
    std::cout << R"(
┌─────────────────────────────────────────────────────────────────────────────┐
│                    Reactor vs Proactor 对比总结                              │
├─────────────────┬───────────────────────┬───────────────────────────────────┤
│ 维度            │ Reactor               │ Proactor                          │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 通知时机        │ I/O就绪时             │ I/O完成时                         │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 数据拷贝        │ 用户主动调用          │ 内核自动完成                      │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 系统调用        │ 等待 + I/O操作        │ 提交 + 等待完成                   │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 编程复杂度      │ 中等                  │ 较高（回调管理）                  │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 平台支持        │ 广泛（所有Unix）      │ 有限（IOCP/io_uring）            │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 网络I/O性能     │ 优秀                  │ 优秀（略优）                      │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 磁盘I/O性能     │ 一般                  │ 优秀                              │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 代表实现        │ epoll/kqueue          │ IOCP/io_uring                     │
├─────────────────┼───────────────────────┼───────────────────────────────────┤
│ 典型框架        │ libevent/muduo        │ Boost.Asio/libuv(模拟)            │
└─────────────────┴───────────────────────┴───────────────────────────────────┘
)" << std::endl;
}
```

#### 3.3 模拟Proactor实现

```cpp
// simulated_proactor.hpp - 基于Reactor模拟的Proactor
#pragma once

#include <sys/epoll.h>
#include <unistd.h>
#include <fcntl.h>
#include <functional>
#include <memory>
#include <unordered_map>
#include <queue>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <iostream>

namespace proactor {

/*
模拟Proactor的实现思路：

┌─────────────────────────────────────────────────────────────────────────────┐
│                    模拟Proactor架构                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  虽然Linux传统上没有原生Proactor支持，但可以通过以下方式模拟：              │
│                                                                             │
│  方案1：Reactor + 线程池                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                     │   │
│  │   用户代码              模拟Proactor                                │   │
│  │   ┌────────┐           ┌──────────────────────────────────┐        │   │
│  │   │async_  │           │  ┌──────────┐   ┌─────────────┐  │        │   │
│  │   │read()  │ ─────────→│  │ Reactor  │   │ Thread Pool │  │        │   │
│  │   └────────┘           │  │ (epoll)  │   │             │  │        │   │
│  │                        │  └────┬─────┘   └──────┬──────┘  │        │   │
│  │                        │       │                │         │        │   │
│  │   ┌────────┐           │       │ fd可读         │ 执行    │        │   │
│  │   │callback│ ◀─────────│       ▼                │ read()  │        │   │
│  │   │(data)  │           │  分发给线程池 ────────→│         │        │   │
│  │   └────────┘           │                        │         │        │   │
│  │                        │  完成队列 ◀────────────┘         │        │   │
│  │                        └──────────────────────────────────┘        │   │
│  │                                                                     │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  这种方式的特点：                                                           │
│  • 对用户呈现Proactor接口                                                   │
│  • 内部使用Reactor监听事件                                                  │
│  • 线程池执行实际I/O操作                                                    │
│  • I/O完成后调用用户回调                                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/

// 完成事件
struct CompletionEvent {
    int fd;
    int result;
    std::vector<char> data;
    std::function<void(int, const char*, size_t)> callback;
};

// 线程池
class ThreadPool {
public:
    ThreadPool(size_t threads) : stop_(false) {
        for (size_t i = 0; i < threads; ++i) {
            workers_.emplace_back([this] {
                while (true) {
                    std::function<void()> task;
                    {
                        std::unique_lock<std::mutex> lock(mutex_);
                        cv_.wait(lock, [this] {
                            return stop_ || !tasks_.empty();
                        });
                        if (stop_ && tasks_.empty()) return;
                        task = std::move(tasks_.front());
                        tasks_.pop();
                    }
                    task();
                }
            });
        }
    }

    ~ThreadPool() {
        {
            std::unique_lock<std::mutex> lock(mutex_);
            stop_ = true;
        }
        cv_.notify_all();
        for (auto& t : workers_) {
            if (t.joinable()) t.join();
        }
    }

    template<typename F>
    void enqueue(F&& f) {
        {
            std::unique_lock<std::mutex> lock(mutex_);
            tasks_.emplace(std::forward<F>(f));
        }
        cv_.notify_one();
    }

private:
    std::vector<std::thread> workers_;
    std::queue<std::function<void()>> tasks_;
    std::mutex mutex_;
    std::condition_variable cv_;
    bool stop_;
};

// 模拟Proactor
class SimulatedProactor {
public:
    using ReadCallback = std::function<void(int result, const char* data, size_t len)>;
    using WriteCallback = std::function<void(int result, size_t bytes_written)>;
    using AcceptCallback = std::function<void(int client_fd)>;

    SimulatedProactor(size_t thread_count = 4)
        : thread_pool_(thread_count),
          epfd_(epoll_create1(EPOLL_CLOEXEC)),
          running_(false) {

        if (epfd_ < 0) {
            throw std::runtime_error("epoll_create1 failed");
        }
    }

    ~SimulatedProactor() {
        stop();
        if (epfd_ >= 0) close(epfd_);
    }

    // 发起异步读
    void async_read(int fd, ReadCallback callback) {
        // 设置非阻塞
        set_nonblocking(fd);

        // 保存回调
        {
            std::lock_guard<std::mutex> lock(callbacks_mutex_);
            read_callbacks_[fd] = std::move(callback);
        }

        // 注册到epoll
        epoll_event ev;
        ev.events = EPOLLIN | EPOLLET;
        ev.data.fd = fd;
        epoll_ctl(epfd_, EPOLL_CTL_ADD, fd, &ev);
    }

    // 发起异步写
    void async_write(int fd, std::string data, WriteCallback callback) {
        // 在线程池中执行写操作
        thread_pool_.enqueue([this, fd, data = std::move(data),
                              callback = std::move(callback)]() {
            ssize_t n = write(fd, data.data(), data.size());

            // 将完成事件投递到主循环
            {
                std::lock_guard<std::mutex> lock(completions_mutex_);
                completions_.push({fd, static_cast<int>(n),
                                  [callback, n]() {
                                      callback(n >= 0 ? 0 : -1, n > 0 ? n : 0);
                                  }});
            }
        });
    }

    // 发起异步accept
    void async_accept(int listen_fd, AcceptCallback callback) {
        set_nonblocking(listen_fd);

        {
            std::lock_guard<std::mutex> lock(callbacks_mutex_);
            accept_callbacks_[listen_fd] = std::move(callback);
        }

        epoll_event ev;
        ev.events = EPOLLIN;
        ev.data.fd = listen_fd;
        epoll_ctl(epfd_, EPOLL_CTL_ADD, listen_fd, &ev);
    }

    void run() {
        running_ = true;
        std::vector<epoll_event> events(1024);

        std::cout << "[SimulatedProactor] Event loop started" << std::endl;

        while (running_) {
            // 处理完成队列中的事件
            process_completions();

            // 等待I/O事件
            int n = epoll_wait(epfd_, events.data(), events.size(), 100);

            for (int i = 0; i < n; ++i) {
                int fd = events[i].data.fd;
                uint32_t ev = events[i].events;

                if (ev & EPOLLIN) {
                    handle_readable(fd);
                }
            }
        }

        std::cout << "[SimulatedProactor] Event loop stopped" << std::endl;
    }

    void stop() {
        running_ = false;
    }

private:
    void set_nonblocking(int fd) {
        int flags = fcntl(fd, F_GETFL, 0);
        fcntl(fd, F_SETFL, flags | O_NONBLOCK);
    }

    void handle_readable(int fd) {
        // 检查是否是accept回调
        AcceptCallback accept_cb;
        {
            std::lock_guard<std::mutex> lock(callbacks_mutex_);
            auto it = accept_callbacks_.find(fd);
            if (it != accept_callbacks_.end()) {
                accept_cb = it->second;
            }
        }

        if (accept_cb) {
            // 处理accept
            int client_fd = accept4(fd, nullptr, nullptr, SOCK_NONBLOCK);
            if (client_fd >= 0) {
                accept_cb(client_fd);
            }
            return;
        }

        // 检查是否是read回调
        ReadCallback read_cb;
        {
            std::lock_guard<std::mutex> lock(callbacks_mutex_);
            auto it = read_callbacks_.find(fd);
            if (it != read_callbacks_.end()) {
                read_cb = it->second;
                read_callbacks_.erase(it);
            }
        }

        if (read_cb) {
            // 在线程池中执行读操作，模拟异步完成
            thread_pool_.enqueue([this, fd, read_cb]() {
                std::vector<char> buffer(4096);
                ssize_t n = read(fd, buffer.data(), buffer.size());

                // 调用回调
                if (n > 0) {
                    read_cb(0, buffer.data(), n);
                } else if (n == 0) {
                    read_cb(0, nullptr, 0);  // EOF
                } else {
                    read_cb(-1, nullptr, 0);  // Error
                }
            });

            // 从epoll移除
            epoll_ctl(epfd_, EPOLL_CTL_DEL, fd, nullptr);
        }
    }

    void process_completions() {
        std::queue<std::function<void()>> to_process;

        {
            std::lock_guard<std::mutex> lock(completions_mutex_);
            std::swap(to_process, completions_);
        }

        while (!to_process.empty()) {
            to_process.front()();
            to_process.pop();
        }
    }

private:
    ThreadPool thread_pool_;
    int epfd_;
    std::atomic<bool> running_;

    std::mutex callbacks_mutex_;
    std::unordered_map<int, ReadCallback> read_callbacks_;
    std::unordered_map<int, AcceptCallback> accept_callbacks_;

    std::mutex completions_mutex_;
    std::queue<std::function<void()>> completions_;
};

} // namespace proactor

/*
模拟Proactor vs 真正Proactor的区别：

┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  模拟Proactor（本实现）：                                                   │
│  • 使用epoll检测I/O就绪                                                    │
│  • 线程池执行实际I/O操作                                                   │
│  • 对用户呈现异步接口                                                      │
│  • 额外线程开销                                                            │
│                                                                             │
│  真正Proactor（IOCP/io_uring）：                                           │
│  • 内核直接执行I/O操作                                                     │
│  • 完成后通知用户                                                          │
│  • 零额外线程开销                                                          │
│  • 更低延迟                                                                │
│                                                                             │
│  Boost.Asio在非Windows平台也使用类似的模拟方式                              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
*/
```

---

### 第一周自测题

#### 概念理解

1. **同步/异步与阻塞/非阻塞的区别是什么？请画出四象限图。**

2. **为什么说I/O多路复用（epoll）是同步I/O？**

3. **Proactor模式的五个核心组件是什么？各自的职责是什么？**

4. **Reactor和Proactor在通知时机上有什么本质区别？**

5. **POSIX AIO有哪些局限性？为什么现代系统不推荐使用？**

#### 编程实践

1. **实现一个简单的POSIX AIO文件读取示例。**

2. **实现一个基于epoll的模拟Proactor。**

3. **对比Reactor和模拟Proactor在相同负载下的性能。**

---

### 第一周检验标准

| 检验项 | 达标要求 | 自评 |
|--------|----------|------|
| 理解I/O模型分类 | 能画出四象限图并解释 | ☐ |
| 理解五种I/O模型 | 能分别解释特点 | ☐ |
| 理解Proactor组件 | 能画出架构图 | ☐ |
| 对比Reactor/Proactor | 能从多维度分析 | ☐ |
| 实现模拟Proactor | 代码能正确运行 | ☐ |
| 了解POSIX AIO | 能解释局限性 | ☐ |

---

### 第一周时间分配

| 内容 | 时间 |
|------|------|
| I/O模型理论学习 | 10小时 |
| Proactor模式组件分析 | 8小时 |
| Reactor vs Proactor对比 | 8小时 |
| 代码实现与实验 | 9小时 |

---

## 第二周：Windows IOCP详解（Day 8-14）

> **本周目标**：深入理解Windows平台上最成熟的Proactor实现——I/O Completion Ports，掌握其核心概念、API使用和高级特性。

```
┌─────────────────────────────────────────────────────────────────┐
│                    第二周学习路线图                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Day 8-9              Day 10-11            Day 12-14           │
│   ┌─────────┐         ┌─────────┐         ┌─────────┐          │
│   │  IOCP   │         │  IOCP   │         │  IOCP   │          │
│   │核心概念 │────────▶│API详解  │────────▶│高级特性 │          │
│   └─────────┘         └─────────┘         └─────────┘          │
│       │                   │                   │                 │
│       ▼                   ▼                   ▼                 │
│   完成端口原理        API函数详解         线程池管理            │
│   重叠I/O机制         WSA异步操作         批量处理             │
│   完成键设计          基础服务器          错误处理             │
│   线程池协作                              性能调优             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Day 8-9：IOCP核心概念（10小时）

#### 8.1 I/O Completion Ports概述

IOCP（I/O Completion Ports）是Windows平台上最高效的异步I/O机制，也是Proactor模式在Windows上的原生实现。它从Windows NT 3.5开始引入，经过多年优化，成为构建高性能服务器的首选方案。

```
IOCP发展历史：
┌─────────────────────────────────────────────────────────────────┐
│                                                                  │
│  Windows NT 3.5 (1994)                                          │
│  └── IOCP首次引入，支持文件I/O                                  │
│       │                                                          │
│       ▼                                                          │
│  Windows NT 4.0 (1996)                                          │
│  └── 完善网络支持，Winsock 2.0集成                              │
│       │                                                          │
│       ▼                                                          │
│  Windows 2000 (2000)                                            │
│  └── 支持更多完成端口，性能优化                                 │
│       │                                                          │
│       ▼                                                          │
│  Windows Vista/Server 2008 (2006)                               │
│  └── GetQueuedCompletionStatusEx批量获取                        │
│       │                                                          │
│       ▼                                                          │
│  Windows 8/Server 2012 (2012)                                   │
│  └── 注册I/O (RIO)，极致性能                                    │
│       │                                                          │
│       ▼                                                          │
│  Windows 10/11 (现在)                                           │
│  └── 持续优化，成为高性能服务器标准                             │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### 8.2 IOCP内部架构

```
IOCP内部架构详解：
┌─────────────────────────────────────────────────────────────────────────┐
│                         I/O Completion Port                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────┐     │
│  │                     设备列表 (Device List)                      │     │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐          │     │
│  │  │ Socket A │ │ Socket B │ │  File C  │ │ Pipe D   │          │     │
│  │  │ Key: 100 │ │ Key: 101 │ │ Key: 102 │ │ Key: 103 │          │     │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘          │     │
│  └───────────────────────────────────────────────────────────────┘     │
│                              │                                          │
│                              │ 异步操作完成                             │
│                              ▼                                          │
│  ┌───────────────────────────────────────────────────────────────┐     │
│  │                   完成队列 (Completion Queue)                   │     │
│  │                        [FIFO队列]                               │     │
│  │  ┌────────────────────────────────────────────────────────┐   │     │
│  │  │ Packet1 │ Packet2 │ Packet3 │ Packet4 │ ... │ PacketN │   │     │
│  │  │ (完成)  │ (完成)  │ (完成)  │ (完成)  │     │ (完成)  │   │     │
│  │  └────────────────────────────────────────────────────────┘   │     │
│  └───────────────────────────────────────────────────────────────┘     │
│                              │                                          │
│                              │ 唤醒等待线程                            │
│                              ▼                                          │
│  ┌───────────────────────────────────────────────────────────────┐     │
│  │                   等待线程队列 (Waiting Thread Queue)          │     │
│  │                        [LIFO栈]                                 │     │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐                       │     │
│  │  │Thread 1  │ │Thread 2  │ │Thread 3  │ ← 后进先出            │     │
│  │  │(waiting) │ │(waiting) │ │(waiting) │                       │     │
│  │  └──────────┘ └──────────┘ └──────────┘                       │     │
│  └───────────────────────────────────────────────────────────────┘     │
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────┐     │
│  │                   活跃线程计数 (Active Thread Count)           │     │
│  │  当前活跃: 2    最大并发: 4 (CPU核心数)                        │     │
│  └───────────────────────────────────────────────────────────────┘     │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

IOCP设计要点：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. 完成队列使用FIFO                                                    │
│     └── 保证I/O完成事件按顺序处理                                       │
│                                                                          │
│  2. 等待线程队列使用LIFO                                                │
│     └── 最近使用的线程优先被唤醒                                        │
│     └── 提高CPU缓存命中率                                               │
│     └── 减少上下文切换                                                  │
│                                                                          │
│  3. 并发控制                                                            │
│     └── 内核自动管理活跃线程数                                          │
│     └── 防止过度并发导致的上下文切换                                    │
│     └── 默认最大并发数 = CPU核心数                                      │
│                                                                          │
│  4. 线程复用                                                            │
│     └── 处理完一个I/O后立即处理下一个                                   │
│     └── 无需创建新线程                                                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 8.3 重叠I/O（Overlapped I/O）机制

重叠I/O是Windows异步I/O的基础，IOCP建立在重叠I/O之上。

```
重叠I/O原理：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  同步I/O流程：                                                          │
│  ┌──────┐                              ┌──────┐                         │
│  │ 应用 │──── read() ──── 阻塞等待 ────│ 内核 │                         │
│  │ 程序 │◀─────────── 数据返回 ────────│      │                         │
│  └──────┘                              └──────┘                         │
│  时间 ──▶ ████████████████████████████████████                         │
│           [    整个过程阻塞，CPU空闲    ]                               │
│                                                                          │
│  重叠I/O流程：                                                          │
│  ┌──────┐                              ┌──────┐                         │
│  │ 应用 │──── ReadFile() + OVERLAPPED ─│ 内核 │                         │
│  │ 程序 │     立即返回                  │      │                         │
│  │      │     继续其他工作...           │      │── 执行I/O操作          │
│  │      │     ...                       │      │                         │
│  │      │◀─── 完成通知（事件/IOCP）────│      │                         │
│  └──────┘                              └──────┘                         │
│  时间 ──▶ ███                    ███                                    │
│           [启动] [做其他事] [处理完成]                                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

OVERLAPPED结构详解：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  typedef struct _OVERLAPPED {                                           │
│      ULONG_PTR  Internal;      // I/O状态码（系统使用）                 │
│      ULONG_PTR  InternalHigh;  // 传输字节数（系统使用）                │
│      union {                                                            │
│          struct {                                                       │
│              DWORD Offset;      // 文件偏移低32位                       │
│              DWORD OffsetHigh;  // 文件偏移高32位                       │
│          };                                                             │
│          PVOID Pointer;         // 指针（很少使用）                     │
│      };                                                                 │
│      HANDLE hEvent;             // 完成事件（可选）                     │
│  } OVERLAPPED;                                                          │
│                                                                          │
│  扩展OVERLAPPED（应用程序使用）：                                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  struct EXTENDED_OVERLAPPED {                                    │   │
│  │      OVERLAPPED    overlapped;   // 必须放在第一个位置           │   │
│  │      // --- 以下为应用程序扩展字段 ---                          │   │
│  │      SOCKET        socket;       // 关联的socket                 │   │
│  │      WSABUF        wsabuf;       // 缓冲区信息                   │   │
│  │      char          buffer[8192]; // 实际数据缓冲区               │   │
│  │      int           operation;    // 操作类型（读/写）            │   │
│  │      void*         user_data;    // 用户自定义数据               │   │
│  │  };                                                              │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  内存布局：                                                             │
│  ┌────────────────────────────────────────────────────────────────┐    │
│  │ EXTENDED_OVERLAPPED                                            │    │
│  ├──────────────┬──────────────────────────────────────────────────┤    │
│  │ OVERLAPPED   │ socket │ wsabuf │ buffer[8192] │ op │ user_data │    │
│  │ (32字节)     │        │        │              │    │           │    │
│  └──────────────┴──────────────────────────────────────────────────┘    │
│  ↑                                                                      │
│  传给系统的指针，完成后可通过CONTAINING_RECORD获取整个结构              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 8.4 完成键（Completion Key）设计

```
完成键设计模式：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  完成键作用：                                                           │
│  - 在创建关联时指定，每次完成事件都会返回                               │
│  - 用于识别是哪个句柄的操作完成                                         │
│  - 可以存储指针，指向连接上下文                                         │
│                                                                          │
│  设计模式1：直接使用句柄值                                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  // 简单但不推荐                                                 │   │
│  │  CreateIoCompletionPort(socket, iocp, (ULONG_PTR)socket, 0);    │   │
│  │                                                                  │   │
│  │  // 完成时                                                       │   │
│  │  SOCKET sock = (SOCKET)completionKey;                           │   │
│  │  // 问题：无法直接获取连接上下文                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  设计模式2：使用连接上下文指针（推荐）                                  │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  struct ConnectionContext {                                      │   │
│  │      SOCKET socket;                                              │   │
│  │      sockaddr_in addr;                                           │   │
│  │      char recv_buffer[8192];                                     │   │
│  │      char send_buffer[8192];                                     │   │
│  │      // 其他连接相关数据...                                      │   │
│  │  };                                                              │   │
│  │                                                                  │   │
│  │  auto ctx = new ConnectionContext();                             │   │
│  │  ctx->socket = client_socket;                                    │   │
│  │  CreateIoCompletionPort(client_socket, iocp,                     │   │
│  │                         (ULONG_PTR)ctx, 0);                      │   │
│  │                                                                  │   │
│  │  // 完成时                                                       │   │
│  │  auto ctx = (ConnectionContext*)completionKey;                   │   │
│  │  ctx->socket;  // 直接访问socket                                 │   │
│  │  ctx->addr;    // 直接访问地址                                   │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  设计模式3：使用索引（适合固定连接池）                                  │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  ConnectionContext connections[MAX_CONNECTIONS];                 │   │
│  │                                                                  │   │
│  │  int index = allocate_connection_slot();                         │   │
│  │  connections[index].socket = client_socket;                      │   │
│  │  CreateIoCompletionPort(client_socket, iocp,                     │   │
│  │                         (ULONG_PTR)index, 0);                    │   │
│  │                                                                  │   │
│  │  // 完成时                                                       │   │
│  │  int index = (int)completionKey;                                 │   │
│  │  ConnectionContext& ctx = connections[index];                    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 8.5 线程池与IOCP的协作

```
IOCP线程池模型：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  传统线程池模型（Thread-per-Connection）：                              │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                  │   │
│  │  Connection 1 ────▶ Thread 1  ████████░░░░░░░░████████          │   │
│  │  Connection 2 ────▶ Thread 2  ░░░░████████░░░░░░░░████          │   │
│  │  Connection 3 ────▶ Thread 3  ████░░░░░░░░████████░░░░          │   │
│  │  Connection 4 ────▶ Thread 4  ░░░░░░░░████████░░░░████          │   │
│  │       ...               ...                                      │   │
│  │  Connection N ────▶ Thread N  ████░░░░████░░░░████░░░░          │   │
│  │                                                                  │   │
│  │  问题：N个连接需要N个线程，大量上下文切换                       │   │
│  │        ████ = 工作    ░░░░ = 等待I/O（浪费）                    │   │
│  │                                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  IOCP线程池模型：                                                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                  │   │
│  │  10000 Connections                                               │   │
│  │       │                                                          │   │
│  │       ▼                                                          │   │
│  │  ┌─────────────────────────────────────────────┐                │   │
│  │  │              IOCP (完成端口)                 │                │   │
│  │  │  ┌─────────────────────────────────────┐   │                │   │
│  │  │  │ 完成队列: [E1][E2][E3][E4][E5]...   │   │                │   │
│  │  │  └─────────────────────────────────────┘   │                │   │
│  │  └─────────────────────────────────────────────┘                │   │
│  │       │                                                          │   │
│  │       ▼                                                          │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐               │   │
│  │  │Thread 1 │ │Thread 2 │ │Thread 3 │ │Thread 4 │  (4个CPU核心) │   │
│  │  │████████ │ │████████ │ │████████ │ │████████ │               │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘               │   │
│  │                                                                  │   │
│  │  优点：4个线程处理10000连接，几乎无等待，高效利用CPU            │   │
│  │        ████ = 持续工作                                          │   │
│  │                                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

线程数量选择：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  推荐公式：线程数 = CPU核心数 × 2                                       │
│                                                                          │
│  原因分析：                                                             │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                  │   │
│  │  情况1：纯CPU计算任务                                           │   │
│  │  └── 线程数 = CPU核心数                                         │   │
│  │  └── 更多线程只会增加上下文切换开销                             │   │
│  │                                                                  │   │
│  │  情况2：I/O密集任务（IOCP场景）                                 │   │
│  │  └── 某些线程可能阻塞在：                                       │   │
│  │      - 数据库查询                                                │   │
│  │      - 文件操作                                                  │   │
│  │      - 其他同步调用                                              │   │
│  │  └── 需要额外线程保持并发度                                     │   │
│  │  └── 线程数 = CPU核心数 × 2 通常是好的起点                      │   │
│  │                                                                  │   │
│  │  情况3：高度异步任务                                            │   │
│  │  └── 几乎没有阻塞操作                                           │   │
│  │  └── 线程数 = CPU核心数 可能就够了                              │   │
│  │                                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  IOCP并发数设置：                                                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │  // 创建IOCP时指定的NumberOfConcurrentThreads                   │   │
│  │  CreateIoCompletionPort(INVALID_HANDLE_VALUE, NULL, 0,          │   │
│  │                         NumberOfConcurrentThreads);             │   │
│  │                                                                  │   │
│  │  // 如果设为0，系统自动使用CPU核心数                            │   │
│  │  // 这是内核允许同时活跃的最大线程数                            │   │
│  │  // 实际创建的线程可以更多，但活跃的不会超过这个数              │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Day 10-11：IOCP API详解（10小时）

#### 10.1 CreateIoCompletionPort详解

```cpp
// iocp_api.hpp - IOCP API详解与封装
#pragma once

#ifdef _WIN32

#include <winsock2.h>
#include <ws2tcpip.h>
#include <windows.h>
#include <mswsock.h>
#include <iostream>
#include <vector>
#include <memory>
#include <functional>
#include <thread>
#include <atomic>

#pragma comment(lib, "ws2_32.lib")

namespace iocp {

/*
 * CreateIoCompletionPort API详解
 *
 * 函数原型：
 * HANDLE CreateIoCompletionPort(
 *     HANDLE    FileHandle,           // 要关联的句柄（文件/socket）
 *     HANDLE    ExistingCompletionPort,// 现有的完成端口（或NULL）
 *     ULONG_PTR CompletionKey,         // 完成键（用户数据）
 *     DWORD     NumberOfConcurrentThreads // 最大并发线程数
 * );
 *
 * 两种使用模式：
 *
 * 模式1：创建新的完成端口
 * ┌────────────────────────────────────────────────────────────────────┐
 * │ HANDLE iocp = CreateIoCompletionPort(                             │
 * │     INVALID_HANDLE_VALUE,  // 不关联任何句柄                      │
 * │     NULL,                  // 创建新的完成端口                    │
 * │     0,                     // 完成键（此时无意义）                │
 * │     0                      // 0表示使用CPU核心数                  │
 * │ );                                                                 │
 * └────────────────────────────────────────────────────────────────────┘
 *
 * 模式2：将句柄关联到现有完成端口
 * ┌────────────────────────────────────────────────────────────────────┐
 * │ CreateIoCompletionPort(                                           │
 * │     socket,               // 要关联的socket                        │
 * │     iocp,                 // 现有的完成端口                        │
 * │     (ULONG_PTR)context,   // 完成键（通常是连接上下文指针）       │
 * │     0                     // 此参数被忽略                          │
 * │ );                                                                 │
 * └────────────────────────────────────────────────────────────────────┘
 */

// 完成端口封装类
class CompletionPort {
public:
    CompletionPort(DWORD concurrent_threads = 0) {
        handle_ = CreateIoCompletionPort(
            INVALID_HANDLE_VALUE,
            NULL,
            0,
            concurrent_threads  // 0 = CPU核心数
        );

        if (handle_ == NULL) {
            throw std::runtime_error("CreateIoCompletionPort failed: " +
                                   std::to_string(GetLastError()));
        }

        std::cout << "[IOCP] 创建完成端口成功，句柄: " << handle_ << std::endl;
    }

    ~CompletionPort() {
        if (handle_ != NULL) {
            CloseHandle(handle_);
            std::cout << "[IOCP] 关闭完成端口" << std::endl;
        }
    }

    // 关联句柄到完成端口
    bool associate(HANDLE file_handle, ULONG_PTR completion_key) {
        HANDLE result = CreateIoCompletionPort(
            file_handle,
            handle_,
            completion_key,
            0  // 此参数在关联时被忽略
        );

        if (result == NULL) {
            std::cerr << "[IOCP] 关联句柄失败: " << GetLastError() << std::endl;
            return false;
        }

        std::cout << "[IOCP] 关联句柄成功，Key: " << completion_key << std::endl;
        return true;
    }

    // 关联socket的便捷方法
    bool associate_socket(SOCKET socket, ULONG_PTR completion_key) {
        return associate((HANDLE)socket, completion_key);
    }

    HANDLE handle() const { return handle_; }

private:
    HANDLE handle_ = NULL;
};

/*
 * GetQueuedCompletionStatus API详解
 *
 * 函数原型：
 * BOOL GetQueuedCompletionStatus(
 *     HANDLE       CompletionPort,      // 完成端口句柄
 *     LPDWORD      lpNumberOfBytes,     // [out] 传输的字节数
 *     PULONG_PTR   lpCompletionKey,     // [out] 完成键
 *     LPOVERLAPPED *lpOverlapped,       // [out] OVERLAPPED指针
 *     DWORD        dwMilliseconds       // 超时时间（INFINITE=无限等待）
 * );
 *
 * 返回值分析：
 * ┌──────────────────────────────────────────────────────────────────────┐
 * │ 返回值  │ lpOverlapped │ 含义                                       │
 * ├─────────┼──────────────┼────────────────────────────────────────────┤
 * │ TRUE    │ 非NULL       │ I/O操作成功完成                            │
 * │ FALSE   │ 非NULL       │ I/O操作完成但失败，调用GetLastError()      │
 * │ FALSE   │ NULL         │ 超时或完成端口关闭                         │
 * └──────────────────────────────────────────────────────────────────────┘
 */

// 完成事件结构
struct CompletionEvent {
    bool success;
    DWORD bytes_transferred;
    ULONG_PTR completion_key;
    OVERLAPPED* overlapped;
    DWORD error_code;
};

// 等待完成事件
inline CompletionEvent wait_completion(HANDLE iocp, DWORD timeout = INFINITE) {
    CompletionEvent event = {};

    event.success = GetQueuedCompletionStatus(
        iocp,
        &event.bytes_transferred,
        &event.completion_key,
        &event.overlapped,
        timeout
    );

    if (!event.success) {
        event.error_code = GetLastError();
    }

    return event;
}

/*
 * GetQueuedCompletionStatusEx - 批量获取完成事件（Vista+）
 *
 * 函数原型：
 * BOOL GetQueuedCompletionStatusEx(
 *     HANDLE             CompletionPort,       // 完成端口句柄
 *     LPOVERLAPPED_ENTRY lpCompletionPortEntries, // [out] 完成事件数组
 *     ULONG              ulCount,              // 数组大小
 *     PULONG             ulNumEntriesRemoved,  // [out] 实际获取数量
 *     DWORD              dwMilliseconds,       // 超时时间
 *     BOOL               fAlertable            // 是否可警告
 * );
 *
 * OVERLAPPED_ENTRY结构：
 * typedef struct _OVERLAPPED_ENTRY {
 *     ULONG_PTR    lpCompletionKey;
 *     LPOVERLAPPED lpOverlapped;
 *     ULONG_PTR    Internal;
 *     DWORD        dwNumberOfBytesTransferred;
 * } OVERLAPPED_ENTRY;
 */

// 批量获取完成事件
inline std::vector<OVERLAPPED_ENTRY> wait_completions(
    HANDLE iocp,
    size_t max_entries,
    DWORD timeout = INFINITE
) {
    std::vector<OVERLAPPED_ENTRY> entries(max_entries);
    ULONG num_entries = 0;

    BOOL success = GetQueuedCompletionStatusEx(
        iocp,
        entries.data(),
        static_cast<ULONG>(max_entries),
        &num_entries,
        timeout,
        FALSE  // 不可警告
    );

    if (success && num_entries > 0) {
        entries.resize(num_entries);
        std::cout << "[IOCP] 批量获取 " << num_entries << " 个完成事件" << std::endl;
    } else {
        entries.clear();
    }

    return entries;
}

/*
 * PostQueuedCompletionStatus - 手动投递完成事件
 *
 * 用途：
 * 1. 通知工作线程退出
 * 2. 自定义事件投递
 * 3. 实现定时器等功能
 *
 * 函数原型：
 * BOOL PostQueuedCompletionStatus(
 *     HANDLE       CompletionPort,  // 完成端口句柄
 *     DWORD        dwNumberOfBytesTransferred, // 传输字节数
 *     ULONG_PTR    dwCompletionKey, // 完成键
 *     LPOVERLAPPED lpOverlapped     // OVERLAPPED指针
 * );
 */

// 投递自定义完成事件
inline bool post_completion(
    HANDLE iocp,
    DWORD bytes,
    ULONG_PTR key,
    OVERLAPPED* overlapped
) {
    return PostQueuedCompletionStatus(iocp, bytes, key, overlapped) != FALSE;
}

// 投递退出通知
inline bool post_quit(HANDLE iocp) {
    // 使用特殊的完成键（0）和NULL overlapped表示退出
    return post_completion(iocp, 0, 0, nullptr);
}

} // namespace iocp

#endif // _WIN32
```

#### 10.2 WSARecv/WSASend异步操作

```cpp
// iocp_operations.hpp - 异步I/O操作封装
#pragma once

#ifdef _WIN32

#include "iocp_api.hpp"
#include <string>
#include <cstring>

namespace iocp {

// 操作类型枚举
enum class OperationType {
    ACCEPT,
    RECV,
    SEND,
    DISCONNECT
};

// 扩展OVERLAPPED结构
struct IOContext : public OVERLAPPED {
    OperationType op_type;      // 操作类型
    SOCKET socket;              // 关联的socket
    WSABUF wsabuf;             // 缓冲区描述
    char buffer[8192];         // 实际缓冲区
    DWORD bytes_transferred;   // 传输字节数
    DWORD flags;               // 标志

    // 连接信息（用于accept）
    SOCKET accept_socket;
    char accept_buffer[2 * (sizeof(sockaddr_in) + 16)];

    IOContext() {
        reset();
    }

    void reset() {
        // 清零OVERLAPPED部分
        Internal = 0;
        InternalHigh = 0;
        Offset = 0;
        OffsetHigh = 0;
        hEvent = NULL;

        // 清零扩展字段
        op_type = OperationType::RECV;
        socket = INVALID_SOCKET;
        std::memset(buffer, 0, sizeof(buffer));
        wsabuf.buf = buffer;
        wsabuf.len = sizeof(buffer);
        bytes_transferred = 0;
        flags = 0;
        accept_socket = INVALID_SOCKET;
        std::memset(accept_buffer, 0, sizeof(accept_buffer));
    }

    // 设置为接收操作
    void setup_recv(SOCKET sock) {
        reset();
        op_type = OperationType::RECV;
        socket = sock;
        wsabuf.buf = buffer;
        wsabuf.len = sizeof(buffer);
    }

    // 设置为发送操作
    void setup_send(SOCKET sock, const char* data, size_t len) {
        reset();
        op_type = OperationType::SEND;
        socket = sock;
        size_t copy_len = std::min(len, sizeof(buffer));
        std::memcpy(buffer, data, copy_len);
        wsabuf.buf = buffer;
        wsabuf.len = static_cast<ULONG>(copy_len);
    }

    // 设置为发送操作（字符串版本）
    void setup_send(SOCKET sock, const std::string& data) {
        setup_send(sock, data.c_str(), data.length());
    }
};

/*
 * WSARecv - 异步接收数据
 *
 * 函数原型：
 * int WSARecv(
 *     SOCKET          s,             // socket
 *     LPWSABUF        lpBuffers,     // 缓冲区数组
 *     DWORD           dwBufferCount, // 缓冲区数量
 *     LPDWORD         lpNumberOfBytesRecvd, // [out] 接收字节数
 *     LPDWORD         lpFlags,       // [in/out] 标志
 *     LPWSAOVERLAPPED lpOverlapped,  // OVERLAPPED结构
 *     LPWSAOVERLAPPED_COMPLETION_ROUTINE lpCompletionRoutine // 完成回调
 * );
 *
 * 返回值：
 * - 0: 操作立即完成（数据已在缓冲区）
 * - SOCKET_ERROR + WSA_IO_PENDING: 操作已投递，将异步完成
 * - SOCKET_ERROR + 其他错误: 操作失败
 */

// 投递异步接收
inline bool async_recv(IOContext* ctx) {
    ctx->flags = 0;
    ctx->bytes_transferred = 0;

    int result = WSARecv(
        ctx->socket,
        &ctx->wsabuf,
        1,                    // 缓冲区数量
        &ctx->bytes_transferred,
        &ctx->flags,
        ctx,                  // OVERLAPPED* (IOContext继承自OVERLAPPED)
        NULL                  // 不使用完成回调
    );

    if (result == SOCKET_ERROR) {
        int error = WSAGetLastError();
        if (error != WSA_IO_PENDING) {
            std::cerr << "[IOCP] WSARecv失败: " << error << std::endl;
            return false;
        }
        // WSA_IO_PENDING表示操作已投递，将异步完成
    }

    return true;
}

/*
 * WSASend - 异步发送数据
 *
 * 函数原型：
 * int WSASend(
 *     SOCKET          s,
 *     LPWSABUF        lpBuffers,
 *     DWORD           dwBufferCount,
 *     LPDWORD         lpNumberOfBytesSent,
 *     DWORD           dwFlags,
 *     LPWSAOVERLAPPED lpOverlapped,
 *     LPWSAOVERLAPPED_COMPLETION_ROUTINE lpCompletionRoutine
 * );
 */

// 投递异步发送
inline bool async_send(IOContext* ctx) {
    DWORD bytes_sent = 0;

    int result = WSASend(
        ctx->socket,
        &ctx->wsabuf,
        1,
        &bytes_sent,
        0,                    // 标志
        ctx,
        NULL
    );

    if (result == SOCKET_ERROR) {
        int error = WSAGetLastError();
        if (error != WSA_IO_PENDING) {
            std::cerr << "[IOCP] WSASend失败: " << error << std::endl;
            return false;
        }
    }

    return true;
}

/*
 * AcceptEx - 异步Accept
 *
 * 需要使用WSAIoctl获取函数指针（为了跨DLL边界）
 *
 * 函数原型：
 * BOOL AcceptEx(
 *     SOCKET       sListenSocket,    // 监听socket
 *     SOCKET       sAcceptSocket,    // 预创建的accept socket
 *     PVOID        lpOutputBuffer,   // 接收缓冲区
 *     DWORD        dwReceiveDataLength, // 接收数据长度
 *     DWORD        dwLocalAddressLength, // 本地地址长度
 *     DWORD        dwRemoteAddressLength, // 远程地址长度
 *     LPDWORD      lpdwBytesReceived, // [out] 接收字节数
 *     LPOVERLAPPED lpOverlapped      // OVERLAPPED
 * );
 */

// AcceptEx函数指针类型
typedef BOOL (WINAPI *LPFN_ACCEPTEX)(
    SOCKET, SOCKET, PVOID, DWORD, DWORD, DWORD, LPDWORD, LPOVERLAPPED
);

// 获取AcceptEx函数指针
inline LPFN_ACCEPTEX get_acceptex_func(SOCKET listen_socket) {
    LPFN_ACCEPTEX lpfnAcceptEx = NULL;
    GUID GuidAcceptEx = WSAID_ACCEPTEX;
    DWORD dwBytes = 0;

    WSAIoctl(
        listen_socket,
        SIO_GET_EXTENSION_FUNCTION_POINTER,
        &GuidAcceptEx,
        sizeof(GuidAcceptEx),
        &lpfnAcceptEx,
        sizeof(lpfnAcceptEx),
        &dwBytes,
        NULL,
        NULL
    );

    return lpfnAcceptEx;
}

// 投递异步Accept
inline bool async_accept(
    SOCKET listen_socket,
    IOContext* ctx,
    LPFN_ACCEPTEX acceptex_func
) {
    // 预创建accept socket
    ctx->accept_socket = WSASocketW(
        AF_INET,
        SOCK_STREAM,
        IPPROTO_TCP,
        NULL,
        0,
        WSA_FLAG_OVERLAPPED
    );

    if (ctx->accept_socket == INVALID_SOCKET) {
        std::cerr << "[IOCP] 创建accept socket失败" << std::endl;
        return false;
    }

    ctx->op_type = OperationType::ACCEPT;
    ctx->socket = listen_socket;

    DWORD bytes_received = 0;
    BOOL result = acceptex_func(
        listen_socket,
        ctx->accept_socket,
        ctx->accept_buffer,
        0,  // 不接收初始数据
        sizeof(sockaddr_in) + 16,
        sizeof(sockaddr_in) + 16,
        &bytes_received,
        ctx
    );

    if (!result) {
        int error = WSAGetLastError();
        if (error != ERROR_IO_PENDING) {
            closesocket(ctx->accept_socket);
            ctx->accept_socket = INVALID_SOCKET;
            std::cerr << "[IOCP] AcceptEx失败: " << error << std::endl;
            return false;
        }
    }

    return true;
}

} // namespace iocp

#endif // _WIN32
```

#### 10.3 IOCP基础服务器

```cpp
// iocp_server_basic.hpp - IOCP基础服务器实现
#pragma once

#ifdef _WIN32

#include "iocp_operations.hpp"
#include <unordered_map>
#include <mutex>
#include <queue>

namespace iocp {

// 连接上下文
struct Connection {
    SOCKET socket;
    sockaddr_in addr;
    IOContext recv_ctx;
    IOContext send_ctx;
    std::queue<std::string> send_queue;  // 发送队列
    bool sending;  // 是否正在发送

    Connection(SOCKET sock) : socket(sock), sending(false) {
        std::memset(&addr, 0, sizeof(addr));
        recv_ctx.socket = sock;
        send_ctx.socket = sock;
    }

    ~Connection() {
        if (socket != INVALID_SOCKET) {
            closesocket(socket);
        }
    }
};

// IOCP服务器
class IOCPServer {
public:
    IOCPServer(uint16_t port, size_t worker_threads = 0)
        : port_(port)
        , running_(false)
    {
        // 初始化Winsock
        WSADATA wsaData;
        if (WSAStartup(MAKEWORD(2, 2), &wsaData) != 0) {
            throw std::runtime_error("WSAStartup failed");
        }

        // 创建完成端口
        iocp_ = std::make_unique<CompletionPort>(0);

        // 确定工作线程数
        if (worker_threads == 0) {
            SYSTEM_INFO sysInfo;
            GetSystemInfo(&sysInfo);
            worker_threads = sysInfo.dwNumberOfProcessors * 2;
        }
        num_workers_ = worker_threads;

        std::cout << "[Server] 工作线程数: " << num_workers_ << std::endl;
    }

    ~IOCPServer() {
        stop();
        WSACleanup();
    }

    bool start() {
        // 创建监听socket
        listen_socket_ = WSASocketW(
            AF_INET,
            SOCK_STREAM,
            IPPROTO_TCP,
            NULL,
            0,
            WSA_FLAG_OVERLAPPED
        );

        if (listen_socket_ == INVALID_SOCKET) {
            std::cerr << "[Server] 创建监听socket失败" << std::endl;
            return false;
        }

        // 绑定地址
        sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_addr.s_addr = INADDR_ANY;
        addr.sin_port = htons(port_);

        if (bind(listen_socket_, (sockaddr*)&addr, sizeof(addr)) == SOCKET_ERROR) {
            std::cerr << "[Server] bind失败" << std::endl;
            return false;
        }

        // 开始监听
        if (listen(listen_socket_, SOMAXCONN) == SOCKET_ERROR) {
            std::cerr << "[Server] listen失败" << std::endl;
            return false;
        }

        // 关联监听socket到IOCP
        iocp_->associate_socket(listen_socket_, 0);

        // 获取AcceptEx函数
        acceptex_func_ = get_acceptex_func(listen_socket_);
        if (!acceptex_func_) {
            std::cerr << "[Server] 获取AcceptEx失败" << std::endl;
            return false;
        }

        running_ = true;

        // 启动工作线程
        for (size_t i = 0; i < num_workers_; ++i) {
            workers_.emplace_back(&IOCPServer::worker_thread, this, i);
        }

        // 投递初始Accept请求
        for (int i = 0; i < 10; ++i) {  // 预投递10个Accept
            post_accept();
        }

        std::cout << "[Server] 服务器启动，监听端口 " << port_ << std::endl;
        return true;
    }

    void stop() {
        if (!running_) return;

        running_ = false;

        // 投递退出通知给所有工作线程
        for (size_t i = 0; i < num_workers_; ++i) {
            post_quit(iocp_->handle());
        }

        // 等待工作线程结束
        for (auto& t : workers_) {
            if (t.joinable()) {
                t.join();
            }
        }
        workers_.clear();

        // 关闭监听socket
        if (listen_socket_ != INVALID_SOCKET) {
            closesocket(listen_socket_);
            listen_socket_ = INVALID_SOCKET;
        }

        // 关闭所有连接
        std::lock_guard<std::mutex> lock(conn_mutex_);
        connections_.clear();

        std::cout << "[Server] 服务器已停止" << std::endl;
    }

    void wait() {
        for (auto& t : workers_) {
            if (t.joinable()) {
                t.join();
            }
        }
    }

protected:
    // 虚函数：处理接收到的数据
    virtual void on_receive(Connection* conn, const char* data, size_t len) {
        // 默认实现：Echo
        std::string msg(data, len);
        async_send_to(conn, msg);
    }

    // 虚函数：新连接建立
    virtual void on_connect(Connection* conn) {
        std::cout << "[Server] 新连接: " << inet_ntoa(conn->addr.sin_addr)
                  << ":" << ntohs(conn->addr.sin_port) << std::endl;
    }

    // 虚函数：连接断开
    virtual void on_disconnect(Connection* conn) {
        std::cout << "[Server] 连接断开: " << inet_ntoa(conn->addr.sin_addr)
                  << ":" << ntohs(conn->addr.sin_port) << std::endl;
    }

private:
    void worker_thread(size_t worker_id) {
        std::cout << "[Worker " << worker_id << "] 启动" << std::endl;

        while (running_) {
            CompletionEvent event = wait_completion(iocp_->handle(), 1000);

            // 检查是否是退出通知
            if (event.completion_key == 0 && event.overlapped == nullptr) {
                if (!event.success && event.error_code == WAIT_TIMEOUT) {
                    continue;  // 超时，继续等待
                }
                std::cout << "[Worker " << worker_id << "] 收到退出通知" << std::endl;
                break;
            }

            // 处理完成事件
            IOContext* ctx = static_cast<IOContext*>(event.overlapped);

            if (ctx->op_type == OperationType::ACCEPT) {
                handle_accept(ctx, event);
            } else {
                Connection* conn = reinterpret_cast<Connection*>(event.completion_key);

                if (!event.success || event.bytes_transferred == 0) {
                    // 连接断开或错误
                    handle_disconnect(conn);
                } else if (ctx->op_type == OperationType::RECV) {
                    handle_recv(conn, ctx, event.bytes_transferred);
                } else if (ctx->op_type == OperationType::SEND) {
                    handle_send(conn, ctx, event.bytes_transferred);
                }
            }
        }

        std::cout << "[Worker " << worker_id << "] 退出" << std::endl;
    }

    void post_accept() {
        auto ctx = new IOContext();
        if (!async_accept(listen_socket_, ctx, acceptex_func_)) {
            delete ctx;
        }
    }

    void handle_accept(IOContext* ctx, const CompletionEvent& event) {
        if (!event.success) {
            closesocket(ctx->accept_socket);
            delete ctx;
            post_accept();  // 重新投递Accept
            return;
        }

        SOCKET client_socket = ctx->accept_socket;

        // 设置socket选项（继承监听socket的属性）
        setsockopt(client_socket, SOL_SOCKET, SO_UPDATE_ACCEPT_CONTEXT,
                   (char*)&listen_socket_, sizeof(listen_socket_));

        // 创建连接上下文
        auto conn = std::make_shared<Connection>(client_socket);

        // 获取客户端地址
        sockaddr_in* local_addr = nullptr;
        sockaddr_in* remote_addr = nullptr;
        int local_len = sizeof(sockaddr_in);
        int remote_len = sizeof(sockaddr_in);

        // GetAcceptExSockaddrs解析地址（需要动态获取函数指针）
        LPFN_GETACCEPTEXSOCKADDRS lpfnGetAcceptExSockaddrs = nullptr;
        GUID GuidGetAcceptExSockaddrs = WSAID_GETACCEPTEXSOCKADDRS;
        DWORD dwBytes = 0;
        WSAIoctl(listen_socket_, SIO_GET_EXTENSION_FUNCTION_POINTER,
                 &GuidGetAcceptExSockaddrs, sizeof(GuidGetAcceptExSockaddrs),
                 &lpfnGetAcceptExSockaddrs, sizeof(lpfnGetAcceptExSockaddrs),
                 &dwBytes, NULL, NULL);

        if (lpfnGetAcceptExSockaddrs) {
            lpfnGetAcceptExSockaddrs(
                ctx->accept_buffer,
                0,
                sizeof(sockaddr_in) + 16,
                sizeof(sockaddr_in) + 16,
                (sockaddr**)&local_addr,
                &local_len,
                (sockaddr**)&remote_addr,
                &remote_len
            );

            if (remote_addr) {
                conn->addr = *remote_addr;
            }
        }

        // 关联到IOCP
        iocp_->associate_socket(client_socket, reinterpret_cast<ULONG_PTR>(conn.get()));

        // 保存连接
        {
            std::lock_guard<std::mutex> lock(conn_mutex_);
            connections_[client_socket] = conn;
        }

        // 通知连接建立
        on_connect(conn.get());

        // 投递接收请求
        conn->recv_ctx.setup_recv(client_socket);
        async_recv(&conn->recv_ctx);

        // 释放Accept上下文，投递新的Accept
        delete ctx;
        post_accept();
    }

    void handle_recv(Connection* conn, IOContext* ctx, DWORD bytes_transferred) {
        // 调用虚函数处理数据
        on_receive(conn, ctx->buffer, bytes_transferred);

        // 继续投递接收请求
        ctx->setup_recv(conn->socket);
        if (!async_recv(ctx)) {
            handle_disconnect(conn);
        }
    }

    void handle_send(Connection* conn, IOContext* ctx, DWORD bytes_transferred) {
        std::lock_guard<std::mutex> lock(conn_mutex_);

        // 检查是否有更多数据要发送
        if (!conn->send_queue.empty()) {
            std::string data = std::move(conn->send_queue.front());
            conn->send_queue.pop();

            ctx->setup_send(conn->socket, data);
            if (!async_send(ctx)) {
                conn->sending = false;
                // 发送失败，不断开连接，等待下次发送
            }
        } else {
            conn->sending = false;
        }
    }

    void handle_disconnect(Connection* conn) {
        on_disconnect(conn);

        std::lock_guard<std::mutex> lock(conn_mutex_);
        connections_.erase(conn->socket);
    }

    void async_send_to(Connection* conn, const std::string& data) {
        std::lock_guard<std::mutex> lock(conn_mutex_);

        if (conn->sending) {
            // 正在发送，加入队列
            conn->send_queue.push(data);
        } else {
            // 立即发送
            conn->sending = true;
            conn->send_ctx.setup_send(conn->socket, data);
            if (!async_send(&conn->send_ctx)) {
                conn->sending = false;
            }
        }
    }

private:
    uint16_t port_;
    SOCKET listen_socket_ = INVALID_SOCKET;
    std::unique_ptr<CompletionPort> iocp_;
    LPFN_ACCEPTEX acceptex_func_ = nullptr;

    std::vector<std::thread> workers_;
    size_t num_workers_;
    std::atomic<bool> running_;

    std::mutex conn_mutex_;
    std::unordered_map<SOCKET, std::shared_ptr<Connection>> connections_;
};

} // namespace iocp

#endif // _WIN32
```

### Day 12-14：IOCP高级特性（15小时）

#### 12.1 I/O线程池自动管理

```
IOCP线程池自动管理机制：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  IOCP内核级线程管理：                                                   │
│                                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                        IOCP内核对象                                │ │
│  │                                                                    │ │
│  │  活跃线程计数器: [  2  ]  最大并发数: [  4  ] (CPU核心数)        │ │
│  │                                                                    │ │
│  │  当活跃线程 < 最大并发数时：                                      │ │
│  │    → 唤醒等待队列中的线程                                         │ │
│  │                                                                    │ │
│  │  当活跃线程 >= 最大并发数时：                                     │ │
│  │    → 新完成的事件排队等待                                         │ │
│  │    → 等待线程保持睡眠                                             │ │
│  │                                                                    │ │
│  │  当活跃线程阻塞（如数据库调用）时：                               │ │
│  │    → 内核自动将其标记为非活跃                                     │ │
│  │    → 唤醒另一个等待线程                                           │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  示例场景（4核CPU，8个工作线程）：                                      │
│                                                                          │
│  时刻T1: 4个线程处理I/O                                                 │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐             │
│  │ T1 │ │ T2 │ │ T3 │ │ T4 │ │ T5 │ │ T6 │ │ T7 │ │ T8 │             │
│  │活跃│ │活跃│ │活跃│ │活跃│ │等待│ │等待│ │等待│ │等待│             │
│  └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘             │
│                                                                          │
│  时刻T2: T2阻塞在数据库调用，T5被唤醒                                  │
│  ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐ ┌────┐             │
│  │ T1 │ │ T2 │ │ T3 │ │ T4 │ │ T5 │ │ T6 │ │ T7 │ │ T8 │             │
│  │活跃│ │阻塞│ │活跃│ │活跃│ │活跃│ │等待│ │等待│ │等待│             │
│  └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘ └────┘             │
│          ↓                      ↑                                       │
│          └──── 内核自动调度 ────┘                                       │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 12.2 批量完成事件处理

```cpp
// iocp_batch.hpp - 批量处理完成事件
#pragma once

#ifdef _WIN32

#include "iocp_api.hpp"
#include <chrono>

namespace iocp {

/*
 * 批量获取完成事件的优势：
 *
 * 单次获取 vs 批量获取：
 * ┌─────────────────────────────────────────────────────────────────┐
 * │                                                                  │
 * │  单次获取（GetQueuedCompletionStatus）：                        │
 * │  ┌──────┐     ┌──────┐     ┌──────┐     ┌──────┐               │
 * │  │ 获取 │────▶│ 处理 │────▶│ 获取 │────▶│ 处理 │               │
 * │  │ 事件1│     │ 事件1│     │ 事件2│     │ 事件2│               │
 * │  └──────┘     └──────┘     └──────┘     └──────┘               │
 * │  系统调用      用户态       系统调用      用户态                 │
 * │                                                                  │
 * │  批量获取（GetQueuedCompletionStatusEx）：                      │
 * │  ┌────────────────┐     ┌─────────────────────────┐            │
 * │  │ 批量获取       │────▶│ 批量处理                │            │
 * │  │ 事件1,2,3,4... │     │ 事件1,2,3,4...         │            │
 * │  └────────────────┘     └─────────────────────────┘            │
 * │  一次系统调用            多次用户态处理                         │
 * │                                                                  │
 * │  优势：                                                         │
 * │  1. 减少用户态/内核态切换                                       │
 * │  2. 更好的CPU缓存利用                                           │
 * │  3. 适合高吞吐量场景                                            │
 * │                                                                  │
 * └─────────────────────────────────────────────────────────────────┘
 */

// 批量处理器
class BatchProcessor {
public:
    static constexpr size_t MAX_BATCH_SIZE = 64;

    struct Stats {
        uint64_t total_events = 0;
        uint64_t total_batches = 0;
        uint64_t single_events = 0;   // 批大小为1的次数
        uint64_t full_batches = 0;    // 满批次数
        double avg_batch_size = 0.0;
    };

    BatchProcessor(HANDLE iocp) : iocp_(iocp) {}

    // 批量获取并处理
    template<typename Handler>
    size_t process_batch(Handler&& handler, DWORD timeout = 0) {
        OVERLAPPED_ENTRY entries[MAX_BATCH_SIZE];
        ULONG num_entries = 0;

        BOOL success = GetQueuedCompletionStatusEx(
            iocp_,
            entries,
            MAX_BATCH_SIZE,
            &num_entries,
            timeout,
            FALSE
        );

        if (!success || num_entries == 0) {
            return 0;
        }

        // 更新统计
        stats_.total_events += num_entries;
        stats_.total_batches++;
        if (num_entries == 1) stats_.single_events++;
        if (num_entries == MAX_BATCH_SIZE) stats_.full_batches++;
        stats_.avg_batch_size = static_cast<double>(stats_.total_events) /
                                stats_.total_batches;

        // 处理每个事件
        for (ULONG i = 0; i < num_entries; ++i) {
            handler(
                entries[i].lpCompletionKey,
                entries[i].lpOverlapped,
                entries[i].dwNumberOfBytesTransferred
            );
        }

        return num_entries;
    }

    const Stats& stats() const { return stats_; }

    void print_stats() const {
        std::cout << "\n[Batch Stats]\n"
                  << "  Total events: " << stats_.total_events << "\n"
                  << "  Total batches: " << stats_.total_batches << "\n"
                  << "  Single events: " << stats_.single_events << "\n"
                  << "  Full batches: " << stats_.full_batches << "\n"
                  << "  Avg batch size: " << stats_.avg_batch_size << "\n";
    }

private:
    HANDLE iocp_;
    Stats stats_;
};

/*
 * 性能对比示例（伪代码）：
 *
 * 测试条件：100万次I/O完成事件
 *
 * 单次获取：
 * - 系统调用次数：1,000,000
 * - 耗时：~2.5秒
 *
 * 批量获取（batch=64）：
 * - 系统调用次数：~15,625
 * - 耗时：~0.8秒
 *
 * 性能提升：约3倍
 */

} // namespace iocp

#endif // _WIN32
```

#### 12.3 错误处理与资源回收

```cpp
// iocp_error_handling.hpp - 错误处理与资源管理
#pragma once

#ifdef _WIN32

#include "iocp_operations.hpp"
#include <string>
#include <sstream>

namespace iocp {

/*
 * IOCP错误处理策略：
 *
 * ┌─────────────────────────────────────────────────────────────────────┐
 * │                          错误类型分类                               │
 * ├─────────────────────────────────────────────────────────────────────┤
 * │                                                                      │
 * │  1. 连接级错误（需要关闭连接）                                      │
 * │     - ERROR_NETNAME_DELETED (64) - 对端关闭                         │
 * │     - WSAECONNRESET (10054) - 连接重置                              │
 * │     - WSAECONNABORTED (10053) - 连接中止                            │
 * │     - 传输字节数为0 - 正常关闭                                      │
 * │                                                                      │
 * │  2. 可恢复错误（可以重试）                                          │
 * │     - WSAEWOULDBLOCK (10035) - 资源暂时不可用                       │
 * │     - ERROR_IO_PENDING (997) - 操作已投递                           │
 * │     - WSAEINTR (10004) - 被中断                                     │
 * │                                                                      │
 * │  3. 严重错误（需要记录/报警）                                       │
 * │     - WSAENOBUFS (10055) - 缓冲区不足                               │
 * │     - ERROR_NOT_ENOUGH_MEMORY - 内存不足                            │
 * │     - 其他未知错误                                                   │
 * │                                                                      │
 * └─────────────────────────────────────────────────────────────────────┘
 */

// 错误码分类
enum class ErrorCategory {
    CONNECTION_CLOSED,   // 连接关闭（正常或异常）
    RECOVERABLE,         // 可恢复
    FATAL,               // 严重错误
    UNKNOWN              // 未知
};

// 分析错误码
inline ErrorCategory categorize_error(DWORD error) {
    switch (error) {
        // 连接关闭类
        case ERROR_NETNAME_DELETED:
        case WSAECONNRESET:
        case WSAECONNABORTED:
        case WSAESHUTDOWN:
        case WSAENOTCONN:
        case ERROR_CONNECTION_ABORTED:
            return ErrorCategory::CONNECTION_CLOSED;

        // 可恢复类
        case WSAEWOULDBLOCK:
        case ERROR_IO_PENDING:
        case WSAEINTR:
        case WSAEINPROGRESS:
            return ErrorCategory::RECOVERABLE;

        // 严重错误类
        case WSAENOBUFS:
        case ERROR_NOT_ENOUGH_MEMORY:
        case WSAEMFILE:  // 文件描述符耗尽
            return ErrorCategory::FATAL;

        default:
            return ErrorCategory::UNKNOWN;
    }
}

// 错误信息转字符串
inline std::string error_to_string(DWORD error) {
    char* msg = nullptr;
    DWORD len = FormatMessageA(
        FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM,
        NULL,
        error,
        MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT),
        (LPSTR)&msg,
        0,
        NULL
    );

    std::string result;
    if (len > 0 && msg) {
        result = std::string(msg, len);
        // 移除末尾换行
        while (!result.empty() && (result.back() == '\n' || result.back() == '\r')) {
            result.pop_back();
        }
        LocalFree(msg);
    } else {
        result = "Unknown error";
    }

    std::ostringstream oss;
    oss << result << " (code: " << error << ")";
    return oss.str();
}

/*
 * 资源回收策略：
 *
 * ┌─────────────────────────────────────────────────────────────────────┐
 * │                         资源生命周期管理                            │
 * ├─────────────────────────────────────────────────────────────────────┤
 * │                                                                      │
 * │  问题：OVERLAPPED结构何时可以安全释放？                             │
 * │                                                                      │
 * │  错误做法：                                                         │
 * │  ┌───────────────────────────────────────────────────────────────┐ │
 * │  │  auto ctx = new IOContext();                                  │ │
 * │  │  WSARecv(..., ctx, ...);                                      │ │
 * │  │  delete ctx;  // 错误！I/O可能还在进行中                      │ │
 * │  └───────────────────────────────────────────────────────────────┘ │
 * │                                                                      │
 * │  正确做法：                                                         │
 * │  ┌───────────────────────────────────────────────────────────────┐ │
 * │  │  投递I/O ─────▶ 等待完成 ─────▶ 处理完成 ─────▶ 释放资源     │ │
 * │  │                       │                                       │ │
 * │  │                       └─── 完成前不能释放OVERLAPPED          │ │
 * │  └───────────────────────────────────────────────────────────────┘ │
 * │                                                                      │
 * │  连接关闭时的资源回收：                                             │
 * │  ┌───────────────────────────────────────────────────────────────┐ │
 * │  │  1. 取消所有挂起的I/O操作                                     │ │
 * │  │     CancelIoEx(socket, NULL);                                 │ │
 * │  │                                                                │ │
 * │  │  2. 等待所有I/O完成通知                                       │ │
 * │  │     (完成端口会收到失败通知)                                  │ │
 * │  │                                                                │ │
 * │  │  3. 关闭socket                                                 │ │
 * │  │     closesocket(socket);                                       │ │
 * │  │                                                                │ │
 * │  │  4. 释放连接上下文                                             │ │
 * │  │     delete connection;                                         │ │
 * │  └───────────────────────────────────────────────────────────────┘ │
 * │                                                                      │
 * └─────────────────────────────────────────────────────────────────────┘
 */

// 安全的连接关闭
class SafeConnectionCloser {
public:
    static void close(SOCKET socket) {
        if (socket == INVALID_SOCKET) return;

        // 1. 取消所有挂起的I/O
        CancelIoEx((HANDLE)socket, NULL);

        // 2. 禁用发送和接收
        shutdown(socket, SD_BOTH);

        // 3. 设置linger选项（可选）
        linger lin = {1, 0};  // 立即关闭，丢弃未发送数据
        setsockopt(socket, SOL_SOCKET, SO_LINGER, (char*)&lin, sizeof(lin));

        // 4. 关闭socket
        closesocket(socket);
    }
};

/*
 * 挂起I/O追踪器
 * 用于追踪每个连接上有多少挂起的I/O操作
 */
class PendingIOTracker {
public:
    void on_io_started() {
        pending_count_.fetch_add(1, std::memory_order_relaxed);
    }

    void on_io_completed() {
        pending_count_.fetch_sub(1, std::memory_order_relaxed);
    }

    int pending_count() const {
        return pending_count_.load(std::memory_order_relaxed);
    }

    // 等待所有挂起的I/O完成
    bool wait_all_completed(int timeout_ms) {
        auto start = std::chrono::steady_clock::now();
        while (pending_count() > 0) {
            auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(
                std::chrono::steady_clock::now() - start
            ).count();

            if (elapsed >= timeout_ms) {
                return false;
            }

            std::this_thread::sleep_for(std::chrono::milliseconds(1));
        }
        return true;
    }

private:
    std::atomic<int> pending_count_{0};
};

} // namespace iocp

#endif // _WIN32
```

#### 12.4 IOCP性能调优

```
IOCP性能调优指南：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. 线程数量调优                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  初始值：CPU核心数 × 2                                            │ │
│  │                                                                    │ │
│  │  调优方法：                                                       │ │
│  │  - 监控线程等待时间                                               │ │
│  │  - 如果经常有线程空闲 → 减少线程数                               │ │
│  │  - 如果完成队列经常堆积 → 增加线程数                             │ │
│  │                                                                    │ │
│  │  场景示例：                                                       │ │
│  │  ┌────────────────────────────────────────────────────────────┐  │ │
│  │  │ 纯网络I/O（无阻塞调用）：线程数 = CPU核心数               │  │ │
│  │  │ 有数据库访问：线程数 = CPU核心数 × 2~4                    │  │ │
│  │  │ 有大量文件操作：线程数 = CPU核心数 × 4~8                  │  │ │
│  │  └────────────────────────────────────────────────────────────┘  │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  2. 缓冲区大小调优                                                      │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  影响因素：                                                       │ │
│  │  - 网络带宽                                                       │ │
│  │  - 数据包平均大小                                                 │ │
│  │  - 内存使用                                                       │ │
│  │                                                                    │ │
│  │  推荐值：                                                         │ │
│  │  ┌────────────────────────────────────────────────────────────┐  │ │
│  │  │ 小数据包场景（如游戏）：1KB ~ 4KB                          │  │ │
│  │  │ 一般Web服务：8KB ~ 16KB                                    │  │ │
│  │  │ 文件传输：64KB ~ 256KB                                     │  │ │
│  │  └────────────────────────────────────────────────────────────┘  │ │
│  │                                                                    │ │
│  │  权衡：                                                           │ │
│  │  - 大缓冲区：减少I/O次数，但增加内存使用                         │ │
│  │  - 小缓冲区：节省内存，但可能增加I/O次数                         │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  3. Accept预投递数量                                                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  考虑因素：                                                       │ │
│  │  - 预期的连接速率                                                 │ │
│  │  - 每个Accept消耗的资源（预创建的socket）                        │ │
│  │                                                                    │ │
│  │  推荐值：                                                         │ │
│  │  ┌────────────────────────────────────────────────────────────┐  │ │
│  │  │ 低连接速率（< 100/秒）：5 ~ 10 个                         │  │ │
│  │  │ 中等连接速率（100~1000/秒）：10 ~ 50 个                   │  │ │
│  │  │ 高连接速率（> 1000/秒）：50 ~ 200 个                      │  │ │
│  │  └────────────────────────────────────────────────────────────┘  │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  4. Socket选项优化                                                      │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  // 关闭Nagle算法（减少延迟）                                    │ │
│  │  int nodelay = 1;                                                  │ │
│  │  setsockopt(socket, IPPROTO_TCP, TCP_NODELAY,                     │ │
│  │             (char*)&nodelay, sizeof(nodelay));                    │ │
│  │                                                                    │ │
│  │  // 调整发送/接收缓冲区                                          │ │
│  │  int bufsize = 65536;                                              │ │
│  │  setsockopt(socket, SOL_SOCKET, SO_SNDBUF,                        │ │
│  │             (char*)&bufsize, sizeof(bufsize));                    │ │
│  │  setsockopt(socket, SOL_SOCKET, SO_RCVBUF,                        │ │
│  │             (char*)&bufsize, sizeof(bufsize));                    │ │
│  │                                                                    │ │
│  │  // Keep-Alive（检测死连接）                                     │ │
│  │  int keepalive = 1;                                                │ │
│  │  setsockopt(socket, SOL_SOCKET, SO_KEEPALIVE,                     │ │
│  │             (char*)&keepalive, sizeof(keepalive));                │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  5. 内存池优化                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  问题：频繁new/delete IOContext会导致                             │ │
│  │  - 内存碎片                                                       │ │
│  │  - 分配器锁竞争                                                   │ │
│  │                                                                    │ │
│  │  解决：使用对象池                                                 │ │
│  │  ┌────────────────────────────────────────────────────────────┐  │ │
│  │  │  template<typename T, size_t PoolSize>                      │  │ │
│  │  │  class ObjectPool {                                         │  │ │
│  │  │      std::array<T, PoolSize> objects_;                      │  │ │
│  │  │      std::stack<T*> free_list_;                             │  │ │
│  │  │      std::mutex mutex_;                                     │  │ │
│  │  │                                                             │  │ │
│  │  │  public:                                                    │  │ │
│  │  │      T* acquire() {                                         │  │ │
│  │  │          std::lock_guard lock(mutex_);                      │  │ │
│  │  │          if (free_list_.empty()) return nullptr;            │  │ │
│  │  │          T* obj = free_list_.top();                         │  │ │
│  │  │          free_list_.pop();                                  │  │ │
│  │  │          return obj;                                        │  │ │
│  │  │      }                                                      │  │ │
│  │  │      void release(T* obj) {                                 │  │ │
│  │  │          std::lock_guard lock(mutex_);                      │  │ │
│  │  │          free_list_.push(obj);                              │  │ │
│  │  │      }                                                      │  │ │
│  │  │  };                                                         │  │ │
│  │  └────────────────────────────────────────────────────────────┘  │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 12.5 高性能IOCP服务器完整示例

```cpp
// high_performance_iocp_server.hpp - 高性能IOCP服务器
#pragma once

#ifdef _WIN32

#include "iocp_error_handling.hpp"
#include "iocp_batch.hpp"
#include <shared_mutex>
#include <array>
#include <atomic>

namespace iocp {

/*
 * 高性能IOCP服务器设计要点：
 *
 * 1. 使用对象池管理IOContext
 * 2. 批量获取完成事件
 * 3. 无锁/少锁设计
 * 4. 合理的缓冲区管理
 * 5. 优雅的关闭处理
 */

// IOContext对象池
template<size_t PoolSize = 10000>
class IOContextPool {
public:
    IOContextPool() {
        // 预分配所有对象
        for (size_t i = 0; i < PoolSize; ++i) {
            free_list_.push(&contexts_[i]);
        }
    }

    IOContext* acquire() {
        std::lock_guard<std::mutex> lock(mutex_);
        if (free_list_.empty()) {
            // 池耗尽，动态分配（降级策略）
            overflow_count_.fetch_add(1);
            return new IOContext();
        }
        IOContext* ctx = free_list_.top();
        free_list_.pop();
        ctx->reset();
        return ctx;
    }

    void release(IOContext* ctx) {
        // 检查是否是池内对象
        if (ctx >= &contexts_[0] && ctx < &contexts_[PoolSize]) {
            std::lock_guard<std::mutex> lock(mutex_);
            free_list_.push(ctx);
        } else {
            // 动态分配的，直接删除
            delete ctx;
        }
    }

    size_t available() const {
        std::lock_guard<std::mutex> lock(mutex_);
        return free_list_.size();
    }

    uint64_t overflow_count() const {
        return overflow_count_.load();
    }

private:
    std::array<IOContext, PoolSize> contexts_;
    std::stack<IOContext*> free_list_;
    mutable std::mutex mutex_;
    std::atomic<uint64_t> overflow_count_{0};
};

// 高性能连接上下文
struct HPConnection {
    SOCKET socket = INVALID_SOCKET;
    sockaddr_in addr = {};

    // 使用智能指针管理IOContext（从池中获取）
    IOContext* recv_ctx = nullptr;
    IOContext* send_ctx = nullptr;

    // 发送队列（无锁设计可以考虑使用无锁队列）
    std::queue<std::vector<char>> send_queue;
    std::mutex send_mutex;
    bool is_sending = false;

    // 挂起I/O计数
    PendingIOTracker pending_io;

    // 连接统计
    std::atomic<uint64_t> bytes_received{0};
    std::atomic<uint64_t> bytes_sent{0};
    std::atomic<uint64_t> messages_received{0};
    std::atomic<uint64_t> messages_sent{0};

    std::chrono::steady_clock::time_point connect_time;
    std::chrono::steady_clock::time_point last_active_time;

    HPConnection() {
        connect_time = std::chrono::steady_clock::now();
        last_active_time = connect_time;
    }

    void update_activity() {
        last_active_time = std::chrono::steady_clock::now();
    }

    // 获取连接持续时间（秒）
    double duration_seconds() const {
        auto now = std::chrono::steady_clock::now();
        return std::chrono::duration<double>(now - connect_time).count();
    }

    // 获取空闲时间（秒）
    double idle_seconds() const {
        auto now = std::chrono::steady_clock::now();
        return std::chrono::duration<double>(now - last_active_time).count();
    }
};

// 服务器统计信息
struct ServerStats {
    std::atomic<uint64_t> total_connections{0};
    std::atomic<uint64_t> active_connections{0};
    std::atomic<uint64_t> total_bytes_received{0};
    std::atomic<uint64_t> total_bytes_sent{0};
    std::atomic<uint64_t> total_messages{0};
    std::atomic<uint64_t> errors{0};

    void print() const {
        std::cout << "\n========== Server Stats ==========" << std::endl;
        std::cout << "Total connections: " << total_connections << std::endl;
        std::cout << "Active connections: " << active_connections << std::endl;
        std::cout << "Bytes received: " << total_bytes_received << std::endl;
        std::cout << "Bytes sent: " << total_bytes_sent << std::endl;
        std::cout << "Total messages: " << total_messages << std::endl;
        std::cout << "Errors: " << errors << std::endl;
        std::cout << "==================================" << std::endl;
    }
};

// 高性能IOCP服务器
class HighPerformanceServer {
public:
    HighPerformanceServer(uint16_t port, size_t worker_threads = 0)
        : port_(port)
    {
        WSADATA wsaData;
        if (WSAStartup(MAKEWORD(2, 2), &wsaData) != 0) {
            throw std::runtime_error("WSAStartup failed");
        }

        iocp_ = std::make_unique<CompletionPort>(0);

        if (worker_threads == 0) {
            SYSTEM_INFO sysInfo;
            GetSystemInfo(&sysInfo);
            worker_threads = sysInfo.dwNumberOfProcessors * 2;
        }
        num_workers_ = worker_threads;
    }

    ~HighPerformanceServer() {
        stop();
        WSACleanup();
    }

    bool start() {
        listen_socket_ = WSASocketW(AF_INET, SOCK_STREAM, IPPROTO_TCP,
                                    NULL, 0, WSA_FLAG_OVERLAPPED);
        if (listen_socket_ == INVALID_SOCKET) return false;

        // 设置socket选项
        int opt = 1;
        setsockopt(listen_socket_, SOL_SOCKET, SO_REUSEADDR, (char*)&opt, sizeof(opt));

        sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_addr.s_addr = INADDR_ANY;
        addr.sin_port = htons(port_);

        if (bind(listen_socket_, (sockaddr*)&addr, sizeof(addr)) == SOCKET_ERROR) {
            return false;
        }

        if (listen(listen_socket_, SOMAXCONN) == SOCKET_ERROR) {
            return false;
        }

        iocp_->associate_socket(listen_socket_, 0);

        acceptex_func_ = get_acceptex_func(listen_socket_);
        if (!acceptex_func_) return false;

        running_ = true;

        // 启动工作线程
        for (size_t i = 0; i < num_workers_; ++i) {
            workers_.emplace_back(&HighPerformanceServer::worker_thread, this, i);
        }

        // 启动统计线程
        stats_thread_ = std::thread(&HighPerformanceServer::stats_thread, this);

        // 预投递Accept
        for (int i = 0; i < ACCEPT_COUNT; ++i) {
            post_accept();
        }

        std::cout << "[HP Server] Started on port " << port_
                  << " with " << num_workers_ << " workers" << std::endl;
        return true;
    }

    void stop() {
        if (!running_) return;
        running_ = false;

        // 通知工作线程退出
        for (size_t i = 0; i < num_workers_; ++i) {
            post_quit(iocp_->handle());
        }

        // 等待工作线程
        for (auto& t : workers_) {
            if (t.joinable()) t.join();
        }

        // 等待统计线程
        if (stats_thread_.joinable()) {
            stats_thread_.join();
        }

        // 关闭所有连接
        {
            std::unique_lock<std::shared_mutex> lock(conn_mutex_);
            for (auto& [sock, conn] : connections_) {
                SafeConnectionCloser::close(conn->socket);
                if (conn->recv_ctx) ctx_pool_.release(conn->recv_ctx);
                if (conn->send_ctx) ctx_pool_.release(conn->send_ctx);
            }
            connections_.clear();
        }

        if (listen_socket_ != INVALID_SOCKET) {
            closesocket(listen_socket_);
            listen_socket_ = INVALID_SOCKET;
        }

        stats_.print();
        std::cout << "[HP Server] IOContext pool overflow: "
                  << ctx_pool_.overflow_count() << std::endl;
    }

    void wait() {
        for (auto& t : workers_) {
            if (t.joinable()) t.join();
        }
    }

    const ServerStats& stats() const { return stats_; }

private:
    static constexpr int ACCEPT_COUNT = 50;  // 预投递的Accept数量

    void worker_thread(size_t id) {
        BatchProcessor batch(iocp_->handle());

        while (running_) {
            // 批量获取完成事件
            size_t processed = batch.process_batch(
                [this](ULONG_PTR key, OVERLAPPED* overlapped, DWORD bytes) {
                    this->handle_completion(key, overlapped, bytes);
                },
                100  // 100ms超时
            );

            if (processed == 0 && !running_) {
                break;
            }
        }

        std::cout << "[Worker " << id << "] Batch stats:" << std::endl;
        batch.print_stats();
    }

    void stats_thread() {
        while (running_) {
            std::this_thread::sleep_for(std::chrono::seconds(10));
            if (running_) {
                stats_.print();
                std::cout << "IOContext pool available: "
                          << ctx_pool_.available() << std::endl;
            }
        }
    }

    void post_accept() {
        auto ctx = ctx_pool_.acquire();
        if (!async_accept(listen_socket_, ctx, acceptex_func_)) {
            ctx_pool_.release(ctx);
        }
    }

    void handle_completion(ULONG_PTR key, OVERLAPPED* overlapped, DWORD bytes) {
        IOContext* ctx = static_cast<IOContext*>(overlapped);

        // 检查退出信号
        if (key == 0 && overlapped == nullptr) {
            return;
        }

        if (ctx->op_type == OperationType::ACCEPT) {
            handle_accept(ctx, bytes);
        } else {
            HPConnection* conn = reinterpret_cast<HPConnection*>(key);
            conn->pending_io.on_io_completed();

            if (bytes == 0 && ctx->op_type != OperationType::DISCONNECT) {
                handle_disconnect(conn);
            } else if (ctx->op_type == OperationType::RECV) {
                handle_recv(conn, ctx, bytes);
            } else if (ctx->op_type == OperationType::SEND) {
                handle_send(conn, ctx, bytes);
            }
        }
    }

    void handle_accept(IOContext* ctx, DWORD bytes) {
        SOCKET client = ctx->accept_socket;

        // 设置socket属性
        setsockopt(client, SOL_SOCKET, SO_UPDATE_ACCEPT_CONTEXT,
                   (char*)&listen_socket_, sizeof(listen_socket_));

        int nodelay = 1;
        setsockopt(client, IPPROTO_TCP, TCP_NODELAY, (char*)&nodelay, sizeof(nodelay));

        // 创建连接
        auto conn = std::make_shared<HPConnection>();
        conn->socket = client;
        conn->recv_ctx = ctx_pool_.acquire();
        conn->send_ctx = ctx_pool_.acquire();
        conn->recv_ctx->socket = client;
        conn->send_ctx->socket = client;

        // 关联IOCP
        iocp_->associate_socket(client, reinterpret_cast<ULONG_PTR>(conn.get()));

        // 保存连接
        {
            std::unique_lock<std::shared_mutex> lock(conn_mutex_);
            connections_[client] = conn;
        }

        stats_.total_connections++;
        stats_.active_connections++;

        // 投递接收
        conn->recv_ctx->setup_recv(client);
        conn->pending_io.on_io_started();
        async_recv(conn->recv_ctx);

        // 释放并重新投递Accept
        ctx_pool_.release(ctx);
        post_accept();
    }

    void handle_recv(HPConnection* conn, IOContext* ctx, DWORD bytes) {
        conn->update_activity();
        conn->bytes_received += bytes;
        conn->messages_received++;
        stats_.total_bytes_received += bytes;
        stats_.total_messages++;

        // Echo: 发送接收到的数据
        std::vector<char> data(ctx->buffer, ctx->buffer + bytes);
        send_to(conn, std::move(data));

        // 继续接收
        ctx->setup_recv(conn->socket);
        conn->pending_io.on_io_started();
        if (!async_recv(ctx)) {
            conn->pending_io.on_io_completed();
            handle_disconnect(conn);
        }
    }

    void handle_send(HPConnection* conn, IOContext* ctx, DWORD bytes) {
        conn->bytes_sent += bytes;
        conn->messages_sent++;
        stats_.total_bytes_sent += bytes;

        std::lock_guard<std::mutex> lock(conn->send_mutex);

        if (!conn->send_queue.empty()) {
            auto& data = conn->send_queue.front();
            ctx->setup_send(conn->socket, data.data(), data.size());
            conn->send_queue.pop();

            conn->pending_io.on_io_started();
            if (!async_send(ctx)) {
                conn->pending_io.on_io_completed();
                conn->is_sending = false;
            }
        } else {
            conn->is_sending = false;
        }
    }

    void handle_disconnect(HPConnection* conn) {
        stats_.active_connections--;

        SafeConnectionCloser::close(conn->socket);

        std::unique_lock<std::shared_mutex> lock(conn_mutex_);
        auto it = connections_.find(conn->socket);
        if (it != connections_.end()) {
            if (it->second->recv_ctx) ctx_pool_.release(it->second->recv_ctx);
            if (it->second->send_ctx) ctx_pool_.release(it->second->send_ctx);
            connections_.erase(it);
        }
    }

    void send_to(HPConnection* conn, std::vector<char> data) {
        std::lock_guard<std::mutex> lock(conn->send_mutex);

        if (conn->is_sending) {
            conn->send_queue.push(std::move(data));
        } else {
            conn->is_sending = true;
            conn->send_ctx->setup_send(conn->socket, data.data(), data.size());
            conn->pending_io.on_io_started();
            if (!async_send(conn->send_ctx)) {
                conn->pending_io.on_io_completed();
                conn->is_sending = false;
            }
        }
    }

private:
    uint16_t port_;
    SOCKET listen_socket_ = INVALID_SOCKET;
    std::unique_ptr<CompletionPort> iocp_;
    LPFN_ACCEPTEX acceptex_func_ = nullptr;

    std::vector<std::thread> workers_;
    std::thread stats_thread_;
    size_t num_workers_;
    std::atomic<bool> running_{false};

    mutable std::shared_mutex conn_mutex_;
    std::unordered_map<SOCKET, std::shared_ptr<HPConnection>> connections_;

    IOContextPool<> ctx_pool_;
    ServerStats stats_;
};

} // namespace iocp

#endif // _WIN32
```

---

### 第二周自测题

#### 概念理解题

1. **IOCP等待线程队列为什么使用LIFO（后进先出）而不是FIFO？**

   <details>
   <summary>参考答案</summary>

   LIFO设计有以下优势：
   - **CPU缓存友好**：最近活跃的线程更可能有热缓存
   - **减少上下文切换**：避免唤醒长时间休眠的线程
   - **提高局部性**：相同线程处理连续事件，提高数据局部性
   </details>

2. **为什么需要使用AcceptEx而不是普通的accept？**

   <details>
   <summary>参考答案</summary>

   - `accept`是阻塞调用，会阻塞调用线程
   - `AcceptEx`是异步调用，可以与IOCP配合
   - `AcceptEx`可以在accept的同时接收初始数据
   - `AcceptEx`需要预先创建socket，减少accept时的开销
   </details>

3. **Completion Key的最佳实践是什么？**

   <details>
   <summary>参考答案</summary>

   最佳实践是将Completion Key设置为连接上下文（Connection Context）的指针：
   - 可以直接访问连接相关的所有数据
   - 避免额外的查找操作
   - 生命周期管理清晰
   </details>

#### 代码分析题

4. **以下代码有什么问题？**

   ```cpp
   void handle_recv(Connection* conn, IOContext* ctx, DWORD bytes) {
       // 处理数据
       process_data(ctx->buffer, bytes);

       // 释放IOContext
       delete ctx;

       // 投递新的接收请求
       auto new_ctx = new IOContext();
       new_ctx->setup_recv(conn->socket);
       async_recv(new_ctx);
   }
   ```

   <details>
   <summary>参考答案</summary>

   问题：
   - 每次接收都new/delete IOContext，效率低，会产生内存碎片
   - 应该重用IOContext，或使用对象池
   - 正确做法：`ctx->setup_recv(conn->socket); async_recv(ctx);`
   </details>

5. **如何正确关闭一个IOCP关联的连接？**

   <details>
   <summary>参考答案</summary>

   ```cpp
   // 1. 取消所有挂起的I/O
   CancelIoEx((HANDLE)socket, NULL);

   // 2. 等待所有挂起的I/O完成（通过完成端口收到失败通知）
   // 或使用PendingIOTracker追踪

   // 3. 关闭socket
   shutdown(socket, SD_BOTH);
   closesocket(socket);

   // 4. 释放连接相关资源
   // 确保所有IOContext都已释放
   ```
   </details>

---

### 第二周检验标准

| 检验项 | 标准 | 自评 |
|--------|------|------|
| 理解IOCP原理 | 能画出IOCP内部架构图 | ☐ |
| 理解重叠I/O | 能解释OVERLAPPED结构作用 | ☐ |
| 掌握核心API | 能正确使用Create/Get/Post函数 | ☐ |
| 理解线程管理 | 能解释LIFO和并发控制 | ☐ |
| 实现基础服务器 | Echo服务器能正确运行 | ☐ |
| 理解错误处理 | 能分类处理不同错误 | ☐ |
| 掌握资源管理 | 能正确管理连接生命周期 | ☐ |
| 性能调优意识 | 知道主要调优方向 | ☐ |

---

### 第二周时间分配

| 内容 | 时间 |
|------|------|
| IOCP核心概念理解 | 8小时 |
| API学习与实验 | 8小时 |
| 基础服务器实现 | 8小时 |
| 高级特性学习 | 6小时 |
| 性能调优实践 | 5小时 |

---

## 第三周：Linux io_uring实现Proactor（Day 15-21）

> **本周目标**：深入学习Linux最新的高性能异步I/O接口io_uring，理解其革命性设计，并基于io_uring实现完整的Proactor框架。

```
┌─────────────────────────────────────────────────────────────────┐
│                    第三周学习路线图                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Day 15-16            Day 17-18            Day 19-21           │
│   ┌─────────┐         ┌─────────┐         ┌─────────┐          │
│   │io_uring │         │io_uring │         │ Proactor│          │
│   │  基础   │────────▶│API详解  │────────▶│  实现   │          │
│   └─────────┘         └─────────┘         └─────────┘          │
│       │                   │                   │                 │
│       ▼                   ▼                   ▼                 │
│   诞生背景             liburing库          AsyncOperation      │
│   SQ/CQ原理            prep系列函数        Proactor核心        │
│   内存映射             submit/wait         Echo服务器          │
│   零拷贝设计           SQE/CQE结构         完整框架            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Day 15-16：io_uring基础（10小时）

#### 15.1 io_uring的诞生背景

```
Linux异步I/O的演进历程：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  2002年: POSIX AIO (aio_read/aio_write)                                 │
│  └── 问题：用户态模拟，实际是线程池实现，性能差                        │
│       │                                                                  │
│       ▼                                                                  │
│  2007年: Linux Native AIO (io_submit/io_getevents)                      │
│  └── 改进：内核原生支持                                                 │
│  └── 问题：只支持O_DIRECT，不支持buffered I/O                          │
│  └── 问题：不支持网络I/O                                                │
│  └── 问题：API复杂，每次需要系统调用                                   │
│       │                                                                  │
│       ▼                                                                  │
│  2019年: io_uring (Linux 5.1)                                           │
│  └── 革命性设计：                                                       │
│      - 真正的异步I/O                                                    │
│      - 支持所有类型I/O（文件、网络、定时器等）                         │
│      - 共享内存环形缓冲区，减少系统调用                                │
│      - 零拷贝提交                                                       │
│      - 批量操作支持                                                     │
│      - 链式操作支持                                                     │
│                                                                          │
│  io_uring性能对比（每秒IOPS）：                                        │
│  ┌────────────────────────────────────────────────────────────────┐    │
│  │                                                                 │    │
│  │  epoll      ████████████████████████  ~800K IOPS               │    │
│  │  Linux AIO  ██████████████████████████████  ~1M IOPS           │    │
│  │  io_uring   ████████████████████████████████████████  ~1.7M IOPS│   │
│  │                                                                 │    │
│  │  （基于高性能NVMe SSD的随机读测试）                             │    │
│  │                                                                 │    │
│  └────────────────────────────────────────────────────────────────┘    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

io_uring的设计目标：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. 减少系统调用开销                                                    │
│     ┌───────────────────────────────────────────────────────────────┐  │
│     │  传统方式：每个I/O操作 = 1次系统调用                          │  │
│     │  io_uring：批量提交 + 批量收割，可以完全无系统调用            │  │
│     │                                                                │  │
│     │  1000个I/O操作：                                               │  │
│     │    传统：1000次系统调用                                        │  │
│     │    io_uring：最少0次（轮询模式），最多2次（提交+等待）        │  │
│     └───────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  2. 零拷贝（Zero-copy）                                                │
│     ┌───────────────────────────────────────────────────────────────┐  │
│     │  SQ/CQ通过mmap共享，应用程序直接写入内核可见的内存            │  │
│     │  无需在用户态和内核态之间拷贝请求数据                         │  │
│     └───────────────────────────────────────────────────────────────┘  │
│                                                                          │
│  3. 统一的I/O接口                                                       │
│     ┌───────────────────────────────────────────────────────────────┐  │
│     │  支持的操作类型：                                              │  │
│     │  - 文件读写（read/write/readv/writev）                        │  │
│     │  - 网络操作（recv/send/accept/connect）                       │  │
│     │  - 文件操作（openat/close/statx）                             │  │
│     │  - 定时器（timeout/link_timeout）                             │  │
│     │  - 取消操作（cancel）                                          │  │
│     │  - 同步（fsync/fdatasync）                                     │  │
│     │  - 内存管理（madvise/fadvise）                                │  │
│     │  - 等等...                                                     │  │
│     └───────────────────────────────────────────────────────────────┘  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 15.2 io_uring架构：提交队列与完成队列

```
io_uring核心架构：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                         应用程序（用户态）                               │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │   1. 准备SQE        2. 更新SQ tail      3. 通知内核（可选）      │ │
│  │       │                   │                    │                  │ │
│  └───────┼───────────────────┼────────────────────┼──────────────────┘ │
│          │                   │                    │                    │
│          ▼                   ▼                    ▼                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                  共享内存区域（mmap）                              │ │
│  │  ┌─────────────────────────────────────────────────────────────┐ │ │
│  │  │              提交队列 (Submission Queue - SQ)                │ │ │
│  │  │                                                              │ │ │
│  │  │   SQ Ring Buffer (存储SQE索引)                               │ │ │
│  │  │   ┌─────────────────────────────────────────────────────┐   │ │ │
│  │  │   │  head  │ [idx0] [idx1] [idx2] [idx3] ... │  tail   │   │ │ │
│  │  │   └─────────────────────────────────────────────────────┘   │ │ │
│  │  │        ↑ 内核消费                               ↑ 应用生产  │ │ │
│  │  │                                                              │ │ │
│  │  │   SQE Array (实际的提交条目)                                 │ │ │
│  │  │   ┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐                      │ │ │
│  │  │   │SQE 0 │ │SQE 1 │ │SQE 2 │ │SQE 3 │ ...                  │ │ │
│  │  │   │opcode│ │opcode│ │opcode│ │opcode│                      │ │ │
│  │  │   │fd    │ │fd    │ │fd    │ │fd    │                      │ │ │
│  │  │   │buf   │ │buf   │ │buf   │ │buf   │                      │ │ │
│  │  │   │...   │ │...   │ │...   │ │...   │                      │ │ │
│  │  │   └──────┘ └──────┘ └──────┘ └──────┘                      │ │ │
│  │  └─────────────────────────────────────────────────────────────┘ │ │
│  │                                                                   │ │
│  │  ┌─────────────────────────────────────────────────────────────┐ │ │
│  │  │              完成队列 (Completion Queue - CQ)                │ │ │
│  │  │                                                              │ │ │
│  │  │   CQ Ring Buffer (存储CQE)                                   │ │ │
│  │  │   ┌─────────────────────────────────────────────────────┐   │ │ │
│  │  │   │  head  │ [CQE0] [CQE1] [CQE2] [CQE3]... │  tail    │   │ │ │
│  │  │   └─────────────────────────────────────────────────────┘   │ │ │
│  │  │        ↑ 应用消费                               ↑ 内核生产  │ │ │
│  │  │                                                              │ │ │
│  │  └─────────────────────────────────────────────────────────────┘ │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│          │                   │                    │                    │
│          ▼                   ▼                    ▼                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │   4. 处理SQE        5. 更新CQ tail      6. 等待应用消费          │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                         内核（内核态）                                  │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

SQ与CQ的分离设计：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  为什么SQ要分成Ring Buffer和SQE Array两部分？                          │
│                                                                          │
│  原因1：支持乱序提交                                                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  应用可以预先准备多个SQE，然后按需要的顺序提交                    │ │
│  │                                                                    │ │
│  │  SQE Array:  [SQE0] [SQE1] [SQE2] [SQE3] [SQE4]                  │ │
│  │                                                                    │ │
│  │  SQ Ring:    [2] [0] [4]   // 实际提交顺序：SQE2, SQE0, SQE4     │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  原因2：SQE可以预分配和重用                                            │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  SQE Array是固定大小的数组，应用可以维护自己的空闲SQE列表        │ │
│  │  避免动态内存分配                                                  │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  原因3：内核可以批量处理                                               │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  内核可以一次读取Ring Buffer中的多个索引，批量处理对应的SQE      │ │
│  │  提高缓存效率                                                      │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 15.3 io_uring_setup系统调用

```
io_uring初始化流程：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  // io_uring_setup 系统调用                                             │
│  int io_uring_setup(unsigned entries, struct io_uring_params *p);       │
│                                                                          │
│  参数说明：                                                             │
│  - entries: SQ/CQ的大小（会被向上取整到2的幂）                         │
│  - p: 配置参数和返回值                                                  │
│                                                                          │
│  struct io_uring_params {                                               │
│      __u32 sq_entries;      // [out] SQ实际大小                        │
│      __u32 cq_entries;      // [out] CQ实际大小（默认2倍SQ）           │
│      __u32 flags;           // [in] 配置标志                            │
│      __u32 sq_thread_cpu;   // [in] SQ轮询线程绑定的CPU                │
│      __u32 sq_thread_idle;  // [in] SQ轮询线程空闲超时(ms)             │
│      __u32 features;        // [out] 内核支持的特性                     │
│      __u32 wq_fd;           // [in] 工作队列共享的io_uring fd          │
│      __u32 resv[3];                                                     │
│      struct io_sqring_offsets sq_off;  // [out] SQ内存映射偏移         │
│      struct io_cqring_offsets cq_off;  // [out] CQ内存映射偏移         │
│  };                                                                      │
│                                                                          │
│  重要标志（flags）：                                                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  IORING_SETUP_IOPOLL                                              │ │
│  │  └── I/O轮询模式，用于高性能存储设备                              │ │
│  │                                                                    │ │
│  │  IORING_SETUP_SQPOLL                                              │ │
│  │  └── SQ轮询模式，内核线程轮询SQ，无需系统调用提交               │ │
│  │                                                                    │ │
│  │  IORING_SETUP_SQ_AFF                                              │ │
│  │  └── 将SQ轮询线程绑定到指定CPU                                   │ │
│  │                                                                    │ │
│  │  IORING_SETUP_CQSIZE                                              │ │
│  │  └── 允许指定CQ大小（否则默认2倍SQ）                             │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

内存映射过程：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  // 步骤1：调用io_uring_setup获取fd和偏移信息                          │
│  int ring_fd = io_uring_setup(1024, &params);                           │
│                                                                          │
│  // 步骤2：映射SQ Ring和SQE Array                                       │
│  // SQ Ring映射                                                         │
│  void *sq_ptr = mmap(NULL,                                              │
│      params.sq_off.array + params.sq_entries * sizeof(__u32),           │
│      PROT_READ | PROT_WRITE,                                            │
│      MAP_SHARED | MAP_POPULATE,                                         │
│      ring_fd,                                                           │
│      IORING_OFF_SQ_RING);                                               │
│                                                                          │
│  // SQE Array映射（可能与SQ Ring在同一映射，取决于内核版本）           │
│  void *sqes = mmap(NULL,                                                │
│      params.sq_entries * sizeof(struct io_uring_sqe),                   │
│      PROT_READ | PROT_WRITE,                                            │
│      MAP_SHARED | MAP_POPULATE,                                         │
│      ring_fd,                                                           │
│      IORING_OFF_SQES);                                                  │
│                                                                          │
│  // 步骤3：映射CQ Ring                                                  │
│  void *cq_ptr = mmap(NULL,                                              │
│      params.cq_off.cqes + params.cq_entries * sizeof(struct io_uring_cqe),│
│      PROT_READ | PROT_WRITE,                                            │
│      MAP_SHARED | MAP_POPULATE,                                         │
│      ring_fd,                                                           │
│      IORING_OFF_CQ_RING);                                               │
│                                                                          │
│  // 步骤4：通过偏移计算各指针位置                                       │
│  unsigned *sq_head = sq_ptr + params.sq_off.head;                       │
│  unsigned *sq_tail = sq_ptr + params.sq_off.tail;                       │
│  unsigned *sq_array = sq_ptr + params.sq_off.array;                     │
│                                                                          │
│  unsigned *cq_head = cq_ptr + params.cq_off.head;                       │
│  unsigned *cq_tail = cq_ptr + params.cq_off.tail;                       │
│  struct io_uring_cqe *cqes = cq_ptr + params.cq_off.cqes;              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

内存布局可视化：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  mmap区域1：SQ Ring (IORING_OFF_SQ_RING)                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ [head] [tail] [ring_mask] [ring_entries] [flags] [dropped]      │   │
│  │ [array[0]] [array[1]] [array[2]] ... [array[sq_entries-1]]      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  mmap区域2：SQE Array (IORING_OFF_SQES)                                │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ [SQE 0] [SQE 1] [SQE 2] ... [SQE sq_entries-1]                  │   │
│  │ (每个SQE 64字节)                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  mmap区域3：CQ Ring (IORING_OFF_CQ_RING)                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ [head] [tail] [ring_mask] [ring_entries] [overflow] [cqes]      │   │
│  │ [CQE 0] [CQE 1] [CQE 2] ... [CQE cq_entries-1]                  │   │
│  │ (每个CQE 16字节)                                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 15.4 SQE与CQE数据结构

```cpp
// io_uring_structures.hpp - io_uring核心数据结构详解
#pragma once

#include <linux/io_uring.h>
#include <cstdint>

/*
 * Submission Queue Entry (SQE) - 64字节
 *
 * struct io_uring_sqe {
 *     __u8    opcode;        // 操作码
 *     __u8    flags;         // SQE标志
 *     __u16   ioprio;        // I/O优先级
 *     __s32   fd;            // 文件描述符
 *     union {
 *         __u64   off;       // 文件偏移
 *         __u64   addr2;     // 第二个地址
 *     };
 *     union {
 *         __u64   addr;      // 缓冲区地址
 *         __u64   splice_off_in;
 *     };
 *     __u32   len;           // 缓冲区长度
 *     union {
 *         __kernel_rwf_t  rw_flags;    // read/write标志
 *         __u32   fsync_flags;         // fsync标志
 *         __u16   poll_events;         // poll事件
 *         __u32   sync_range_flags;
 *         __u32   msg_flags;           // send/recv标志
 *         __u32   timeout_flags;
 *         __u32   accept_flags;
 *         __u32   cancel_flags;
 *         __u32   open_flags;
 *         __u32   statx_flags;
 *         __u32   fadvise_advice;
 *         __u32   splice_flags;
 *     };
 *     __u64   user_data;     // 用户数据（会原样返回到CQE）
 *     union {
 *         struct {
 *             union {
 *                 __u16   buf_index;   // 固定缓冲区索引
 *                 __u16   buf_group;   // 缓冲区组
 *             };
 *             __u16   personality;     // 凭证
 *             __s32   splice_fd_in;
 *         };
 *         __u64   __pad2[3];
 *     };
 * };
 */

/*
 * 常用操作码 (opcode)
 */
namespace io_uring_op {
    // 空操作（用于测试）
    constexpr uint8_t NOP = IORING_OP_NOP;

    // 读操作
    constexpr uint8_t READV = IORING_OP_READV;          // 向量读
    constexpr uint8_t READ_FIXED = IORING_OP_READ_FIXED; // 固定缓冲区读
    constexpr uint8_t READ = IORING_OP_READ;            // 普通读

    // 写操作
    constexpr uint8_t WRITEV = IORING_OP_WRITEV;
    constexpr uint8_t WRITE_FIXED = IORING_OP_WRITE_FIXED;
    constexpr uint8_t WRITE = IORING_OP_WRITE;

    // 同步操作
    constexpr uint8_t FSYNC = IORING_OP_FSYNC;
    constexpr uint8_t SYNC_FILE_RANGE = IORING_OP_SYNC_FILE_RANGE;

    // 网络操作
    constexpr uint8_t ACCEPT = IORING_OP_ACCEPT;
    constexpr uint8_t CONNECT = IORING_OP_CONNECT;
    constexpr uint8_t RECV = IORING_OP_RECV;
    constexpr uint8_t SEND = IORING_OP_SEND;
    constexpr uint8_t RECVMSG = IORING_OP_RECVMSG;
    constexpr uint8_t SENDMSG = IORING_OP_SENDMSG;

    // 定时器
    constexpr uint8_t TIMEOUT = IORING_OP_TIMEOUT;
    constexpr uint8_t TIMEOUT_REMOVE = IORING_OP_TIMEOUT_REMOVE;
    constexpr uint8_t LINK_TIMEOUT = IORING_OP_LINK_TIMEOUT;

    // 文件操作
    constexpr uint8_t OPENAT = IORING_OP_OPENAT;
    constexpr uint8_t CLOSE = IORING_OP_CLOSE;
    constexpr uint8_t STATX = IORING_OP_STATX;

    // 取消操作
    constexpr uint8_t ASYNC_CANCEL = IORING_OP_ASYNC_CANCEL;

    // poll
    constexpr uint8_t POLL_ADD = IORING_OP_POLL_ADD;
    constexpr uint8_t POLL_REMOVE = IORING_OP_POLL_REMOVE;
}

/*
 * SQE标志 (flags)
 */
namespace sqe_flags {
    // 固定文件（使用注册的fd）
    constexpr uint8_t FIXED_FILE = IOSQE_FIXED_FILE;

    // I/O漏斗（用于限制并发）
    constexpr uint8_t IO_DRAIN = IOSQE_IO_DRAIN;

    // 链接操作（下一个SQE依赖此SQE成功）
    constexpr uint8_t IO_LINK = IOSQE_IO_LINK;

    // 硬链接（即使失败也继续链）
    constexpr uint8_t IO_HARDLINK = IOSQE_IO_HARDLINK;

    // 异步执行（强制在异步上下文执行）
    constexpr uint8_t ASYNC = IOSQE_ASYNC;

    // 使用缓冲区选择
    constexpr uint8_t BUFFER_SELECT = IOSQE_BUFFER_SELECT;
}

/*
 * Completion Queue Entry (CQE) - 16字节
 *
 * struct io_uring_cqe {
 *     __u64   user_data;  // 从SQE复制的用户数据
 *     __s32   res;        // 操作结果（成功时为传输字节数，失败时为负的错误码）
 *     __u32   flags;      // 完成标志
 * };
 */

/*
 * CQE标志
 */
namespace cqe_flags {
    // 缓冲区选择结果在flags高16位
    constexpr uint32_t BUFFER = IORING_CQE_F_BUFFER;

    // 更多数据可用（用于多射操作）
    constexpr uint32_t MORE = IORING_CQE_F_MORE;

    // socket相关：发送零拷贝通知
    constexpr uint32_t SOCK_NONEMPTY = IORING_CQE_F_SOCK_NONEMPTY;
}

/*
 * SQE内存布局图示：
 *
 * Offset  Field           Size    Description
 * ──────────────────────────────────────────────
 * 0       opcode          1       操作类型
 * 1       flags           1       SQE标志
 * 2       ioprio          2       I/O优先级
 * 4       fd              4       文件描述符
 * 8       off/addr2       8       偏移/第二地址
 * 16      addr            8       缓冲区地址
 * 24      len             4       长度
 * 28      op_flags        4       操作特定标志
 * 32      user_data       8       用户数据
 * 40      buf_index       2       缓冲区索引
 * 42      personality     2       凭证
 * 44      splice_fd_in    4       splice输入fd
 * 48      __pad2          16      保留
 * ──────────────────────────────────────────────
 * Total                   64      bytes
 *
 * CQE内存布局图示：
 *
 * Offset  Field           Size    Description
 * ──────────────────────────────────────────────
 * 0       user_data       8       用户数据
 * 8       res             4       结果
 * 12      flags           4       标志
 * ──────────────────────────────────────────────
 * Total                   16      bytes
 */
```

### Day 17-18：io_uring API详解（10小时）

#### 17.1 liburing库的使用

```cpp
// io_uring_liburing.hpp - liburing库封装与使用示例
#pragma once

#ifdef __linux__

#include <liburing.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <unistd.h>
#include <fcntl.h>
#include <cstring>
#include <iostream>
#include <stdexcept>
#include <memory>
#include <functional>

namespace io_uring_proactor {

/*
 * liburing简化了io_uring的使用：
 *
 * 原生io_uring使用步骤：
 * 1. io_uring_setup() 创建实例
 * 2. mmap() 映射SQ/CQ内存
 * 3. 手动管理head/tail指针
 * 4. io_uring_enter() 提交和等待
 *
 * liburing使用步骤：
 * 1. io_uring_queue_init() 一步完成初始化
 * 2. io_uring_get_sqe() 获取SQE
 * 3. io_uring_prep_xxx() 准备操作
 * 4. io_uring_submit() 提交
 * 5. io_uring_wait_cqe() 等待完成
 */

// io_uring实例封装
class IoUring {
public:
    IoUring(unsigned entries = 1024, unsigned flags = 0) {
        int ret = io_uring_queue_init(entries, &ring_, flags);
        if (ret < 0) {
            throw std::runtime_error("io_uring_queue_init failed: " +
                                   std::string(strerror(-ret)));
        }
        std::cout << "[io_uring] 初始化成功，队列大小: " << entries << std::endl;
    }

    ~IoUring() {
        io_uring_queue_exit(&ring_);
        std::cout << "[io_uring] 已销毁" << std::endl;
    }

    // 禁止拷贝
    IoUring(const IoUring&) = delete;
    IoUring& operator=(const IoUring&) = delete;

    // 获取SQE
    io_uring_sqe* get_sqe() {
        io_uring_sqe* sqe = io_uring_get_sqe(&ring_);
        if (!sqe) {
            // SQ已满，先提交一些
            io_uring_submit(&ring_);
            sqe = io_uring_get_sqe(&ring_);
        }
        return sqe;
    }

    // 提交所有待处理的SQE
    int submit() {
        return io_uring_submit(&ring_);
    }

    // 提交并等待至少一个完成
    int submit_and_wait(unsigned wait_nr = 1) {
        return io_uring_submit_and_wait(&ring_, wait_nr);
    }

    // 等待完成事件
    int wait_cqe(io_uring_cqe** cqe_ptr) {
        return io_uring_wait_cqe(&ring_, cqe_ptr);
    }

    // 带超时的等待
    int wait_cqe_timeout(io_uring_cqe** cqe_ptr, __kernel_timespec* ts) {
        return io_uring_wait_cqe_timeout(&ring_, cqe_ptr, ts);
    }

    // 非阻塞获取CQE
    int peek_cqe(io_uring_cqe** cqe_ptr) {
        return io_uring_peek_cqe(&ring_, cqe_ptr);
    }

    // 批量获取CQE
    unsigned peek_batch_cqe(io_uring_cqe** cqes, unsigned count) {
        return io_uring_peek_batch_cqe(&ring_, cqes, count);
    }

    // 标记CQE已处理
    void cqe_seen(io_uring_cqe* cqe) {
        io_uring_cqe_seen(&ring_, cqe);
    }

    // 获取底层ring结构
    io_uring* ring() { return &ring_; }

private:
    io_uring ring_;
};

/*
 * io_uring_prep_xxx 系列函数详解
 */

// 准备读操作
inline void prep_read(io_uring_sqe* sqe, int fd, void* buf, unsigned nbytes,
                      off_t offset, uint64_t user_data) {
    io_uring_prep_read(sqe, fd, buf, nbytes, offset);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备写操作
inline void prep_write(io_uring_sqe* sqe, int fd, const void* buf,
                       unsigned nbytes, off_t offset, uint64_t user_data) {
    io_uring_prep_write(sqe, fd, buf, nbytes, offset);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备向量读（readv）
inline void prep_readv(io_uring_sqe* sqe, int fd, const iovec* iovecs,
                       unsigned nr_vecs, off_t offset, uint64_t user_data) {
    io_uring_prep_readv(sqe, fd, iovecs, nr_vecs, offset);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备向量写（writev）
inline void prep_writev(io_uring_sqe* sqe, int fd, const iovec* iovecs,
                        unsigned nr_vecs, off_t offset, uint64_t user_data) {
    io_uring_prep_writev(sqe, fd, iovecs, nr_vecs, offset);
    io_uring_sqe_set_data64(sqe, user_data);
}

/*
 * 网络操作
 */

// 准备accept
inline void prep_accept(io_uring_sqe* sqe, int fd, sockaddr* addr,
                        socklen_t* addrlen, int flags, uint64_t user_data) {
    io_uring_prep_accept(sqe, fd, addr, addrlen, flags);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备connect
inline void prep_connect(io_uring_sqe* sqe, int fd, const sockaddr* addr,
                         socklen_t addrlen, uint64_t user_data) {
    io_uring_prep_connect(sqe, fd, addr, addrlen);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备recv
inline void prep_recv(io_uring_sqe* sqe, int sockfd, void* buf, size_t len,
                      int flags, uint64_t user_data) {
    io_uring_prep_recv(sqe, sockfd, buf, len, flags);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备send
inline void prep_send(io_uring_sqe* sqe, int sockfd, const void* buf,
                      size_t len, int flags, uint64_t user_data) {
    io_uring_prep_send(sqe, sockfd, buf, len, flags);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备close
inline void prep_close(io_uring_sqe* sqe, int fd, uint64_t user_data) {
    io_uring_prep_close(sqe, fd);
    io_uring_sqe_set_data64(sqe, user_data);
}

/*
 * 定时器操作
 */

// 准备超时
inline void prep_timeout(io_uring_sqe* sqe, __kernel_timespec* ts,
                         unsigned count, unsigned flags, uint64_t user_data) {
    io_uring_prep_timeout(sqe, ts, count, flags);
    io_uring_sqe_set_data64(sqe, user_data);
}

// 准备取消操作
inline void prep_cancel(io_uring_sqe* sqe, uint64_t user_data_to_cancel,
                        int flags, uint64_t user_data) {
    io_uring_prep_cancel64(sqe, user_data_to_cancel, flags);
    io_uring_sqe_set_data64(sqe, user_data);
}

/*
 * 链式操作示例
 *
 * 链式操作允许多个SQE按顺序执行：
 * SQE1 --link--> SQE2 --link--> SQE3
 *
 * 如果SQE1失败，SQE2和SQE3会被取消
 */
inline void set_link(io_uring_sqe* sqe) {
    sqe->flags |= IOSQE_IO_LINK;
}

// 硬链接：即使前一个失败，也继续执行
inline void set_hardlink(io_uring_sqe* sqe) {
    sqe->flags |= IOSQE_IO_HARDLINK;
}

} // namespace io_uring_proactor

#endif // __linux__
```

#### 17.2 io_uring基础示例

```cpp
// io_uring_basic_example.cpp - io_uring基础使用示例
#ifdef __linux__

#include "io_uring_liburing.hpp"
#include <vector>
#include <fstream>

using namespace io_uring_proactor;

/*
 * 示例1：文件异步读写
 */
void file_async_io_example() {
    std::cout << "\n=== 文件异步I/O示例 ===" << std::endl;

    IoUring ring(32);

    // 创建测试文件
    const char* filename = "/tmp/io_uring_test.txt";
    const char* write_data = "Hello, io_uring!\n";
    size_t data_len = strlen(write_data);

    // 打开文件（用于写入）
    int fd = open(filename, O_RDWR | O_CREAT | O_TRUNC, 0644);
    if (fd < 0) {
        std::cerr << "open failed: " << strerror(errno) << std::endl;
        return;
    }

    // 准备写操作
    {
        io_uring_sqe* sqe = ring.get_sqe();
        prep_write(sqe, fd, write_data, data_len, 0, 1);  // user_data = 1
    }

    // 提交并等待
    ring.submit_and_wait(1);

    // 获取写完成
    io_uring_cqe* cqe;
    ring.wait_cqe(&cqe);
    if (cqe->res < 0) {
        std::cerr << "write failed: " << strerror(-cqe->res) << std::endl;
    } else {
        std::cout << "写入 " << cqe->res << " 字节，user_data: "
                  << io_uring_cqe_get_data64(cqe) << std::endl;
    }
    ring.cqe_seen(cqe);

    // 准备读操作
    char read_buf[128] = {0};
    {
        io_uring_sqe* sqe = ring.get_sqe();
        prep_read(sqe, fd, read_buf, sizeof(read_buf), 0, 2);  // user_data = 2
    }

    ring.submit_and_wait(1);

    ring.wait_cqe(&cqe);
    if (cqe->res < 0) {
        std::cerr << "read failed: " << strerror(-cqe->res) << std::endl;
    } else {
        std::cout << "读取 " << cqe->res << " 字节: " << read_buf;
    }
    ring.cqe_seen(cqe);

    close(fd);
    unlink(filename);
}

/*
 * 示例2：批量I/O操作
 */
void batch_io_example() {
    std::cout << "\n=== 批量I/O示例 ===" << std::endl;

    IoUring ring(64);

    // 创建多个测试文件
    const int NUM_FILES = 5;
    std::vector<int> fds(NUM_FILES);
    std::vector<std::string> filenames;
    std::vector<std::vector<char>> buffers(NUM_FILES);

    for (int i = 0; i < NUM_FILES; ++i) {
        filenames.push_back("/tmp/io_uring_batch_" + std::to_string(i) + ".txt");

        // 创建文件并写入一些数据
        fds[i] = open(filenames[i].c_str(), O_RDWR | O_CREAT | O_TRUNC, 0644);
        std::string data = "File " + std::to_string(i) + " content\n";
        write(fds[i], data.c_str(), data.size());
        lseek(fds[i], 0, SEEK_SET);

        buffers[i].resize(64);
    }

    // 批量提交读操作
    for (int i = 0; i < NUM_FILES; ++i) {
        io_uring_sqe* sqe = ring.get_sqe();
        prep_read(sqe, fds[i], buffers[i].data(), buffers[i].size(), 0, i);
    }

    std::cout << "批量提交 " << NUM_FILES << " 个读操作" << std::endl;
    int submitted = ring.submit();
    std::cout << "实际提交: " << submitted << std::endl;

    // 批量获取完成
    io_uring_cqe* cqes[NUM_FILES];
    int completed = 0;

    while (completed < NUM_FILES) {
        io_uring_cqe* cqe;
        ring.wait_cqe(&cqe);

        uint64_t user_data = io_uring_cqe_get_data64(cqe);
        if (cqe->res >= 0) {
            buffers[user_data][cqe->res] = '\0';
            std::cout << "文件 " << user_data << " 读取完成: "
                      << buffers[user_data].data();
        }

        ring.cqe_seen(cqe);
        completed++;
    }

    // 清理
    for (int i = 0; i < NUM_FILES; ++i) {
        close(fds[i]);
        unlink(filenames[i].c_str());
    }
}

/*
 * 示例3：链式操作（写入后立即读取）
 */
void linked_operation_example() {
    std::cout << "\n=== 链式操作示例 ===" << std::endl;

    IoUring ring(32);

    const char* filename = "/tmp/io_uring_link_test.txt";
    int fd = open(filename, O_RDWR | O_CREAT | O_TRUNC, 0644);

    const char* write_data = "Linked operation test\n";
    char read_buf[64] = {0};

    // SQE1: 写入
    {
        io_uring_sqe* sqe = ring.get_sqe();
        prep_write(sqe, fd, write_data, strlen(write_data), 0, 1);
        set_link(sqe);  // 链接到下一个SQE
    }

    // SQE2: 读取（依赖SQE1完成）
    {
        io_uring_sqe* sqe = ring.get_sqe();
        prep_read(sqe, fd, read_buf, sizeof(read_buf), 0, 2);
    }

    std::cout << "提交链式操作: 写入 -> 读取" << std::endl;
    ring.submit();

    // 等待两个完成
    for (int i = 0; i < 2; ++i) {
        io_uring_cqe* cqe;
        ring.wait_cqe(&cqe);

        uint64_t user_data = io_uring_cqe_get_data64(cqe);
        std::cout << "操作 " << user_data << " 完成，结果: " << cqe->res << std::endl;

        if (user_data == 2 && cqe->res > 0) {
            std::cout << "读取内容: " << read_buf;
        }

        ring.cqe_seen(cqe);
    }

    close(fd);
    unlink(filename);
}

/*
 * 示例4：定时器
 */
void timeout_example() {
    std::cout << "\n=== 定时器示例 ===" << std::endl;

    IoUring ring(32);

    // 设置1秒超时
    __kernel_timespec ts = {.tv_sec = 1, .tv_nsec = 0};

    io_uring_sqe* sqe = ring.get_sqe();
    prep_timeout(sqe, &ts, 0, 0, 100);

    std::cout << "提交1秒定时器..." << std::endl;
    auto start = std::chrono::steady_clock::now();

    ring.submit();

    io_uring_cqe* cqe;
    ring.wait_cqe(&cqe);

    auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(
        std::chrono::steady_clock::now() - start
    ).count();

    if (cqe->res == -ETIME) {
        std::cout << "定时器触发！耗时: " << elapsed << "ms" << std::endl;
    }

    ring.cqe_seen(cqe);
}

// 主函数
int main() {
    try {
        file_async_io_example();
        batch_io_example();
        linked_operation_example();
        timeout_example();
    } catch (const std::exception& e) {
        std::cerr << "异常: " << e.what() << std::endl;
        return 1;
    }
    return 0;
}

#else

int main() {
    std::cout << "io_uring仅支持Linux系统" << std::endl;
    return 0;
}

#endif // __linux__
```

### Day 19-21：基于io_uring的Proactor实现（15小时）

#### 19.1 AsyncOperation抽象设计

```cpp
// async_operation.hpp - 异步操作抽象
#pragma once

#ifdef __linux__

#include <cstdint>
#include <functional>
#include <memory>
#include <vector>
#include <sys/socket.h>

namespace io_uring_proactor {

// 操作类型枚举
enum class OpType : uint8_t {
    NONE = 0,
    ACCEPT,
    CONNECT,
    RECV,
    SEND,
    READ,
    WRITE,
    CLOSE,
    TIMEOUT,
    CANCEL
};

// 操作结果
struct OpResult {
    int32_t result;    // 成功时为字节数/fd，失败时为负的错误码
    uint32_t flags;    // CQE flags

    bool success() const { return result >= 0; }
    int error_code() const { return result < 0 ? -result : 0; }
};

// 完成回调类型
using CompletionCallback = std::function<void(const OpResult&)>;

/*
 * 异步操作基类
 *
 * 设计要点：
 * 1. 每个异步操作对应一个AsyncOperation对象
 * 2. user_data存储AsyncOperation指针
 * 3. 完成时通过user_data找回操作对象
 * 4. 调用回调处理结果
 */
class AsyncOperation {
public:
    AsyncOperation(OpType type, CompletionCallback callback)
        : type_(type)
        , callback_(std::move(callback))
    {}

    virtual ~AsyncOperation() = default;

    // 准备SQE（由子类实现）
    virtual void prepare(io_uring_sqe* sqe) = 0;

    // 处理完成
    void complete(const OpResult& result) {
        if (callback_) {
            callback_(result);
        }
    }

    OpType type() const { return type_; }

    // 获取user_data（本对象地址）
    uint64_t user_data() const {
        return reinterpret_cast<uint64_t>(this);
    }

    // 从user_data恢复对象
    static AsyncOperation* from_user_data(uint64_t data) {
        return reinterpret_cast<AsyncOperation*>(data);
    }

protected:
    OpType type_;
    CompletionCallback callback_;
};

/*
 * Accept操作
 */
class AcceptOp : public AsyncOperation {
public:
    AcceptOp(int listen_fd, CompletionCallback callback)
        : AsyncOperation(OpType::ACCEPT, std::move(callback))
        , listen_fd_(listen_fd)
        , addr_len_(sizeof(addr_))
    {
        memset(&addr_, 0, sizeof(addr_));
    }

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_accept(sqe, listen_fd_,
                            (sockaddr*)&addr_, &addr_len_, 0);
        io_uring_sqe_set_data64(sqe, user_data());
    }

    const sockaddr_in& client_addr() const { return addr_; }

private:
    int listen_fd_;
    sockaddr_in addr_;
    socklen_t addr_len_;
};

/*
 * Connect操作
 */
class ConnectOp : public AsyncOperation {
public:
    ConnectOp(int fd, const sockaddr_in& addr, CompletionCallback callback)
        : AsyncOperation(OpType::CONNECT, std::move(callback))
        , fd_(fd)
        , addr_(addr)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_connect(sqe, fd_,
                             (const sockaddr*)&addr_, sizeof(addr_));
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    int fd_;
    sockaddr_in addr_;
};

/*
 * Recv操作
 */
class RecvOp : public AsyncOperation {
public:
    RecvOp(int fd, void* buf, size_t len, CompletionCallback callback)
        : AsyncOperation(OpType::RECV, std::move(callback))
        , fd_(fd)
        , buf_(buf)
        , len_(len)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_recv(sqe, fd_, buf_, len_, 0);
        io_uring_sqe_set_data64(sqe, user_data());
    }

    void* buffer() { return buf_; }
    size_t length() const { return len_; }

private:
    int fd_;
    void* buf_;
    size_t len_;
};

/*
 * Send操作
 */
class SendOp : public AsyncOperation {
public:
    SendOp(int fd, const void* buf, size_t len, CompletionCallback callback)
        : AsyncOperation(OpType::SEND, std::move(callback))
        , fd_(fd)
        , buf_(buf)
        , len_(len)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_send(sqe, fd_, buf_, len_, 0);
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    int fd_;
    const void* buf_;
    size_t len_;
};

/*
 * Read操作（文件）
 */
class ReadOp : public AsyncOperation {
public:
    ReadOp(int fd, void* buf, size_t len, off_t offset,
           CompletionCallback callback)
        : AsyncOperation(OpType::READ, std::move(callback))
        , fd_(fd)
        , buf_(buf)
        , len_(len)
        , offset_(offset)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_read(sqe, fd_, buf_, len_, offset_);
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    int fd_;
    void* buf_;
    size_t len_;
    off_t offset_;
};

/*
 * Write操作（文件）
 */
class WriteOp : public AsyncOperation {
public:
    WriteOp(int fd, const void* buf, size_t len, off_t offset,
            CompletionCallback callback)
        : AsyncOperation(OpType::WRITE, std::move(callback))
        , fd_(fd)
        , buf_(buf)
        , len_(len)
        , offset_(offset)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_write(sqe, fd_, buf_, len_, offset_);
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    int fd_;
    const void* buf_;
    size_t len_;
    off_t offset_;
};

/*
 * Close操作
 */
class CloseOp : public AsyncOperation {
public:
    CloseOp(int fd, CompletionCallback callback)
        : AsyncOperation(OpType::CLOSE, std::move(callback))
        , fd_(fd)
    {}

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_close(sqe, fd_);
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    int fd_;
};

/*
 * Timeout操作
 */
class TimeoutOp : public AsyncOperation {
public:
    TimeoutOp(uint64_t ms, CompletionCallback callback)
        : AsyncOperation(OpType::TIMEOUT, std::move(callback))
    {
        ts_.tv_sec = ms / 1000;
        ts_.tv_nsec = (ms % 1000) * 1000000;
    }

    void prepare(io_uring_sqe* sqe) override {
        io_uring_prep_timeout(sqe, &ts_, 0, 0);
        io_uring_sqe_set_data64(sqe, user_data());
    }

private:
    __kernel_timespec ts_;
};

} // namespace io_uring_proactor

#endif // __linux__
```

#### 19.2 Proactor核心实现

```cpp
// proactor.hpp - io_uring Proactor核心实现
#pragma once

#ifdef __linux__

#include "async_operation.hpp"
#include <liburing.h>
#include <unordered_set>
#include <list>
#include <mutex>
#include <atomic>
#include <thread>
#include <condition_variable>

namespace io_uring_proactor {

/*
 * Proactor核心类
 *
 * 职责：
 * 1. 管理io_uring实例
 * 2. 接受异步操作请求
 * 3. 分发完成事件
 * 4. 管理操作生命周期
 */
class Proactor {
public:
    struct Config {
        unsigned queue_depth = 1024;      // SQ/CQ深度
        unsigned flags = 0;               // io_uring标志
        bool sqpoll = false;              // 是否使用SQPOLL模式
        unsigned sqpoll_idle_ms = 10000;  // SQPOLL空闲超时
    };

    explicit Proactor(const Config& config = Config()) {
        io_uring_params params = {};

        if (config.sqpoll) {
            params.flags |= IORING_SETUP_SQPOLL;
            params.sq_thread_idle = config.sqpoll_idle_ms;
        }
        params.flags |= config.flags;

        int ret = io_uring_queue_init_params(config.queue_depth, &ring_, &params);
        if (ret < 0) {
            throw std::runtime_error("io_uring_queue_init failed: " +
                                   std::string(strerror(-ret)));
        }

        sqpoll_mode_ = (params.flags & IORING_SETUP_SQPOLL) != 0;

        std::cout << "[Proactor] 初始化成功"
                  << ", 队列深度: " << config.queue_depth
                  << ", SQPOLL: " << (sqpoll_mode_ ? "启用" : "禁用")
                  << std::endl;
    }

    ~Proactor() {
        stop();
        io_uring_queue_exit(&ring_);

        // 清理未完成的操作
        std::lock_guard<std::mutex> lock(ops_mutex_);
        for (auto* op : pending_ops_) {
            delete op;
        }
    }

    // 禁止拷贝
    Proactor(const Proactor&) = delete;
    Proactor& operator=(const Proactor&) = delete;

    /*
     * 提交异步操作
     *
     * @param op 操作对象（Proactor接管所有权）
     * @return true表示提交成功
     */
    bool submit(AsyncOperation* op) {
        if (!running_) {
            delete op;
            return false;
        }

        std::lock_guard<std::mutex> lock(submit_mutex_);

        io_uring_sqe* sqe = io_uring_get_sqe(&ring_);
        if (!sqe) {
            // SQ已满，尝试提交后再获取
            io_uring_submit(&ring_);
            sqe = io_uring_get_sqe(&ring_);
            if (!sqe) {
                delete op;
                return false;
            }
        }

        // 准备SQE
        op->prepare(sqe);

        // 记录操作
        {
            std::lock_guard<std::mutex> ops_lock(ops_mutex_);
            pending_ops_.insert(op);
        }

        // 在非SQPOLL模式下需要显式提交
        if (!sqpoll_mode_) {
            io_uring_submit(&ring_);
        }

        return true;
    }

    /*
     * 批量提交操作
     */
    int submit_batch(std::vector<AsyncOperation*>& ops) {
        if (!running_ || ops.empty()) {
            for (auto* op : ops) delete op;
            ops.clear();
            return 0;
        }

        std::lock_guard<std::mutex> lock(submit_mutex_);

        int submitted = 0;
        for (auto* op : ops) {
            io_uring_sqe* sqe = io_uring_get_sqe(&ring_);
            if (!sqe) {
                io_uring_submit(&ring_);
                sqe = io_uring_get_sqe(&ring_);
                if (!sqe) {
                    delete op;
                    continue;
                }
            }

            op->prepare(sqe);

            {
                std::lock_guard<std::mutex> ops_lock(ops_mutex_);
                pending_ops_.insert(op);
            }

            submitted++;
        }

        ops.clear();

        if (!sqpoll_mode_ && submitted > 0) {
            io_uring_submit(&ring_);
        }

        return submitted;
    }

    /*
     * 处理完成事件
     *
     * @param timeout_ms 超时时间（-1表示无限等待）
     * @return 处理的事件数量
     */
    int poll(int timeout_ms = -1) {
        if (!running_) return 0;

        io_uring_cqe* cqe;
        int ret;

        if (timeout_ms < 0) {
            ret = io_uring_wait_cqe(&ring_, &cqe);
        } else if (timeout_ms == 0) {
            ret = io_uring_peek_cqe(&ring_, &cqe);
        } else {
            __kernel_timespec ts = {
                .tv_sec = timeout_ms / 1000,
                .tv_nsec = (timeout_ms % 1000) * 1000000
            };
            ret = io_uring_wait_cqe_timeout(&ring_, &cqe, &ts);
        }

        if (ret < 0) {
            if (ret == -ETIME || ret == -EAGAIN) {
                return 0;  // 超时或无事件
            }
            return ret;  // 错误
        }

        // 处理完成事件
        int processed = 0;
        unsigned head;
        io_uring_for_each_cqe(&ring_, head, cqe) {
            process_cqe(cqe);
            processed++;
        }

        io_uring_cq_advance(&ring_, processed);
        return processed;
    }

    /*
     * 运行事件循环
     */
    void run() {
        running_ = true;

        std::cout << "[Proactor] 事件循环启动" << std::endl;

        while (running_) {
            int ret = poll(100);  // 100ms超时
            if (ret < 0 && ret != -ETIME) {
                std::cerr << "[Proactor] poll错误: " << strerror(-ret) << std::endl;
            }
        }

        std::cout << "[Proactor] 事件循环退出" << std::endl;
    }

    /*
     * 在后台线程运行
     */
    void run_in_thread() {
        event_thread_ = std::thread(&Proactor::run, this);
    }

    /*
     * 停止事件循环
     */
    void stop() {
        running_ = false;
        if (event_thread_.joinable()) {
            event_thread_.join();
        }
    }

    bool is_running() const { return running_; }

    // 统计信息
    uint64_t total_operations() const { return total_ops_.load(); }
    uint64_t total_completions() const { return total_completions_.load(); }

private:
    void process_cqe(io_uring_cqe* cqe) {
        uint64_t user_data = io_uring_cqe_get_data64(cqe);

        // 特殊user_data处理（如唤醒信号）
        if (user_data == 0) {
            return;
        }

        auto* op = AsyncOperation::from_user_data(user_data);

        // 从pending集合移除
        {
            std::lock_guard<std::mutex> lock(ops_mutex_);
            pending_ops_.erase(op);
        }

        // 调用完成回调
        OpResult result;
        result.result = cqe->res;
        result.flags = cqe->flags;

        op->complete(result);

        // 删除操作对象
        delete op;

        total_completions_.fetch_add(1);
    }

private:
    io_uring ring_;
    bool sqpoll_mode_ = false;

    std::atomic<bool> running_{false};
    std::thread event_thread_;

    std::mutex submit_mutex_;
    std::mutex ops_mutex_;
    std::unordered_set<AsyncOperation*> pending_ops_;

    std::atomic<uint64_t> total_ops_{0};
    std::atomic<uint64_t> total_completions_{0};
};

/*
 * 便捷函数：创建并提交操作
 */

inline bool async_accept(Proactor& proactor, int listen_fd,
                         CompletionCallback callback) {
    auto* op = new AcceptOp(listen_fd, std::move(callback));
    return proactor.submit(op);
}

inline bool async_recv(Proactor& proactor, int fd, void* buf, size_t len,
                       CompletionCallback callback) {
    auto* op = new RecvOp(fd, buf, len, std::move(callback));
    return proactor.submit(op);
}

inline bool async_send(Proactor& proactor, int fd, const void* buf, size_t len,
                       CompletionCallback callback) {
    auto* op = new SendOp(fd, buf, len, std::move(callback));
    return proactor.submit(op);
}

inline bool async_close(Proactor& proactor, int fd,
                        CompletionCallback callback = nullptr) {
    auto* op = new CloseOp(fd, std::move(callback));
    return proactor.submit(op);
}

inline bool async_timeout(Proactor& proactor, uint64_t ms,
                          CompletionCallback callback) {
    auto* op = new TimeoutOp(ms, std::move(callback));
    return proactor.submit(op);
}

} // namespace io_uring_proactor

#endif // __linux__
```

#### 19.3 io_uring Echo服务器

```cpp
// io_uring_echo_server.hpp - 基于io_uring的Echo服务器
#pragma once

#ifdef __linux__

#include "proactor.hpp"
#include <unordered_map>
#include <netinet/tcp.h>

namespace io_uring_proactor {

// 连接上下文
struct Connection {
    int fd;
    sockaddr_in addr;
    std::vector<char> recv_buffer;
    std::vector<char> send_buffer;

    Connection(int fd_, const sockaddr_in& addr_)
        : fd(fd_), addr(addr_), recv_buffer(8192) {}
};

// Echo服务器
class EchoServer {
public:
    EchoServer(uint16_t port, Proactor& proactor)
        : port_(port)
        , proactor_(proactor)
    {}

    ~EchoServer() {
        stop();
    }

    bool start() {
        // 创建监听socket
        listen_fd_ = socket(AF_INET, SOCK_STREAM, 0);
        if (listen_fd_ < 0) {
            std::cerr << "socket failed: " << strerror(errno) << std::endl;
            return false;
        }

        // 设置选项
        int opt = 1;
        setsockopt(listen_fd_, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));
        setsockopt(listen_fd_, SOL_SOCKET, SO_REUSEPORT, &opt, sizeof(opt));

        // 绑定
        sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_addr.s_addr = INADDR_ANY;
        addr.sin_port = htons(port_);

        if (bind(listen_fd_, (sockaddr*)&addr, sizeof(addr)) < 0) {
            std::cerr << "bind failed: " << strerror(errno) << std::endl;
            close(listen_fd_);
            return false;
        }

        // 监听
        if (listen(listen_fd_, SOMAXCONN) < 0) {
            std::cerr << "listen failed: " << strerror(errno) << std::endl;
            close(listen_fd_);
            return false;
        }

        running_ = true;

        // 投递初始accept
        for (int i = 0; i < ACCEPT_BATCH; ++i) {
            post_accept();
        }

        std::cout << "[EchoServer] 启动，监听端口 " << port_ << std::endl;
        return true;
    }

    void stop() {
        if (!running_) return;
        running_ = false;

        // 关闭监听socket
        if (listen_fd_ >= 0) {
            close(listen_fd_);
            listen_fd_ = -1;
        }

        // 关闭所有连接
        std::lock_guard<std::mutex> lock(conn_mutex_);
        for (auto& [fd, conn] : connections_) {
            close(conn->fd);
        }
        connections_.clear();

        stats_.print();
    }

    // 统计信息
    struct Stats {
        std::atomic<uint64_t> total_connections{0};
        std::atomic<uint64_t> active_connections{0};
        std::atomic<uint64_t> bytes_received{0};
        std::atomic<uint64_t> bytes_sent{0};
        std::atomic<uint64_t> messages{0};

        void print() const {
            std::cout << "\n=== EchoServer Stats ===" << std::endl;
            std::cout << "Total connections: " << total_connections << std::endl;
            std::cout << "Active connections: " << active_connections << std::endl;
            std::cout << "Bytes received: " << bytes_received << std::endl;
            std::cout << "Bytes sent: " << bytes_sent << std::endl;
            std::cout << "Messages: " << messages << std::endl;
        }
    };

    const Stats& stats() const { return stats_; }

private:
    static constexpr int ACCEPT_BATCH = 32;

    void post_accept() {
        if (!running_) return;

        auto* op = new AcceptOp(listen_fd_,
            [this](const OpResult& result) {
                handle_accept(result);
            });
        proactor_.submit(op);
    }

    void handle_accept(const OpResult& result) {
        if (!result.success()) {
            if (result.error_code() != ECANCELED && running_) {
                std::cerr << "[EchoServer] accept失败: "
                          << strerror(result.error_code()) << std::endl;
            }
            // 重新投递accept
            if (running_) post_accept();
            return;
        }

        int client_fd = result.result;

        // 设置TCP_NODELAY
        int opt = 1;
        setsockopt(client_fd, IPPROTO_TCP, TCP_NODELAY, &opt, sizeof(opt));

        // 获取客户端地址（从AcceptOp获取）
        // 注意：这里简化处理，实际应该从AcceptOp中获取
        sockaddr_in client_addr = {};

        // 创建连接
        auto conn = std::make_shared<Connection>(client_fd, client_addr);

        {
            std::lock_guard<std::mutex> lock(conn_mutex_);
            connections_[client_fd] = conn;
        }

        stats_.total_connections++;
        stats_.active_connections++;

        std::cout << "[EchoServer] 新连接 fd=" << client_fd << std::endl;

        // 投递接收
        post_recv(conn);

        // 重新投递accept
        post_accept();
    }

    void post_recv(std::shared_ptr<Connection> conn) {
        if (!running_) return;

        async_recv(proactor_, conn->fd,
                   conn->recv_buffer.data(), conn->recv_buffer.size(),
            [this, conn](const OpResult& result) {
                handle_recv(conn, result);
            });
    }

    void handle_recv(std::shared_ptr<Connection> conn, const OpResult& result) {
        if (!result.success() || result.result == 0) {
            // 连接关闭或错误
            close_connection(conn);
            return;
        }

        size_t bytes = result.result;
        stats_.bytes_received += bytes;
        stats_.messages++;

        // Echo: 发送接收到的数据
        conn->send_buffer.assign(
            conn->recv_buffer.begin(),
            conn->recv_buffer.begin() + bytes
        );

        post_send(conn, bytes);
    }

    void post_send(std::shared_ptr<Connection> conn, size_t len) {
        if (!running_) return;

        async_send(proactor_, conn->fd,
                   conn->send_buffer.data(), len,
            [this, conn](const OpResult& result) {
                handle_send(conn, result);
            });
    }

    void handle_send(std::shared_ptr<Connection> conn, const OpResult& result) {
        if (!result.success()) {
            close_connection(conn);
            return;
        }

        stats_.bytes_sent += result.result;

        // 继续接收
        post_recv(conn);
    }

    void close_connection(std::shared_ptr<Connection> conn) {
        {
            std::lock_guard<std::mutex> lock(conn_mutex_);
            if (connections_.erase(conn->fd) == 0) {
                return;  // 已经关闭
            }
        }

        stats_.active_connections--;
        std::cout << "[EchoServer] 关闭连接 fd=" << conn->fd << std::endl;

        async_close(proactor_, conn->fd);
    }

private:
    uint16_t port_;
    Proactor& proactor_;
    int listen_fd_ = -1;
    std::atomic<bool> running_{false};

    std::mutex conn_mutex_;
    std::unordered_map<int, std::shared_ptr<Connection>> connections_;

    Stats stats_;
};

} // namespace io_uring_proactor

#endif // __linux__
```

---

### 第三周自测题

#### 概念理解题

1. **io_uring的SQ为什么要分成Ring Buffer和SQE Array两部分？**

   <details>
   <summary>参考答案</summary>

   - **支持乱序提交**：应用可以按需要的顺序提交预先准备的SQE
   - **SQE重用**：SQE Array固定大小，可以维护空闲列表避免动态分配
   - **批量处理优化**：内核可以批量读取索引，提高缓存效率
   - **内存布局优化**：SQE较大(64字节)，索引较小(4字节)，分离可以优化内存访问
   </details>

2. **SQPOLL模式的优势和劣势是什么？**

   <details>
   <summary>参考答案</summary>

   优势：
   - 无需系统调用提交，应用直接写SQ
   - 极低延迟，适合高频小I/O

   劣势：
   - 占用一个CPU核心运行内核轮询线程
   - 不活跃时浪费CPU资源（可设置idle超时）
   - 需要特权（CAP_SYS_NICE）
   </details>

3. **链式操作（IO_LINK）有什么用途？**

   <details>
   <summary>参考答案</summary>

   - **原子操作序列**：写入后立即同步，确保顺序
   - **超时控制**：读操作链接timeout，实现读超时
   - **错误传播**：前一个失败，后续被取消
   - **减少延迟**：一次提交多个有序操作
   </details>

#### 代码分析题

4. **以下代码有什么问题？**

   ```cpp
   void async_read_file(Proactor& proactor, int fd) {
       char buffer[1024];
       auto* op = new ReadOp(fd, buffer, sizeof(buffer), 0,
           [](const OpResult& result) {
               // 处理结果
           });
       proactor.submit(op);
   }
   ```

   <details>
   <summary>参考答案</summary>

   问题：`buffer`是栈上变量，函数返回后被销毁，但异步操作还在使用该内存。

   修正：使用堆内存或将buffer放入回调捕获的shared_ptr中。

   ```cpp
   void async_read_file(Proactor& proactor, int fd) {
       auto buffer = std::make_shared<std::array<char, 1024>>();
       auto* op = new ReadOp(fd, buffer->data(), buffer->size(), 0,
           [buffer](const OpResult& result) {
               // buffer在回调中保持有效
           });
       proactor.submit(op);
   }
   ```
   </details>

5. **如何实现带超时的recv操作？**

   <details>
   <summary>参考答案</summary>

   使用链式操作：
   ```cpp
   // SQE1: recv
   io_uring_sqe* sqe1 = ring.get_sqe();
   io_uring_prep_recv(sqe1, fd, buf, len, 0);
   sqe1->flags |= IOSQE_IO_LINK;  // 链接
   io_uring_sqe_set_data64(sqe1, RECV_OP);

   // SQE2: link_timeout
   io_uring_sqe* sqe2 = ring.get_sqe();
   __kernel_timespec ts = {5, 0};  // 5秒超时
   io_uring_prep_link_timeout(sqe2, &ts, 0);
   io_uring_sqe_set_data64(sqe2, TIMEOUT_OP);

   // 如果recv在5秒内完成，timeout被取消
   // 如果超时，recv被取消，返回-ECANCELED
   ```
   </details>

---

### 第三周检验标准

| 检验项 | 标准 | 自评 |
|--------|------|------|
| 理解io_uring架构 | 能画出SQ/CQ架构图 | ☐ |
| 理解SQE/CQE结构 | 能解释各字段含义 | ☐ |
| 掌握liburing | 能使用prep系列函数 | ☐ |
| 理解提交/完成流程 | 能解释数据流向 | ☐ |
| 实现AsyncOperation | 抽象设计合理 | ☐ |
| 实现Proactor核心 | 事件循环正确运行 | ☐ |
| 实现Echo服务器 | 能正确处理连接 | ☐ |
| 理解SQPOLL | 知道何时使用 | ☐ |

---

### 第三周时间分配

| 内容 | 时间 |
|------|------|
| io_uring基础概念 | 8小时 |
| liburing API学习 | 7小时 |
| AsyncOperation设计 | 6小时 |
| Proactor实现 | 8小时 |
| Echo服务器实现 | 6小时 |

---

## 第四周：跨平台Proactor与实战（Day 22-28）

> **本周目标**：设计跨平台异步I/O抽象层，分析Boost.Asio的实现，完成高性能Proactor服务器框架，并进行性能对比测试。

```
┌─────────────────────────────────────────────────────────────────┐
│                    第四周学习路线图                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   Day 22-23            Day 24-25            Day 26-28           │
│   ┌─────────┐         ┌─────────┐         ┌─────────┐          │
│   │跨平台   │         │Boost    │         │ 完整    │          │
│   │抽象设计 │────────▶│Asio分析 │────────▶│ 框架    │          │
│   └─────────┘         └─────────┘         └─────────┘          │
│       │                   │                   │                 │
│       ▼                   ▼                   ▼                 │
│   IAsyncIO接口        io_context          完整框架            │
│   IOCP后端            async_read/write    HTTP服务器          │
│   io_uring后端        strand机制          性能测试            │
│   模拟Proactor        定时器              对比分析            │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Day 22-23：跨平台异步I/O设计（10小时）

#### 22.1 平台抽象层设计

```
跨平台异步I/O设计思路：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  设计目标：                                                             │
│  1. 统一的API接口                                                       │
│  2. 最大化利用平台特性                                                  │
│  3. 编译时选择后端                                                      │
│  4. 零运行时开销                                                        │
│                                                                          │
│  架构图：                                                               │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      应用层                                      │   │
│  │   ┌─────────────────────────────────────────────────────────┐   │   │
│  │   │              IAsyncIO (统一接口)                         │   │   │
│  │   │                                                          │   │   │
│  │   │  async_read()  async_write()  async_accept()  run()     │   │   │
│  │   └─────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼ (编译时多态/模板)                        │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      平台抽象层                                  │   │
│  │   ┌─────────────┐ ┌─────────────┐ ┌─────────────┐               │   │
│  │   │   Windows   │ │    Linux    │ │    macOS    │               │   │
│  │   │    IOCP     │ │  io_uring   │ │    kqueue   │               │   │
│  │   │   Backend   │ │   Backend   │ │   Backend   │               │   │
│  │   └─────────────┘ └─────────────┘ └─────────────┘               │   │
│  │                                                                  │   │
│  │   ┌─────────────────────────────────────────────────┐           │   │
│  │   │            模拟Proactor后端                      │           │   │
│  │   │     (基于epoll/kqueue的Reactor + ThreadPool)    │           │   │
│  │   └─────────────────────────────────────────────────┘           │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              │                                          │
│                              ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      操作系统                                    │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 22.2 统一接口定义

```cpp
// async_io_interface.hpp - 跨平台异步I/O接口定义
#pragma once

#include <cstdint>
#include <functional>
#include <memory>
#include <chrono>
#include <system_error>

namespace cross_platform {

// 错误码
enum class AsyncError {
    SUCCESS = 0,
    CANCELLED,
    TIMEOUT,
    CONNECTION_CLOSED,
    CONNECTION_REFUSED,
    NETWORK_ERROR,
    BUFFER_OVERFLOW,
    INVALID_ARGUMENT,
    OPERATION_ABORTED,
    UNKNOWN
};

// 错误类别
class AsyncErrorCategory : public std::error_category {
public:
    const char* name() const noexcept override { return "async_io"; }

    std::string message(int ev) const override {
        switch (static_cast<AsyncError>(ev)) {
            case AsyncError::SUCCESS: return "Success";
            case AsyncError::CANCELLED: return "Operation cancelled";
            case AsyncError::TIMEOUT: return "Operation timed out";
            case AsyncError::CONNECTION_CLOSED: return "Connection closed";
            case AsyncError::CONNECTION_REFUSED: return "Connection refused";
            case AsyncError::NETWORK_ERROR: return "Network error";
            case AsyncError::BUFFER_OVERFLOW: return "Buffer overflow";
            case AsyncError::INVALID_ARGUMENT: return "Invalid argument";
            case AsyncError::OPERATION_ABORTED: return "Operation aborted";
            default: return "Unknown error";
        }
    }
};

inline const AsyncErrorCategory& async_error_category() {
    static AsyncErrorCategory instance;
    return instance;
}

inline std::error_code make_error_code(AsyncError e) {
    return {static_cast<int>(e), async_error_category()};
}

// 完成结果
struct CompletionResult {
    std::error_code error;
    size_t bytes_transferred = 0;

    bool success() const { return !error; }
    explicit operator bool() const { return success(); }
};

// 回调类型
using CompletionHandler = std::function<void(const CompletionResult&)>;
using AcceptHandler = std::function<void(const CompletionResult&, int /*new_fd*/)>;
using ConnectHandler = std::function<void(const CompletionResult&)>;
using TimerHandler = std::function<void(const CompletionResult&)>;

// 缓冲区
struct Buffer {
    void* data;
    size_t size;

    Buffer() : data(nullptr), size(0) {}
    Buffer(void* d, size_t s) : data(d), size(s) {}

    template<typename T, size_t N>
    Buffer(T (&arr)[N]) : data(arr), size(N * sizeof(T)) {}

    template<typename Container>
    static Buffer from_container(Container& c) {
        return Buffer(c.data(), c.size() * sizeof(typename Container::value_type));
    }
};

struct ConstBuffer {
    const void* data;
    size_t size;

    ConstBuffer() : data(nullptr), size(0) {}
    ConstBuffer(const void* d, size_t s) : data(d), size(s) {}

    template<typename T, size_t N>
    ConstBuffer(const T (&arr)[N]) : data(arr), size(N * sizeof(T)) {}

    template<typename Container>
    static ConstBuffer from_container(const Container& c) {
        return ConstBuffer(c.data(), c.size() * sizeof(typename Container::value_type));
    }
};

/*
 * IAsyncIO - 异步I/O接口
 *
 * 定义了所有后端必须实现的接口
 */
class IAsyncIO {
public:
    virtual ~IAsyncIO() = default;

    // 运行事件循环
    virtual void run() = 0;

    // 运行一次（非阻塞）
    virtual size_t poll() = 0;

    // 停止事件循环
    virtual void stop() = 0;

    // 是否在运行
    virtual bool is_running() const = 0;

    // 异步读取
    virtual void async_read(
        int fd,
        Buffer buffer,
        CompletionHandler handler
    ) = 0;

    // 异步写入
    virtual void async_write(
        int fd,
        ConstBuffer buffer,
        CompletionHandler handler
    ) = 0;

    // 异步Accept
    virtual void async_accept(
        int listen_fd,
        AcceptHandler handler
    ) = 0;

    // 异步Connect
    virtual void async_connect(
        int fd,
        const void* addr,
        size_t addr_len,
        ConnectHandler handler
    ) = 0;

    // 异步定时器
    virtual void async_wait(
        std::chrono::milliseconds timeout,
        TimerHandler handler
    ) = 0;

    // 投递到事件循环执行
    virtual void post(std::function<void()> func) = 0;

    // 取消操作（如果支持）
    virtual void cancel(int fd) { /* 默认空实现 */ }
};

/*
 * 工厂函数声明（由各平台实现提供）
 */
std::unique_ptr<IAsyncIO> create_async_io();

} // namespace cross_platform
```

#### 22.3 模拟Proactor后端（基于epoll）

```cpp
// simulated_proactor_backend.hpp - 基于epoll的模拟Proactor
#pragma once

#ifdef __linux__

#include "async_io_interface.hpp"
#include <sys/epoll.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <unistd.h>
#include <fcntl.h>
#include <thread>
#include <mutex>
#include <queue>
#include <unordered_map>
#include <atomic>
#include <condition_variable>

namespace cross_platform {

/*
 * 模拟Proactor的实现思路：
 *
 * ┌─────────────────────────────────────────────────────────────────────┐
 * │                                                                      │
 * │  真正的Proactor：                                                   │
 * │  应用 ──▶ 提交异步操作 ──▶ 内核执行 ──▶ 完成通知 ──▶ 回调         │
 * │                                                                      │
 * │  模拟Proactor：                                                     │
 * │  应用 ──▶ 提交操作 ──▶ Reactor等待就绪 ──▶ 线程池执行I/O          │
 * │                      ──▶ 完成通知 ──▶ 回调                         │
 * │                                                                      │
 * │  关键：使用线程池在后台执行实际的I/O操作                           │
 * │                                                                      │
 * └─────────────────────────────────────────────────────────────────────┘
 */

// 操作类型
enum class SimOpType {
    READ,
    WRITE,
    ACCEPT,
    CONNECT,
    TIMER
};

// 待处理操作
struct PendingOp {
    SimOpType type;
    int fd;
    Buffer buffer;
    ConstBuffer const_buffer;
    void* addr;
    size_t addr_len;
    std::chrono::steady_clock::time_point deadline;

    CompletionHandler completion_handler;
    AcceptHandler accept_handler;
    ConnectHandler connect_handler;
    TimerHandler timer_handler;
};

// 线程池
class ThreadPool {
public:
    ThreadPool(size_t num_threads = 4) : stop_(false) {
        for (size_t i = 0; i < num_threads; ++i) {
            workers_.emplace_back([this] {
                while (true) {
                    std::function<void()> task;
                    {
                        std::unique_lock<std::mutex> lock(mutex_);
                        cv_.wait(lock, [this] {
                            return stop_ || !tasks_.empty();
                        });
                        if (stop_ && tasks_.empty()) return;
                        task = std::move(tasks_.front());
                        tasks_.pop();
                    }
                    task();
                }
            });
        }
    }

    ~ThreadPool() {
        {
            std::lock_guard<std::mutex> lock(mutex_);
            stop_ = true;
        }
        cv_.notify_all();
        for (auto& t : workers_) {
            if (t.joinable()) t.join();
        }
    }

    template<typename F>
    void enqueue(F&& f) {
        {
            std::lock_guard<std::mutex> lock(mutex_);
            tasks_.push(std::forward<F>(f));
        }
        cv_.notify_one();
    }

private:
    std::vector<std::thread> workers_;
    std::queue<std::function<void()>> tasks_;
    std::mutex mutex_;
    std::condition_variable cv_;
    bool stop_;
};

// 模拟Proactor实现
class SimulatedProactor : public IAsyncIO {
public:
    SimulatedProactor(size_t thread_pool_size = 4)
        : thread_pool_(thread_pool_size)
        , running_(false)
    {
        epfd_ = epoll_create1(EPOLL_CLOEXEC);
        if (epfd_ < 0) {
            throw std::runtime_error("epoll_create1 failed");
        }

        // 创建事件fd用于唤醒
        wake_fd_ = eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC);
        if (wake_fd_ < 0) {
            close(epfd_);
            throw std::runtime_error("eventfd failed");
        }

        // 注册wake_fd
        epoll_event ev = {};
        ev.events = EPOLLIN;
        ev.data.fd = wake_fd_;
        epoll_ctl(epfd_, EPOLL_CTL_ADD, wake_fd_, &ev);
    }

    ~SimulatedProactor() {
        stop();
        close(wake_fd_);
        close(epfd_);
    }

    void run() override {
        running_ = true;

        while (running_) {
            // 处理定时器
            process_timers();

            // 等待事件
            epoll_event events[64];
            int timeout = get_next_timer_timeout();
            int n = epoll_wait(epfd_, events, 64, timeout);

            for (int i = 0; i < n; ++i) {
                if (events[i].data.fd == wake_fd_) {
                    // 消费wake事件
                    uint64_t val;
                    read(wake_fd_, &val, sizeof(val));
                    continue;
                }

                handle_event(events[i].data.fd, events[i].events);
            }

            // 处理投递的任务
            process_posted_tasks();
        }
    }

    size_t poll() override {
        process_timers();

        epoll_event events[64];
        int n = epoll_wait(epfd_, events, 64, 0);

        size_t processed = 0;
        for (int i = 0; i < n; ++i) {
            if (events[i].data.fd == wake_fd_) {
                uint64_t val;
                read(wake_fd_, &val, sizeof(val));
                continue;
            }
            handle_event(events[i].data.fd, events[i].events);
            processed++;
        }

        process_posted_tasks();
        return processed;
    }

    void stop() override {
        running_ = false;
        wake();
    }

    bool is_running() const override { return running_; }

    void async_read(int fd, Buffer buffer, CompletionHandler handler) override {
        // 设置非阻塞
        set_nonblocking(fd);

        // 添加到epoll监听
        add_fd(fd, EPOLLIN);

        // 保存操作信息
        auto op = std::make_shared<PendingOp>();
        op->type = SimOpType::READ;
        op->fd = fd;
        op->buffer = buffer;
        op->completion_handler = std::move(handler);

        std::lock_guard<std::mutex> lock(ops_mutex_);
        pending_reads_[fd] = op;
    }

    void async_write(int fd, ConstBuffer buffer, CompletionHandler handler) override {
        set_nonblocking(fd);
        add_fd(fd, EPOLLOUT);

        auto op = std::make_shared<PendingOp>();
        op->type = SimOpType::WRITE;
        op->fd = fd;
        op->const_buffer = buffer;
        op->completion_handler = std::move(handler);

        std::lock_guard<std::mutex> lock(ops_mutex_);
        pending_writes_[fd] = op;
    }

    void async_accept(int listen_fd, AcceptHandler handler) override {
        set_nonblocking(listen_fd);
        add_fd(listen_fd, EPOLLIN);

        auto op = std::make_shared<PendingOp>();
        op->type = SimOpType::ACCEPT;
        op->fd = listen_fd;
        op->accept_handler = std::move(handler);

        std::lock_guard<std::mutex> lock(ops_mutex_);
        pending_accepts_[listen_fd] = op;
    }

    void async_connect(int fd, const void* addr, size_t addr_len,
                       ConnectHandler handler) override {
        set_nonblocking(fd);

        // 发起非阻塞connect
        int ret = ::connect(fd, (const sockaddr*)addr, addr_len);
        if (ret == 0) {
            // 立即完成
            post([handler] {
                handler(CompletionResult{});
            });
            return;
        }

        if (errno != EINPROGRESS) {
            post([handler, err = errno] {
                CompletionResult result;
                result.error = std::make_error_code(
                    static_cast<std::errc>(err));
                handler(result);
            });
            return;
        }

        // 等待可写表示连接完成
        add_fd(fd, EPOLLOUT);

        auto op = std::make_shared<PendingOp>();
        op->type = SimOpType::CONNECT;
        op->fd = fd;
        op->connect_handler = std::move(handler);

        std::lock_guard<std::mutex> lock(ops_mutex_);
        pending_connects_[fd] = op;
    }

    void async_wait(std::chrono::milliseconds timeout,
                    TimerHandler handler) override {
        auto op = std::make_shared<PendingOp>();
        op->type = SimOpType::TIMER;
        op->deadline = std::chrono::steady_clock::now() + timeout;
        op->timer_handler = std::move(handler);

        std::lock_guard<std::mutex> lock(timer_mutex_);
        timers_.push(op);
    }

    void post(std::function<void()> func) override {
        {
            std::lock_guard<std::mutex> lock(post_mutex_);
            posted_tasks_.push(std::move(func));
        }
        wake();
    }

    void cancel(int fd) override {
        remove_fd(fd);

        std::lock_guard<std::mutex> lock(ops_mutex_);
        pending_reads_.erase(fd);
        pending_writes_.erase(fd);
        pending_accepts_.erase(fd);
        pending_connects_.erase(fd);
    }

private:
    void set_nonblocking(int fd) {
        int flags = fcntl(fd, F_GETFL, 0);
        fcntl(fd, F_SETFL, flags | O_NONBLOCK);
    }

    void add_fd(int fd, uint32_t events) {
        epoll_event ev = {};
        ev.events = events | EPOLLET;
        ev.data.fd = fd;

        if (epoll_ctl(epfd_, EPOLL_CTL_MOD, fd, &ev) < 0) {
            if (errno == ENOENT) {
                epoll_ctl(epfd_, EPOLL_CTL_ADD, fd, &ev);
            }
        }
    }

    void remove_fd(int fd) {
        epoll_ctl(epfd_, EPOLL_CTL_DEL, fd, nullptr);
    }

    void wake() {
        uint64_t val = 1;
        write(wake_fd_, &val, sizeof(val));
    }

    void handle_event(int fd, uint32_t events) {
        std::shared_ptr<PendingOp> op;

        // 查找对应的操作
        {
            std::lock_guard<std::mutex> lock(ops_mutex_);

            if (events & EPOLLIN) {
                auto it = pending_accepts_.find(fd);
                if (it != pending_accepts_.end()) {
                    op = it->second;
                    pending_accepts_.erase(it);
                } else {
                    it = pending_reads_.find(fd);
                    if (it != pending_reads_.end()) {
                        op = it->second;
                        pending_reads_.erase(it);
                    }
                }
            }

            if (!op && (events & EPOLLOUT)) {
                auto it = pending_connects_.find(fd);
                if (it != pending_connects_.end()) {
                    op = it->second;
                    pending_connects_.erase(it);
                } else {
                    it = pending_writes_.find(fd);
                    if (it != pending_writes_.end()) {
                        op = it->second;
                        pending_writes_.erase(it);
                    }
                }
            }
        }

        if (!op) return;

        // 在线程池中执行实际I/O
        thread_pool_.enqueue([this, op, events] {
            execute_io(op, events);
        });
    }

    void execute_io(std::shared_ptr<PendingOp> op, uint32_t events) {
        CompletionResult result;

        switch (op->type) {
            case SimOpType::READ: {
                ssize_t n = read(op->fd, op->buffer.data, op->buffer.size);
                if (n > 0) {
                    result.bytes_transferred = n;
                } else if (n == 0) {
                    result.error = make_error_code(AsyncError::CONNECTION_CLOSED);
                } else {
                    result.error = std::make_error_code(
                        static_cast<std::errc>(errno));
                }
                if (op->completion_handler) {
                    post([op, result] { op->completion_handler(result); });
                }
                break;
            }

            case SimOpType::WRITE: {
                ssize_t n = write(op->fd, op->const_buffer.data,
                                  op->const_buffer.size);
                if (n >= 0) {
                    result.bytes_transferred = n;
                } else {
                    result.error = std::make_error_code(
                        static_cast<std::errc>(errno));
                }
                if (op->completion_handler) {
                    post([op, result] { op->completion_handler(result); });
                }
                break;
            }

            case SimOpType::ACCEPT: {
                sockaddr_in addr;
                socklen_t len = sizeof(addr);
                int new_fd = accept4(op->fd, (sockaddr*)&addr, &len,
                                    SOCK_NONBLOCK);
                if (new_fd >= 0) {
                    if (op->accept_handler) {
                        post([op, result, new_fd] {
                            op->accept_handler(result, new_fd);
                        });
                    }
                } else {
                    result.error = std::make_error_code(
                        static_cast<std::errc>(errno));
                    if (op->accept_handler) {
                        post([op, result] {
                            op->accept_handler(result, -1);
                        });
                    }
                }
                break;
            }

            case SimOpType::CONNECT: {
                // 检查连接结果
                int err = 0;
                socklen_t len = sizeof(err);
                getsockopt(op->fd, SOL_SOCKET, SO_ERROR, &err, &len);

                if (err != 0) {
                    result.error = std::make_error_code(
                        static_cast<std::errc>(err));
                }

                if (op->connect_handler) {
                    post([op, result] { op->connect_handler(result); });
                }
                break;
            }

            default:
                break;
        }
    }

    void process_timers() {
        auto now = std::chrono::steady_clock::now();
        std::vector<std::shared_ptr<PendingOp>> expired;

        {
            std::lock_guard<std::mutex> lock(timer_mutex_);
            while (!timers_.empty() && timers_.top()->deadline <= now) {
                expired.push_back(timers_.top());
                timers_.pop();
            }
        }

        for (auto& op : expired) {
            CompletionResult result;
            result.error = make_error_code(AsyncError::TIMEOUT);
            if (op->timer_handler) {
                op->timer_handler(result);
            }
        }
    }

    int get_next_timer_timeout() {
        std::lock_guard<std::mutex> lock(timer_mutex_);
        if (timers_.empty()) return 100;  // 默认100ms

        auto now = std::chrono::steady_clock::now();
        auto next = timers_.top()->deadline;

        if (next <= now) return 0;

        auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(
            next - now).count();
        return std::min(static_cast<long long>(ms), 100LL);
    }

    void process_posted_tasks() {
        std::queue<std::function<void()>> tasks;
        {
            std::lock_guard<std::mutex> lock(post_mutex_);
            tasks.swap(posted_tasks_);
        }

        while (!tasks.empty()) {
            tasks.front()();
            tasks.pop();
        }
    }

    // 定时器比较器
    struct TimerCompare {
        bool operator()(const std::shared_ptr<PendingOp>& a,
                       const std::shared_ptr<PendingOp>& b) const {
            return a->deadline > b->deadline;
        }
    };

private:
    int epfd_;
    int wake_fd_;
    ThreadPool thread_pool_;
    std::atomic<bool> running_;

    std::mutex ops_mutex_;
    std::unordered_map<int, std::shared_ptr<PendingOp>> pending_reads_;
    std::unordered_map<int, std::shared_ptr<PendingOp>> pending_writes_;
    std::unordered_map<int, std::shared_ptr<PendingOp>> pending_accepts_;
    std::unordered_map<int, std::shared_ptr<PendingOp>> pending_connects_;

    std::mutex timer_mutex_;
    std::priority_queue<std::shared_ptr<PendingOp>,
                       std::vector<std::shared_ptr<PendingOp>>,
                       TimerCompare> timers_;

    std::mutex post_mutex_;
    std::queue<std::function<void()>> posted_tasks_;
};

// 工厂函数
inline std::unique_ptr<IAsyncIO> create_async_io() {
    // 可以在这里选择使用io_uring或模拟Proactor
    return std::make_unique<SimulatedProactor>();
}

} // namespace cross_platform

#endif // __linux__
```

### Day 24-25：Boost.Asio源码分析（10小时）

#### 24.1 Asio整体架构

```
Boost.Asio架构分析：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Asio是最广泛使用的C++异步I/O库，其设计思想对后来的网络库影响深远     │
│                                                                          │
│  核心组件：                                                             │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                                                                  │   │
│  │  ┌─────────────────────────────────────────────────────────┐   │   │
│  │  │                  io_context                              │   │   │
│  │  │  (事件循环核心，管理I/O服务和完成处理器)               │   │   │
│  │  └─────────────────────────────────────────────────────────┘   │   │
│  │                           │                                     │   │
│  │           ┌───────────────┼───────────────┐                    │   │
│  │           ▼               ▼               ▼                    │   │
│  │  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐           │   │
│  │  │   socket     │ │   timer      │ │   signal     │           │   │
│  │  │   service    │ │   service    │ │   service    │           │   │
│  │  └──────────────┘ └──────────────┘ └──────────────┘           │   │
│  │           │               │               │                    │   │
│  │           └───────────────┼───────────────┘                    │   │
│  │                           ▼                                     │   │
│  │  ┌─────────────────────────────────────────────────────────┐   │   │
│  │  │              Platform-specific Backend                   │   │   │
│  │  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐       │   │   │
│  │  │  │  IOCP   │ │ io_uring│ │  epoll  │ │ kqueue  │       │   │   │
│  │  │  │(Windows)│ │ (Linux) │ │ (Linux) │ │ (macOS) │       │   │   │
│  │  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘       │   │   │
│  │  └─────────────────────────────────────────────────────────┘   │   │
│  │                                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                          │
│  设计模式：                                                             │
│  - Proactor模式：async_xxx + handler                                   │
│  - 服务定位器模式：io_context管理各类服务                              │
│  - 链式调用：使用strand保证串行执行                                    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

io_context工作流程：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. 初始化阶段                                                          │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  io_context ctx;                                                   │ │
│  │  // 创建scheduler（调度器）                                       │ │
│  │  // 创建平台特定的reactor/proactor                                │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  2. 注册异步操作                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  tcp::socket sock(ctx);                                            │ │
│  │  sock.async_read_some(buffer, handler);                           │ │
│  │  // 操作被提交到scheduler的操作队列                               │ │
│  │  // reactor/proactor开始监控fd                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  3. 运行事件循环                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  ctx.run();                                                        │ │
│  │  while (有待处理的工作) {                                          │ │
│  │      // 等待I/O事件或定时器                                       │ │
│  │      reactor.wait();                                               │ │
│  │      // 执行就绪的处理器                                          │ │
│  │      scheduler.run_ready_handlers();                              │ │
│  │  }                                                                 │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  4. 完成处理                                                            │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  // I/O完成后                                                      │ │
│  │  // handler被加入scheduler的完成队列                              │ │
│  │  // run()中执行handler                                             │ │
│  │  handler(error_code, bytes_transferred);                          │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 24.2 strand串行化机制

```
strand机制详解：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  问题：多线程运行io_context时，同一连接的多个handler可能并发执行      │
│                                                                          │
│  场景示例：                                                             │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  Thread 1                    Thread 2                              │ │
│  │  ┌──────────────┐           ┌──────────────┐                      │ │
│  │  │ read_handler │           │ write_handler│                      │ │
│  │  │   (conn A)   │           │   (conn A)   │                      │ │
│  │  └──────────────┘           └──────────────┘                      │ │
│  │         │                          │                               │ │
│  │         └──── 同时访问conn A ──────┘                              │ │
│  │                    ↓                                               │ │
│  │              数据竞争！                                            │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  strand解决方案：                                                       │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  io_context::strand strand_(ctx);                                 │ │
│  │                                                                    │ │
│  │  // 所有通过strand投递的handler串行执行                           │ │
│  │  sock.async_read_some(buffer,                                     │ │
│  │      boost::asio::bind_executor(strand_, read_handler));         │ │
│  │                                                                    │ │
│  │  sock.async_write_some(buffer,                                    │ │
│  │      boost::asio::bind_executor(strand_, write_handler));        │ │
│  │                                                                    │ │
│  │  执行顺序保证：                                                   │ │
│  │  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐        │ │
│  │  │ read_handler │───▶│ write_handler│───▶│ next_handler │        │ │
│  │  └──────────────┘    └──────────────┘    └──────────────┘        │ │
│  │         串行执行，无需额外加锁                                     │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  strand内部实现：                                                       │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  class strand {                                                    │ │
│  │      mutex mutex_;                                                 │ │
│  │      queue<handler> handlers_;  // 待执行队列                     │ │
│  │      bool running_;              // 是否有handler正在执行         │ │
│  │                                                                    │ │
│  │      void post(handler h) {                                       │ │
│  │          lock_guard lock(mutex_);                                 │ │
│  │          handlers_.push(h);                                       │ │
│  │          if (!running_) {                                         │ │
│  │              running_ = true;                                     │ │
│  │              // 投递到io_context执行                              │ │
│  │              io_context_.post(run_handlers);                      │ │
│  │          }                                                        │ │
│  │      }                                                            │ │
│  │                                                                    │ │
│  │      void run_handlers() {                                        │ │
│  │          while (true) {                                           │ │
│  │              handler h;                                           │ │
│  │              {                                                    │ │
│  │                  lock_guard lock(mutex_);                         │ │
│  │                  if (handlers_.empty()) {                         │ │
│  │                      running_ = false;                            │ │
│  │                      return;                                      │ │
│  │                  }                                                │ │
│  │                  h = handlers_.front();                           │ │
│  │                  handlers_.pop();                                 │ │
│  │              }                                                    │ │
│  │              h();  // 执行handler（锁外）                         │ │
│  │          }                                                        │ │
│  │      }                                                            │ │
│  │  };                                                               │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 24.3 Asio风格的async_read/async_write

```cpp
// asio_style_operations.hpp - Asio风格的异步操作
#pragma once

#include "async_io_interface.hpp"
#include <vector>

namespace cross_platform {

/*
 * composed operation：组合操作
 *
 * async_read/async_write是组合操作，可能需要多次底层I/O
 * 比如读取100字节，可能需要多次read才能完成
 */

// 异步读取指定字节数
template<typename AsyncIO, typename DynamicBuffer, typename Handler>
class async_read_op {
public:
    async_read_op(AsyncIO& io, int fd, DynamicBuffer& buffer,
                  size_t bytes_to_read, Handler handler)
        : io_(io)
        , fd_(fd)
        , buffer_(buffer)
        , bytes_to_read_(bytes_to_read)
        , bytes_read_(0)
        , handler_(std::move(handler))
    {}

    void start() {
        do_read();
    }

private:
    void do_read() {
        size_t remaining = bytes_to_read_ - bytes_read_;
        if (remaining == 0) {
            // 完成
            CompletionResult result;
            result.bytes_transferred = bytes_read_;
            handler_(result);
            return;
        }

        // 确保buffer有足够空间
        size_t old_size = buffer_.size();
        buffer_.resize(old_size + remaining);

        Buffer buf(buffer_.data() + old_size, remaining);

        io_.async_read(fd_, buf,
            [this, old_size](const CompletionResult& result) {
                if (!result.success()) {
                    buffer_.resize(old_size);  // 恢复大小
                    handler_(result);
                    return;
                }

                bytes_read_ += result.bytes_transferred;
                buffer_.resize(old_size + result.bytes_transferred);

                // 继续读取
                do_read();
            });
    }

private:
    AsyncIO& io_;
    int fd_;
    DynamicBuffer& buffer_;
    size_t bytes_to_read_;
    size_t bytes_read_;
    Handler handler_;
};

// 便捷函数
template<typename AsyncIO, typename DynamicBuffer, typename Handler>
void async_read_exactly(AsyncIO& io, int fd, DynamicBuffer& buffer,
                        size_t bytes, Handler handler) {
    auto op = std::make_shared<async_read_op<AsyncIO, DynamicBuffer, Handler>>(
        io, fd, buffer, bytes, std::move(handler)
    );
    op->start();
}

// 异步读取直到分隔符
template<typename AsyncIO, typename DynamicBuffer, typename Handler>
class async_read_until_op {
public:
    async_read_until_op(AsyncIO& io, int fd, DynamicBuffer& buffer,
                        char delimiter, Handler handler)
        : io_(io)
        , fd_(fd)
        , buffer_(buffer)
        , delimiter_(delimiter)
        , handler_(std::move(handler))
        , temp_buffer_(1024)
    {}

    void start() {
        do_read();
    }

private:
    void do_read() {
        Buffer buf(temp_buffer_.data(), temp_buffer_.size());

        io_.async_read(fd_, buf,
            [this](const CompletionResult& result) {
                if (!result.success()) {
                    handler_(result);
                    return;
                }

                // 追加到buffer
                size_t old_size = buffer_.size();
                buffer_.insert(buffer_.end(),
                              temp_buffer_.begin(),
                              temp_buffer_.begin() + result.bytes_transferred);

                // 查找分隔符
                for (size_t i = old_size; i < buffer_.size(); ++i) {
                    if (buffer_[i] == delimiter_) {
                        CompletionResult r;
                        r.bytes_transferred = i + 1;  // 包括分隔符
                        handler_(r);
                        return;
                    }
                }

                // 继续读取
                do_read();
            });
    }

private:
    AsyncIO& io_;
    int fd_;
    DynamicBuffer& buffer_;
    char delimiter_;
    Handler handler_;
    std::vector<char> temp_buffer_;
};

template<typename AsyncIO, typename DynamicBuffer, typename Handler>
void async_read_until(AsyncIO& io, int fd, DynamicBuffer& buffer,
                      char delimiter, Handler handler) {
    auto op = std::make_shared<async_read_until_op<AsyncIO, DynamicBuffer, Handler>>(
        io, fd, buffer, delimiter, std::move(handler)
    );
    op->start();
}

// 异步写入所有数据
template<typename AsyncIO, typename Handler>
class async_write_all_op {
public:
    async_write_all_op(AsyncIO& io, int fd, const std::vector<char>& data,
                       Handler handler)
        : io_(io)
        , fd_(fd)
        , data_(data)
        , bytes_written_(0)
        , handler_(std::move(handler))
    {}

    void start() {
        do_write();
    }

private:
    void do_write() {
        size_t remaining = data_.size() - bytes_written_;
        if (remaining == 0) {
            CompletionResult result;
            result.bytes_transferred = bytes_written_;
            handler_(result);
            return;
        }

        ConstBuffer buf(data_.data() + bytes_written_, remaining);

        io_.async_write(fd_, buf,
            [this](const CompletionResult& result) {
                if (!result.success()) {
                    handler_(result);
                    return;
                }

                bytes_written_ += result.bytes_transferred;
                do_write();
            });
    }

private:
    AsyncIO& io_;
    int fd_;
    std::vector<char> data_;
    size_t bytes_written_;
    Handler handler_;
};

template<typename AsyncIO, typename Handler>
void async_write_all(AsyncIO& io, int fd, const std::vector<char>& data,
                     Handler handler) {
    auto op = std::make_shared<async_write_all_op<AsyncIO, Handler>>(
        io, fd, data, std::move(handler)
    );
    op->start();
}

} // namespace cross_platform
```

### Day 26-27：完整Proactor服务器实战（10小时）

#### 26.1 框架整体设计

```cpp
// proactor_framework.hpp - 完整Proactor服务器框架
#pragma once

#include "async_io_interface.hpp"
#include "asio_style_operations.hpp"
#include <unordered_map>
#include <memory>
#include <functional>
#include <atomic>
#include <mutex>

namespace cross_platform {

// 前向声明
class ProactorServer;

/*
 * 连接会话基类
 *
 * 应用程序通过继承此类实现业务逻辑
 */
class Session : public std::enable_shared_from_this<Session> {
public:
    Session(ProactorServer& server, int fd)
        : server_(server)
        , fd_(fd)
        , closed_(false)
    {}

    virtual ~Session() = default;

    // 开始会话（由框架调用）
    virtual void start() = 0;

    // 连接关闭时调用
    virtual void on_close() {}

    // 发送数据
    void send(const std::string& data);
    void send(const std::vector<char>& data);

    // 关闭连接
    void close();

    int fd() const { return fd_; }
    bool is_closed() const { return closed_; }

protected:
    // 异步读取
    void async_read(size_t max_size, std::function<void(const char*, size_t)> handler);

    // 异步读取指定字节
    void async_read_exactly(size_t bytes, std::function<void(const char*, size_t)> handler);

    // 异步读取直到分隔符
    void async_read_until(char delimiter, std::function<void(const char*, size_t)> handler);

protected:
    ProactorServer& server_;
    int fd_;
    std::atomic<bool> closed_;
    std::vector<char> read_buffer_;
    std::vector<char> write_buffer_;
};

/*
 * Proactor服务器
 */
class ProactorServer {
public:
    using SessionFactory = std::function<std::shared_ptr<Session>(ProactorServer&, int)>;

    ProactorServer(std::unique_ptr<IAsyncIO> io)
        : io_(std::move(io))
    {}

    // 设置会话工厂
    void set_session_factory(SessionFactory factory) {
        session_factory_ = std::move(factory);
    }

    // 监听端口
    bool listen(uint16_t port) {
        listen_fd_ = socket(AF_INET, SOCK_STREAM, 0);
        if (listen_fd_ < 0) return false;

        int opt = 1;
        setsockopt(listen_fd_, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

        sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_addr.s_addr = INADDR_ANY;
        addr.sin_port = htons(port);

        if (bind(listen_fd_, (sockaddr*)&addr, sizeof(addr)) < 0) {
            close(listen_fd_);
            return false;
        }

        if (::listen(listen_fd_, SOMAXCONN) < 0) {
            close(listen_fd_);
            return false;
        }

        port_ = port;
        return true;
    }

    // 启动服务器
    void start() {
        running_ = true;
        start_accept();
        std::cout << "[Server] 启动，监听端口 " << port_ << std::endl;
    }

    // 运行事件循环
    void run() {
        io_->run();
    }

    // 停止服务器
    void stop() {
        running_ = false;
        io_->stop();

        // 关闭所有会话
        std::lock_guard<std::mutex> lock(sessions_mutex_);
        for (auto& [fd, session] : sessions_) {
            session->close();
        }
        sessions_.clear();
    }

    // 获取AsyncIO
    IAsyncIO& io() { return *io_; }

    // 移除会话
    void remove_session(int fd) {
        std::lock_guard<std::mutex> lock(sessions_mutex_);
        sessions_.erase(fd);
        stats_.active_sessions--;
    }

    // 统计信息
    struct Stats {
        std::atomic<uint64_t> total_connections{0};
        std::atomic<uint64_t> active_sessions{0};
        std::atomic<uint64_t> bytes_received{0};
        std::atomic<uint64_t> bytes_sent{0};
    };

    const Stats& stats() const { return stats_; }

private:
    void start_accept() {
        if (!running_) return;

        io_->async_accept(listen_fd_,
            [this](const CompletionResult& result, int new_fd) {
                handle_accept(result, new_fd);
            });
    }

    void handle_accept(const CompletionResult& result, int new_fd) {
        if (!result.success()) {
            if (running_) {
                start_accept();
            }
            return;
        }

        stats_.total_connections++;
        stats_.active_sessions++;

        // 创建会话
        auto session = session_factory_(*this, new_fd);

        {
            std::lock_guard<std::mutex> lock(sessions_mutex_);
            sessions_[new_fd] = session;
        }

        // 启动会话
        session->start();

        // 继续accept
        start_accept();
    }

private:
    std::unique_ptr<IAsyncIO> io_;
    SessionFactory session_factory_;
    int listen_fd_ = -1;
    uint16_t port_ = 0;
    std::atomic<bool> running_{false};

    std::mutex sessions_mutex_;
    std::unordered_map<int, std::shared_ptr<Session>> sessions_;

    Stats stats_;
};

// Session方法实现
inline void Session::send(const std::string& data) {
    send(std::vector<char>(data.begin(), data.end()));
}

inline void Session::send(const std::vector<char>& data) {
    if (closed_) return;

    auto self = shared_from_this();
    async_write_all(server_.io(), fd_, data,
        [self](const CompletionResult& result) {
            if (!result.success()) {
                self->close();
            } else {
                // 可以在这里添加发送完成处理
            }
        });
}

inline void Session::close() {
    if (closed_.exchange(true)) return;

    on_close();
    server_.io().cancel(fd_);
    ::close(fd_);
    server_.remove_session(fd_);
}

inline void Session::async_read(size_t max_size,
                                std::function<void(const char*, size_t)> handler) {
    if (closed_) return;

    read_buffer_.resize(max_size);
    Buffer buf(read_buffer_.data(), max_size);

    auto self = shared_from_this();
    server_.io().async_read(fd_, buf,
        [self, handler](const CompletionResult& result) {
            if (!result.success() || result.bytes_transferred == 0) {
                self->close();
                return;
            }
            handler(self->read_buffer_.data(), result.bytes_transferred);
        });
}

} // namespace cross_platform
```

#### 26.2 HTTP服务器示例

```cpp
// http_server.hpp - 简单HTTP服务器示例
#pragma once

#include "proactor_framework.hpp"
#include <sstream>
#include <map>

namespace cross_platform {

// HTTP请求
struct HttpRequest {
    std::string method;
    std::string path;
    std::string version;
    std::map<std::string, std::string> headers;
    std::string body;

    bool parse(const std::string& raw) {
        std::istringstream stream(raw);
        std::string line;

        // 解析请求行
        if (!std::getline(stream, line)) return false;

        // 移除\r
        if (!line.empty() && line.back() == '\r') {
            line.pop_back();
        }

        std::istringstream request_line(line);
        request_line >> method >> path >> version;

        // 解析头部
        while (std::getline(stream, line)) {
            if (!line.empty() && line.back() == '\r') {
                line.pop_back();
            }
            if (line.empty()) break;

            size_t colon = line.find(':');
            if (colon != std::string::npos) {
                std::string key = line.substr(0, colon);
                std::string value = line.substr(colon + 1);
                // 去除前导空格
                if (!value.empty() && value[0] == ' ') {
                    value = value.substr(1);
                }
                headers[key] = value;
            }
        }

        // 读取body（如果有Content-Length）
        auto it = headers.find("Content-Length");
        if (it != headers.end()) {
            size_t content_length = std::stoul(it->second);
            body.resize(content_length);
            stream.read(&body[0], content_length);
        }

        return true;
    }
};

// HTTP响应
struct HttpResponse {
    int status_code = 200;
    std::string status_text = "OK";
    std::map<std::string, std::string> headers;
    std::string body;

    std::string to_string() const {
        std::ostringstream stream;
        stream << "HTTP/1.1 " << status_code << " " << status_text << "\r\n";

        for (const auto& [key, value] : headers) {
            stream << key << ": " << value << "\r\n";
        }

        stream << "Content-Length: " << body.size() << "\r\n";
        stream << "\r\n";
        stream << body;

        return stream.str();
    }
};

// HTTP会话
class HttpSession : public Session {
public:
    using RequestHandler = std::function<HttpResponse(const HttpRequest&)>;

    HttpSession(ProactorServer& server, int fd, RequestHandler handler)
        : Session(server, fd)
        , request_handler_(std::move(handler))
    {}

    void start() override {
        read_request();
    }

private:
    void read_request() {
        auto self = std::static_pointer_cast<HttpSession>(shared_from_this());

        // 读取直到遇到\r\n\r\n（HTTP头部结束）
        async_read(8192, [self](const char* data, size_t len) {
            self->request_data_.append(data, len);

            // 检查是否收到完整头部
            size_t header_end = self->request_data_.find("\r\n\r\n");
            if (header_end == std::string::npos) {
                // 继续读取
                self->read_request();
                return;
            }

            // 解析请求
            HttpRequest request;
            if (request.parse(self->request_data_)) {
                // 处理请求
                HttpResponse response = self->request_handler_(request);

                // 添加常用头部
                response.headers["Server"] = "ProactorHTTP/1.0";
                response.headers["Connection"] = "close";

                // 发送响应
                self->send(response.to_string());
            }

            // HTTP/1.0简化处理：响应后关闭连接
            self->close();
        });
    }

private:
    RequestHandler request_handler_;
    std::string request_data_;
};

// HTTP服务器工厂
class HttpServer {
public:
    HttpServer(std::unique_ptr<IAsyncIO> io)
        : server_(std::move(io))
    {}

    // 注册路由
    void route(const std::string& path, HttpSession::RequestHandler handler) {
        routes_[path] = std::move(handler);
    }

    // 监听
    bool listen(uint16_t port) {
        // 设置会话工厂
        server_.set_session_factory(
            [this](ProactorServer& server, int fd) {
                return std::make_shared<HttpSession>(server, fd,
                    [this](const HttpRequest& req) {
                        return handle_request(req);
                    });
            });

        return server_.listen(port);
    }

    // 启动
    void start() { server_.start(); }

    // 运行
    void run() { server_.run(); }

    // 停止
    void stop() { server_.stop(); }

private:
    HttpResponse handle_request(const HttpRequest& request) {
        // 查找路由
        auto it = routes_.find(request.path);
        if (it != routes_.end()) {
            return it->second(request);
        }

        // 默认404
        HttpResponse response;
        response.status_code = 404;
        response.status_text = "Not Found";
        response.headers["Content-Type"] = "text/plain";
        response.body = "404 Not Found";
        return response;
    }

private:
    ProactorServer server_;
    std::map<std::string, HttpSession::RequestHandler> routes_;
};

} // namespace cross_platform
```

### Day 28：性能对比与总结（5小时）

#### 28.1 Reactor vs Proactor性能测试

```
性能对比测试方案：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  测试环境：                                                             │
│  - 服务器：8核CPU，32GB内存                                            │
│  - 客户端：使用wrk/ab进行压测                                          │
│  - 网络：本地回环或千兆网                                              │
│                                                                          │
│  测试场景：                                                             │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  场景1：Echo服务器                                                │ │
│  │  - 短连接，每连接发送1KB数据后断开                                │ │
│  │  - 测试连接建立/销毁性能                                          │ │
│  │                                                                    │ │
│  │  场景2：HTTP服务器                                                │ │
│  │  - 静态响应，返回固定大小内容                                     │ │
│  │  - 测试请求处理吞吐量                                             │ │
│  │                                                                    │ │
│  │  场景3：高并发长连接                                              │ │
│  │  - 10000个长连接                                                  │ │
│  │  - 每连接每秒发送10条消息                                         │ │
│  │  - 测试高并发下的稳定性                                           │ │
│  │                                                                    │ │
│  │  场景4：大文件传输                                                │ │
│  │  - 传输100MB文件                                                  │ │
│  │  - 测试I/O效率                                                    │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  预期结果分析：                                                         │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  测试项        │ Reactor(epoll) │ Proactor(io_uring) │ 差异      │ │
│  │  ──────────────┼────────────────┼────────────────────┼──────────  │ │
│  │  短连接QPS     │   120K/s       │    150K/s          │  +25%     │ │
│  │  HTTP RPS      │   100K/s       │    130K/s          │  +30%     │ │
│  │  长连接延迟    │   0.3ms        │    0.2ms           │  -33%     │ │
│  │  大文件带宽    │   800MB/s      │    1GB/s           │  +25%     │ │
│  │  CPU使用率     │   75%          │    60%             │  -20%     │ │
│  │                                                                    │ │
│  │  注：实际结果受硬件、内核版本、测试方法影响                       │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 28.2 不同场景下的选择建议

```
Proactor vs Reactor选择指南：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  选择Proactor的场景：                                                   │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  ✓ Windows平台开发                                                │ │
│  │    └── IOCP是Windows最高效的I/O机制                               │ │
│  │                                                                    │ │
│  │  ✓ 高吞吐量文件服务器                                             │ │
│  │    └── 文件I/O受益于真正的异步                                    │ │
│  │                                                                    │ │
│  │  ✓ 需要最低延迟的系统                                             │ │
│  │    └── io_uring的零拷贝减少延迟                                   │ │
│  │                                                                    │ │
│  │  ✓ CPU密集型+I/O混合场景                                          │ │
│  │    └── I/O在内核异步执行，CPU可做其他事                           │ │
│  │                                                                    │ │
│  │  ✓ 需要统一文件和网络I/O的场景                                    │ │
│  │    └── Proactor模型对两者一视同仁                                 │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  选择Reactor的场景：                                                    │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  ✓ Linux传统服务器开发                                            │ │
│  │    └── epoll成熟稳定，文档丰富                                    │ │
│  │                                                                    │ │
│  │  ✓ 内核版本较低的环境                                             │ │
│  │    └── io_uring需要Linux 5.1+                                     │ │
│  │                                                                    │ │
│  │  ✓ 简单的网络代理服务                                             │ │
│  │    └── Reactor模型更直观                                          │ │
│  │                                                                    │ │
│  │  ✓ 需要细粒度控制I/O时机的场景                                    │ │
│  │    └── Reactor可以精确控制何时读写                                │ │
│  │                                                                    │ │
│  │  ✓ 团队对Reactor更熟悉                                            │ │
│  │    └── 开发效率也很重要                                           │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  混合方案：                                                             │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  ✓ 网络I/O用Reactor，文件I/O用线程池                              │ │
│  │    └── 简化网络编程，同时享受异步文件I/O                          │ │
│  │                                                                    │ │
│  │  ✓ 使用Boost.Asio/libuv等成熟框架                                 │ │
│  │    └── 框架自动选择最优后端                                       │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 28.3 云原生时代的演进趋势

```
异步I/O的未来趋势：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. io_uring的全面普及                                                  │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  现状：                                                           │ │
│  │  - Linux 5.1+ 支持基础io_uring                                    │ │
│  │  - Linux 5.6+ 支持网络操作                                        │ │
│  │  - Linux 5.10+ 支持更多高级特性                                   │ │
│  │                                                                    │ │
│  │  趋势：                                                           │ │
│  │  - 主流发行版内核版本持续升级                                     │ │
│  │  - 云服务商逐步支持新内核                                         │ │
│  │  - 更多框架添加io_uring后端                                       │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  2. 协程与异步I/O的结合                                                │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  C++20 协程：                                                     │ │
│  │  co_await async_read(socket, buffer);                             │ │
│  │  co_await async_write(socket, response);                          │ │
│  │                                                                    │ │
│  │  优势：                                                           │ │
│  │  - 异步代码写法接近同步                                           │ │
│  │  - 编译器优化，零开销抽象                                         │ │
│  │  - 更好的错误处理（异常传播）                                     │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  3. eBPF与XDP的兴起                                                     │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  XDP (eXpress Data Path)：                                        │ │
│  │  - 在网卡驱动层处理数据包                                         │ │
│  │  - 绕过内核协议栈                                                 │ │
│  │  - 适用于特定高性能场景                                           │ │
│  │                                                                    │ │
│  │  应用场景：                                                       │ │
│  │  - DDoS防护                                                       │ │
│  │  - 负载均衡                                                       │ │
│  │  - 网络监控                                                       │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  4. Sidecar代理与Service Mesh                                          │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  Envoy等代理：                                                    │ │
│  │  - 使用高效的事件驱动架构                                         │ │
│  │  - 作为Sidecar拦截所有网络流量                                    │ │
│  │  - 提供可观测性、安全性、流量管理                                 │ │
│  │                                                                    │ │
│  │  对开发者影响：                                                   │ │
│  │  - 应用程序可以使用简单的同步模型                                 │ │
│  │  - 复杂的网络功能由Sidecar处理                                    │ │
│  │  - 关注点分离                                                     │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

### 第四周自测题

#### 概念理解题

1. **跨平台异步I/O设计时，编译时多态和运行时多态各有什么优缺点？**

   <details>
   <summary>参考答案</summary>

   编译时多态（模板）：
   - 优点：零运行时开销，内联优化
   - 缺点：编译时间长，代码膨胀

   运行时多态（虚函数）：
   - 优点：灵活性高，可动态切换后端
   - 缺点：虚函数调用开销，无法内联

   实践中常用：接口用运行时多态，热点路径用编译时多态。
   </details>

2. **Boost.Asio的strand是如何实现无锁串行化的？**

   <details>
   <summary>参考答案</summary>

   strand使用锁保护内部队列，但执行handler时释放锁：
   1. handler入队时加锁检查是否有正在执行的handler
   2. 如果没有，标记running并投递执行任务
   3. 执行任务在锁外执行handler
   4. 执行完检查队列，有则继续，无则清除running标记

   关键：handler执行时不持锁，所以多个handler不会竞争锁。
   </details>

3. **为什么模拟Proactor需要线程池？**

   <details>
   <summary>参考答案</summary>

   - Reactor只通知"就绪"，应用需要自己执行I/O
   - 如果在事件循环线程执行I/O，会阻塞其他事件处理
   - 线程池在后台执行I/O，完成后投递回调到事件循环
   - 这样模拟了"完成通知"的语义
   </details>

---

### 第四周检验标准

| 检验项 | 标准 | 自评 |
|--------|------|------|
| 设计跨平台接口 | 接口抽象合理 | ☐ |
| 实现模拟Proactor | 能正确工作 | ☐ |
| 理解Asio架构 | 能解释核心组件 | ☐ |
| 理解strand机制 | 能解释工作原理 | ☐ |
| 实现完整框架 | 框架可扩展 | ☐ |
| 实现HTTP服务器 | 能处理基本请求 | ☐ |
| 性能对比意识 | 知道何时用哪种模式 | ☐ |
| 了解演进趋势 | 知道未来发展方向 | ☐ |

---

### 第四周时间分配

| 内容 | 时间 |
|------|------|
| 跨平台接口设计 | 6小时 |
| 模拟Proactor实现 | 6小时 |
| Asio源码分析 | 8小时 |
| 完整框架实现 | 8小时 |
| HTTP服务器实现 | 4小时 |
| 性能测试与总结 | 3小时 |

---

## 本月检验标准汇总

### 理论检验（20项）

| 序号 | 检验项 | 标准 | 自评 |
|------|--------|------|------|
| 1 | 理解同步/异步区别 | 能用时序图解释 | ☐ |
| 2 | 理解5种I/O模型 | 能画出对比图 | ☐ |
| 3 | 理解Proactor组件 | 能说出所有组件职责 | ☐ |
| 4 | 对比Reactor/Proactor | 能从5个维度分析 | ☐ |
| 5 | 理解IOCP架构 | 能画出内部结构图 | ☐ |
| 6 | 理解重叠I/O | 能解释OVERLAPPED作用 | ☐ |
| 7 | 理解完成键设计 | 知道最佳实践 | ☐ |
| 8 | 理解IOCP线程管理 | 能解释LIFO原因 | ☐ |
| 9 | 理解io_uring架构 | 能画出SQ/CQ图 | ☐ |
| 10 | 理解SQE/CQE结构 | 能解释关键字段 | ☐ |
| 11 | 理解内存映射机制 | 知道零拷贝原理 | ☐ |
| 12 | 理解SQPOLL模式 | 知道适用场景 | ☐ |
| 13 | 理解链式操作 | 能解释IO_LINK用法 | ☐ |
| 14 | 理解跨平台设计 | 能设计抽象层 | ☐ |
| 15 | 理解Asio架构 | 知道核心组件 | ☐ |
| 16 | 理解strand机制 | 能解释串行化原理 | ☐ |
| 17 | 理解组合操作 | 知道async_read_until原理 | ☐ |
| 18 | 理解错误处理 | 能分类处理错误 | ☐ |
| 19 | 性能对比意识 | 知道选择依据 | ☐ |
| 20 | 了解演进趋势 | 知道io_uring/协程趋势 | ☐ |

### 实践检验（20项）

| 序号 | 检验项 | 标准 | 自评 |
|------|--------|------|------|
| 1 | 实现I/O模型示例 | 代码能运行 | ☐ |
| 2 | 实现模拟Proactor | 基于epoll+线程池 | ☐ |
| 3 | IOCP基础服务器 | 能处理连接 | ☐ |
| 4 | IOCP错误处理 | 能分类处理 | ☐ |
| 5 | IOCP资源管理 | 正确管理生命周期 | ☐ |
| 6 | IOCP批量处理 | 使用GetQueuedCompletionStatusEx | ☐ |
| 7 | IOCP高性能服务器 | 包含对象池 | ☐ |
| 8 | io_uring基础示例 | 文件读写正确 | ☐ |
| 9 | io_uring批量操作 | 批量提交/获取 | ☐ |
| 10 | io_uring链式操作 | 使用IO_LINK | ☐ |
| 11 | AsyncOperation设计 | 抽象合理 | ☐ |
| 12 | Proactor核心实现 | 事件循环正确 | ☐ |
| 13 | io_uring Echo服务器 | 能Echo数据 | ☐ |
| 14 | 跨平台接口实现 | IAsyncIO接口完整 | ☐ |
| 15 | 模拟Proactor后端 | 正确工作 | ☐ |
| 16 | Session框架 | 支持继承扩展 | ☐ |
| 17 | HTTP服务器 | 能处理GET请求 | ☐ |
| 18 | 组合操作实现 | async_read_exactly | ☐ |
| 19 | 定时器实现 | async_wait正确 | ☐ |
| 20 | 性能测试 | 有对比数据 | ☐ |

---

## 输出物清单

```
proactor_project/
├── docs/
│   ├── proactor_architecture.md      # Proactor架构分析文档
│   ├── iocp_analysis.md              # IOCP深度分析
│   ├── io_uring_analysis.md          # io_uring深度分析
│   └── performance_comparison.md     # 性能对比报告
│
├── examples/
│   ├── io_models/
│   │   ├── blocking_io.cpp           # 阻塞I/O示例
│   │   ├── nonblocking_io.cpp        # 非阻塞I/O示例
│   │   ├── io_multiplexing.cpp       # I/O多路复用示例
│   │   └── async_io.cpp              # 异步I/O示例
│   │
│   ├── posix_aio/
│   │   └── posix_aio_example.cpp     # POSIX AIO示例
│   │
│   ├── iocp/
│   │   ├── iocp_basic_server.cpp     # IOCP基础服务器
│   │   ├── iocp_batch_example.cpp    # 批量处理示例
│   │   └── iocp_hp_server.cpp        # 高性能服务器
│   │
│   └── io_uring/
│       ├── io_uring_basic.cpp        # io_uring基础示例
│       ├── io_uring_batch.cpp        # 批量操作示例
│       ├── io_uring_linked.cpp       # 链式操作示例
│       └── io_uring_echo.cpp         # Echo服务器
│
├── include/
│   ├── common/
│   │   ├── buffer.hpp                # 缓冲区定义
│   │   └── error.hpp                 # 错误码定义
│   │
│   ├── proactor/
│   │   ├── async_operation.hpp       # 异步操作基类
│   │   ├── proactor.hpp              # Proactor核心
│   │   └── composed_op.hpp           # 组合操作
│   │
│   ├── backend/
│   │   ├── iocp_backend.hpp          # IOCP后端
│   │   ├── io_uring_backend.hpp      # io_uring后端
│   │   └── simulated_backend.hpp     # 模拟Proactor后端
│   │
│   └── framework/
│       ├── async_io_interface.hpp    # 跨平台接口
│       ├── session.hpp               # 会话基类
│       └── server.hpp                # 服务器框架
│
├── src/
│   ├── iocp_backend.cpp              # IOCP实现
│   ├── io_uring_backend.cpp          # io_uring实现
│   ├── simulated_backend.cpp         # 模拟实现
│   └── http_server.cpp               # HTTP服务器
│
├── tests/
│   ├── test_async_operation.cpp      # 异步操作测试
│   ├── test_proactor.cpp             # Proactor测试
│   └── test_http_server.cpp          # HTTP服务器测试
│
├── benchmark/
│   ├── reactor_vs_proactor.cpp       # 性能对比
│   └── results/                      # 测试结果
│
├── notes/
│   └── month31_proactor.md           # 学习笔记
│
├── CMakeLists.txt
└── README.md
```

### 输出物完成度检查

| 输出物 | 描述 | 完成 |
|--------|------|------|
| proactor_architecture.md | Proactor架构分析文档 | ☐ |
| iocp_analysis.md | Windows IOCP详细分析 | ☐ |
| io_uring_analysis.md | Linux io_uring详细分析 | ☐ |
| io_models示例 | 4种I/O模型演示 | ☐ |
| iocp_basic_server | IOCP基础服务器 | ☐ |
| iocp_hp_server | 高性能IOCP服务器 | ☐ |
| io_uring_basic | io_uring基础示例 | ☐ |
| io_uring_echo | io_uring Echo服务器 | ☐ |
| async_io_interface.hpp | 跨平台接口定义 | ☐ |
| simulated_backend | 模拟Proactor实现 | ☐ |
| http_server | HTTP服务器示例 | ☐ |
| benchmark | 性能对比测试 | ☐ |
| month31_proactor.md | 学习笔记 | ☐ |

---

## 学习建议

### 学习路径图

```
Month-31 Proactor模式学习路径：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  起点：Month-30 Reactor模式基础                                         │
│    │                                                                    │
│    ▼                                                                    │
│  ┌─────────────────┐                                                    │
│  │ 1. I/O模型对比  │ ◀── 理解同步/异步、阻塞/非阻塞                   │
│  └────────┬────────┘                                                    │
│           │                                                              │
│           ▼                                                              │
│  ┌─────────────────┐                                                    │
│  │ 2. Proactor理论 │ ◀── 理解组件和数据流                              │
│  └────────┬────────┘                                                    │
│           │                                                              │
│           ├──────────────────────────────┐                              │
│           ▼                              ▼                              │
│  ┌─────────────────┐          ┌─────────────────┐                      │
│  │ 3a. IOCP实践   │          │ 3b. io_uring实践│                      │
│  │   (Windows)     │          │   (Linux)       │                      │
│  └────────┬────────┘          └────────┬────────┘                      │
│           │                            │                                │
│           └──────────┬─────────────────┘                                │
│                      ▼                                                  │
│           ┌─────────────────┐                                          │
│           │ 4. 跨平台抽象  │ ◀── 设计统一接口                         │
│           └────────┬────────┘                                          │
│                    │                                                    │
│                    ▼                                                    │
│           ┌─────────────────┐                                          │
│           │ 5. 框架实现    │ ◀── 完整Proactor框架                     │
│           └────────┬────────┘                                          │
│                    │                                                    │
│                    ▼                                                    │
│           ┌─────────────────┐                                          │
│           │ 6. 性能测试    │ ◀── 与Reactor对比                        │
│           └────────┬────────┘                                          │
│                    │                                                    │
│                    ▼                                                    │
│  终点：Month-32 Envoy架构分析                                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 调试技巧

```
Proactor调试技巧：
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  1. IOCP调试                                                            │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  // 检查GetLastError                                               │ │
│  │  BOOL ret = GetQueuedCompletionStatus(...);                       │ │
│  │  if (!ret) {                                                       │ │
│  │      DWORD err = GetLastError();                                   │ │
│  │      printf("Error: %d\n", err);                                   │ │
│  │      // 使用FormatMessage获取错误描述                             │ │
│  │  }                                                                 │ │
│  │                                                                    │ │
│  │  // 检查OVERLAPPED状态                                            │ │
│  │  if (overlapped && overlapped->Internal != 0) {                   │ │
│  │      printf("Internal error: %lx\n", overlapped->Internal);       │ │
│  │  }                                                                 │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  2. io_uring调试                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  // 检查CQE结果                                                    │ │
│  │  if (cqe->res < 0) {                                               │ │
│  │      printf("Error: %s\n", strerror(-cqe->res));                  │ │
│  │  }                                                                 │ │
│  │                                                                    │ │
│  │  // 检查SQ/CQ溢出                                                 │ │
│  │  if (*ring->sq.kflags & IORING_SQ_NEED_WAKEUP) {                  │ │
│  │      printf("SQ needs wakeup\n");                                  │ │
│  │  }                                                                 │ │
│  │                                                                    │ │
│  │  // 使用strace跟踪系统调用                                        │ │
│  │  // strace -e io_uring_enter,io_uring_setup ./program             │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  3. 内存问题调试                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  // 常见问题：OVERLAPPED/SQE释放过早                              │ │
│  │                                                                    │ │
│  │  // 错误示例：                                                    │ │
│  │  OVERLAPPED ov = {};  // 栈上分配                                 │ │
│  │  WSARecv(sock, &buf, 1, NULL, &flags, &ov, NULL);                │ │
│  │  // 函数返回后ov被销毁，但I/O还在进行！                          │ │
│  │                                                                    │ │
│  │  // 正确做法：                                                    │ │
│  │  auto* ov = new OVERLAPPED();  // 堆上分配                        │ │
│  │  WSARecv(sock, &buf, 1, NULL, &flags, ov, NULL);                 │ │
│  │  // 在完成回调中delete                                            │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  4. 连接泄漏检查                                                        │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │  // 记录活跃连接数                                                │ │
│  │  std::atomic<int> active_connections{0};                          │ │
│  │                                                                    │ │
│  │  // 连接建立时++                                                  │ │
│  │  active_connections++;                                             │ │
│  │                                                                    │ │
│  │  // 连接关闭时--                                                  │ │
│  │  active_connections--;                                             │ │
│  │                                                                    │ │
│  │  // 定期打印，检查是否持续增长                                    │ │
│  │  printf("Active: %d\n", active_connections.load());               │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 常见错误与解决

| 错误 | 原因 | 解决方案 |
|------|------|----------|
| IOCP投递失败 | SQ已满 | 先提交部分，再重试 |
| 回调不执行 | 未调用run() | 确保事件循环运行 |
| 数据损坏 | 缓冲区过早释放 | 使用shared_ptr延长生命周期 |
| 连接泄漏 | 未正确关闭 | 使用RAII管理资源 |
| 性能差 | 线程数不当 | 调整为CPU核心数×2 |
| 死锁 | handler中同步等待 | 所有操作都用异步 |
| 内存暴涨 | 发送队列积压 | 添加流量控制 |

---

## 结语

```
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   ╔═══════════════════════════════════════════════════════════════════╗ │
│   ║                                                                   ║ │
│   ║       🎉 恭喜完成 Month-31: Proactor模式——异步完成通知 🎉        ║ │
│   ║                                                                   ║ │
│   ╚═══════════════════════════════════════════════════════════════════╝ │
│                                                                          │
│   本月你学到了：                                                        │
│                                                                          │
│   ┌─────────────────────────────────────────────────────────────────┐  │
│   │                                                                  │  │
│   │   ✓ 同步I/O与异步I/O的本质区别                                  │  │
│   │   ✓ Proactor模式的核心组件和工作流程                            │  │
│   │   ✓ Windows IOCP的完整实现和最佳实践                            │  │
│   │   ✓ Linux io_uring的革命性设计                                  │  │
│   │   ✓ 跨平台异步I/O框架的设计思路                                 │  │
│   │   ✓ Boost.Asio的架构精髓                                        │  │
│   │   ✓ 高性能服务器的实现技巧                                      │  │
│   │                                                                  │  │
│   └─────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│   知识总结：                                                            │
│                                                                          │
│         Reactor                    Proactor                             │
│            │                          │                                 │
│     "I/O就绪通知"              "I/O完成通知"                           │
│            │                          │                                 │
│         epoll                   IOCP / io_uring                        │
│         kqueue                                                          │
│            │                          │                                 │
│     应用程序执行I/O            内核执行I/O                             │
│            │                          │                                 │
│     适合网络I/O                适合文件+网络I/O                        │
│            │                          │                                 │
│            └──────────┬───────────────┘                                │
│                       │                                                 │
│               理解两者，灵活选择                                        │
│                                                                          │
│   从Reactor到Proactor，你已经掌握了高性能I/O的两大范式！               │
│   接下来，Month-32将学习Envoy的架构设计，看看这些技术如何在            │
│   现代云原生代理中得到应用。                                           │
│                                                                          │
│   继续前进！ 🚀                                                         │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 下月预告

### Month-32：Envoy架构分析——云原生代理

```
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Month-32 预览：                                                        │
│                                                                          │
│  Envoy是云原生时代最重要的网络代理之一，被Istio等Service Mesh         │
│  广泛采用。它将Month-30/31学到的Reactor/Proactor知识付诸实践。         │
│                                                                          │
│  主要内容：                                                             │
│  ┌───────────────────────────────────────────────────────────────────┐ │
│  │                                                                    │ │
│  │  第一周：Envoy架构概览                                            │ │
│  │  ├── Listener / Filter Chain / Cluster 核心概念                  │ │
│  │  └── 整体数据流分析                                               │ │
│  │                                                                    │ │
│  │  第二周：线程模型                                                 │ │
│  │  ├── Main Thread vs Worker Thread                                │ │
│  │  ├── Thread Local Storage (TLS) 机制                             │ │
│  │  └── 为什么选择这种模型                                          │ │
│  │                                                                    │ │
│  │  第三周：Filter机制                                               │ │
│  │  ├── Network Filter / HTTP Filter                                │ │
│  │  ├── Filter Chain的工作原理                                      │ │
│  │  └── 自定义Filter开发                                            │ │
│  │                                                                    │ │
│  │  第四周：配置热更新                                               │ │
│  │  ├── xDS协议（LDS/RDS/CDS/EDS）                                  │ │
│  │  ├── 配置无损更新机制                                            │ │
│  │  └── 与控制面（Istio）的交互                                     │ │
│  │                                                                    │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  实践项目：简化版代理服务器                                            │
│  - 借鉴Envoy的线程模型                                                 │
│  - 实现Filter Chain机制                                                │
│  - 支持配置热更新                                                      │
│                                                                          │
│  准备工作：                                                             │
│  1. 阅读Envoy官方文档架构部分                                          │
│  2. 克隆Envoy源码，熟悉目录结构                                        │
│  3. 了解gRPC和Protocol Buffers基础                                     │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 附录：参考资源

### 官方文档
- [MSDN: I/O Completion Ports](https://docs.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports)
- [Linux io_uring文档](https://kernel.dk/io_uring.pdf)
- [liburing GitHub](https://github.com/axboe/liburing)
- [Boost.Asio文档](https://www.boost.org/doc/libs/release/doc/html/boost_asio.html)

### 推荐阅读
- 《Windows核心编程》- I/O完成端口章节
- 《Linux高性能服务器编程》
- io_uring作者Jens Axboe的博客

### 源码参考
- [libuv](https://github.com/libuv/libuv) - Node.js的跨平台异步I/O库
- [Tokio](https://github.com/tokio-rs/tokio) - Rust异步运行时，有io_uring后端
- [Boost.Asio](https://github.com/boostorg/asio)
